<!DOCTYPE html>
<!-- saved from url=(0039)https://www.zhihu.com/question/26408259 -->
<html lang="zh" data-hairline="true" data-theme="light" class="gr__zhihu_com"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>如何用简单例子讲解 Q - learning 的具体过程？ - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="renderer" content="webkit"><meta name="force-rendering" content="webkit"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"><meta data-react-helmet="true" name="description" property="og:description" content="Q-learning如何在探索和经验之间进行平衡？Q-learning每次迭代都沿当前Q值最高的路径前进吗？"><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"><link rel="dns-prefetch" href="https://static.zhimg.com/"><link rel="dns-prefetch" href="https://pic1.zhimg.com/"><link rel="dns-prefetch" href="https://pic2.zhimg.com/"><link rel="dns-prefetch" href="https://pic3.zhimg.com/"><link rel="dns-prefetch" href="https://pic4.zhimg.com/"><link href="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/app.b28321a7e9c19c2be505.css" rel="stylesheet"><link href="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/question-routes.13683ce9b28b8450de46.css" rel="stylesheet"><script defer="" crossorigin="anonymous" src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/init.js.download" data-sentry-config="{&quot;dsn&quot;:&quot;https://65e244586890460588f00f2987137aa8@crash2.zhihu.com/193&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;1858-ceb38b16&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrors&quot;:[&quot;origin message&quot;,&quot;Network request failed&quot;,&quot;Loading chunk&quot;,&quot;这个系统不支持该功能。&quot;,&quot;Can&#39;t find variable: webkit&quot;,&quot;Can&#39;t find variable: $&quot;,&quot;内存不足&quot;,&quot;out of memory&quot;,&quot;DOM Exception 18&quot;,&quot;The operation is insecure&quot;,&quot;[object Event]&quot;,&quot;[object FileError]&quot;,&quot;[object DOMError]&quot;,&quot;[object Object]&quot;,&quot;拒绝访问。&quot;,&quot;Maximum call stack size exceeded&quot;,&quot;UploadError&quot;,&quot;无法 fetch&quot;,&quot;draft-js&quot;,&quot;缺少 JavaScript 对象&quot;,&quot;componentWillEnter&quot;,&quot;componentWillLeave&quot;,&quot;componentWillAppear&quot;,&quot;getInlineStyleAt&quot;,&quot;getCharacterList&quot;],&quot;whitelistUrls&quot;:[&quot;static.zhihu.com&quot;]}"></script><link rel="stylesheet" type="text/css" href="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/richinput.fcb2b23b621ba0e1867e.css"><script charset="utf-8" src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/main.richinput.b13457a6b3539993f48f.js.download"></script><link rel="stylesheet" type="text/css" href="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/modals.c93442489f530ff4cf26.css"><script charset="utf-8" src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/main.modals.54a735df3baef57fe13a.js.download"></script><link rel="stylesheet" type="text/css" href="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/signflow.0547812679d688e4e3ca.css"><script charset="utf-8" src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/main.signflow.6696ff95fe43f4ed82d4.js.download"></script></head><body class="Entry-body" data-gr-c-s-loaded="true"><div id="root"><div data-zop-usertoken="{}"><div class="LoadingBar"></div><div><header role="banner" class="Sticky AppHeader is-fixed" data-za-module="TopNavBar" style="width: 1395px; top: 0px; left: 0px;"><div class="AppHeader-inner"><a href="https://www.zhihu.com/" aria-label="知乎"><svg viewBox="0 0 200 91" class="Icon ZhihuLogo ZhihuLogo--blue Icon--logo" width="64" height="30" aria-hidden="true" style="height: 30px; width: 64px;"><title></title><g><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></g></svg></a><ul role="navigation" class="Tabs AppHeader-Tabs"><li role="tab" class="Tabs-item AppHeader-Tab Tabs-item--noMeta"><a class="Tabs-link AppHeader-TabsLink" href="https://www.zhihu.com/" data-za-not-track-link="true">首页</a></li><li role="tab" class="Tabs-item AppHeader-Tab Tabs-item--noMeta"><a class="Tabs-link AppHeader-TabsLink" href="https://www.zhihu.com/explore" data-za-not-track-link="true">发现</a></li><li role="tab" class="Tabs-item AppHeader-Tab Tabs-item--noMeta"><a class="Tabs-link AppHeader-TabsLink" href="https://www.zhihu.com/question/waiting" data-za-not-track-link="true">等你来答</a></li></ul><div class="SearchBar" role="search" data-za-module="PresetWordItem"><div class="SearchBar-toolWrapper"><form class="SearchBar-tool"><div><div class="Popover"><div class="SearchBar-input Input-wrapper Input-wrapper--grey"><input type="text" maxlength="100" autocomplete="off" role="combobox" aria-expanded="false" aria-autocomplete="list" aria-activedescendant="AutoComplete6--1" id="Popover5-toggle" aria-haspopup="true" aria-owns="Popover5-content" class="Input" placeholder="暑期男生如何变帅" value=""><div class="Input-after"><button aria-label="搜索" type="button" class="Button SearchBar-searchIcon Button--primary"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Search" fill="currentColor" viewBox="0 0 24 24" width="18" height="18"><path d="M17.068 15.58a8.377 8.377 0 0 0 1.774-5.159 8.421 8.421 0 1 0-8.42 8.421 8.38 8.38 0 0 0 5.158-1.774l3.879 3.88c.957.573 2.131-.464 1.488-1.49l-3.879-3.878zm-6.647 1.157a6.323 6.323 0 0 1-6.316-6.316 6.323 6.323 0 0 1 6.316-6.316 6.323 6.323 0 0 1 6.316 6.316 6.323 6.323 0 0 1-6.316 6.316z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div></form></div></div><div class="AppHeader-userInfo"><div class="AppHeader-profile"><div><button type="button" class="Button AppHeader-login Button--blue">登录</button><button type="button" class="Button Button--primary Button--blue">加入知乎</button></div></div></div></div><div class="PageHeader"><div class="QuestionHeader-content"><div class="QuestionHeader-main"><h1 class="QuestionHeader-title">如何用简单例子讲解 Q - learning 的具体过程？</h1></div><div class="QuestionHeader-side" data-za-detail-view-path-module="ToolBar" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;26408259&quot;}}}"><div class="QuestionButtonGroup"><button type="button" class="Button FollowButton Button--primary Button--blue">关注问题</button><button type="button" class="Button Button--blue"><svg viewBox="0 0 12 12" class="Icon QuestionButton-icon Button-icon Icon--modify" width="14" height="16" aria-hidden="true" style="height: 16px; width: 14px;"><title></title><g><path d="M.423 10.32L0 12l1.667-.474 1.55-.44-2.4-2.33-.394 1.564zM10.153.233c-.327-.318-.85-.31-1.17.018l-.793.817 2.49 2.414.792-.814c.318-.328.312-.852-.017-1.17l-1.3-1.263zM3.84 10.536L1.35 8.122l6.265-6.46 2.49 2.414-6.265 6.46z" fill-rule="evenodd"></path></g></svg>写回答</button></div></div></div></div></header><div class="Sticky--holder" style="position: relative; top: 0px; right: 0px; bottom: 0px; left: 0px; display: block; float: none; margin: 0px; height: 52px;"></div></div><main role="main" class="App-main"><div class="QuestionPage" itemscope="" itemtype="http://schema.org/Question" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;26408259&quot;}}}"><meta itemprop="name" content="如何用简单例子讲解 Q - learning 的具体过程？"><meta itemprop="url" content="https://www.zhihu.com/question/26408259"><meta itemprop="keywords" content="算法,机器学习,无监督学习,深度学习（Deep Learning）,强化学习 (Reinforcement Learning)"><meta itemprop="answerCount" content="19"><meta itemprop="commentCount" content="0"><meta itemprop="dateCreated" content="2014-10-30T07:22:12.000Z"><meta itemprop="dateModified" content="2016-09-01T05:29:23.000Z"><meta itemprop="zhihu:visitsCount"><meta itemprop="zhihu:followerCount" content="1337"><script type="application/ld+json">
        {
          "@context": "https://ziyuan.baidu.com/contexts/cambrian.jsonld",
          "@id": "https://www.zhihu.com/question/26408259",
          "appid": "否",
          "pubDate": "2014-10-30T07:22:12",
          "upDate": "2016-09-01T06:29:23"
        }</script><div data-zop-question="{&quot;title&quot;:&quot;如何用简单例子讲解 Q - learning 的具体过程？&quot;,&quot;topics&quot;:[{&quot;name&quot;:&quot;算法&quot;,&quot;id&quot;:&quot;19553510&quot;},{&quot;name&quot;:&quot;机器学习&quot;,&quot;id&quot;:&quot;19559450&quot;},{&quot;name&quot;:&quot;无监督学习&quot;,&quot;id&quot;:&quot;19590194&quot;},{&quot;name&quot;:&quot;深度学习（Deep Learning）&quot;,&quot;id&quot;:&quot;19813032&quot;},{&quot;name&quot;:&quot;强化学习 (Reinforcement Learning)&quot;,&quot;id&quot;:&quot;20039099&quot;}],&quot;id&quot;:26408259,&quot;isEditable&quot;:false}"><div class="QuestionStatus"></div><div class="QuestionHeader" data-za-detail-view-path-module="QuestionDescription" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;26408259&quot;}}}"><div class="QuestionHeader-content"><div class="QuestionHeader-main"><div class="QuestionHeader-tags"><div class="QuestionHeader-topics"><div class="Tag QuestionTopic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19553510&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/19553510" target="_blank"><div class="Popover"><div id="Popover7-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover7-content">算法</div></div></a></span></div><div class="Tag QuestionTopic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19559450&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/19559450" target="_blank"><div class="Popover"><div id="Popover8-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover8-content">机器学习</div></div></a></span></div><div class="Tag QuestionTopic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19590194&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/19590194" target="_blank"><div class="Popover"><div id="Popover9-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover9-content">无监督学习</div></div></a></span></div><div class="Tag QuestionTopic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19813032&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/19813032" target="_blank"><div class="Popover"><div id="Popover10-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover10-content">深度学习（Deep Learning）</div></div></a></span></div><div class="Tag QuestionTopic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;20039099&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/20039099" target="_blank"><div class="Popover"><div id="Popover11-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover11-content">强化学习 (Reinforcement Learning)</div></div></a></span></div></div></div><h1 class="QuestionHeader-title">如何用简单例子讲解 Q - learning 的具体过程？</h1><div><div class="QuestionHeader-detail"><div class="QuestionRichText QuestionRichText--collapsed"><div><span class="RichText ztext" itemprop="text">Q-learning如何在探索和经验之间进行平衡？Q-learning每次迭代都沿当前Q值最高的路径前进吗？</span></div></div></div></div></div><div class="QuestionHeader-side"><div class="QuestionHeader-follow-status"><div class="QuestionFollowStatus"><div class="NumberBoard QuestionFollowStatus-counts NumberBoard--divider"><div class="NumberBoard-item"><div class="NumberBoard-itemInner"><div class="NumberBoard-itemName">关注者</div><strong class="NumberBoard-itemValue" title="1337">1,337</strong></div></div><div class="NumberBoard-item"><div class="NumberBoard-itemInner"><div class="NumberBoard-itemName">被浏览</div><strong class="NumberBoard-itemValue" title="147660">147,660</strong></div></div></div></div></div></div></div><div class="QuestionHeader-footer"><div class="QuestionHeader-footer-inner"><div class="QuestionHeader-main QuestionHeader-footer-main"><div class="QuestionButtonGroup"><button type="button" class="Button FollowButton Button--primary Button--blue">关注问题</button><button type="button" class="Button Button--blue"><svg viewBox="0 0 12 12" class="Icon QuestionButton-icon Button-icon Icon--modify" width="14" height="16" aria-hidden="true" style="height: 16px; width: 14px;"><title></title><g><path d="M.423 10.32L0 12l1.667-.474 1.55-.44-2.4-2.33-.394 1.564zM10.153.233c-.327-.318-.85-.31-1.17.018l-.793.817 2.49 2.414.792-.814c.318-.328.312-.852-.017-1.17l-1.3-1.263zM3.84 10.536L1.35 8.122l6.265-6.46 2.49 2.414-6.265 6.46z" fill-rule="evenodd"></path></g></svg>写回答</button></div><div class="QuestionHeaderActions"><button type="button" class="Button Button--grey Button--withIcon Button--withLabel" style="margin-right: 16px;"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Invite Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M4 10V8a1 1 0 1 1 2 0v2h2a1 1 0 0 1 0 2H6v2a1 1 0 0 1-2 0v-2H2a1 1 0 0 1 0-2h2zm10.455 2c-2.21 0-4-1.79-4-4s1.79-4 4-4 4 1.79 4 4-1.79 4-4 4zm-7 6c0-2.66 4.845-4 7.272-4C17.155 14 22 15.34 22 18v1.375c0 .345-.28.625-.625.625H8.08a.625.625 0 0 1-.625-.625V18z" fill-rule="evenodd"></path></svg></span>邀请回答</button><div class="QuestionHeader-Comment"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>添加评论</button></div><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="Popover12-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover12-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><div class="Popover"><button aria-label="更多" type="button" id="Popover13-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover13-content" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div><div class="QuestionHeader-actions"></div></div></div></div></div><div><div><div class="Sticky is-fixed" style="width: 1395px; top: 52px; left: 0px;"></div><div class="Sticky--holder" style="position: static; top: auto; right: auto; bottom: auto; left: auto; display: block; float: none; margin: 0px; height: 0px;"></div></div></div></div><div class="Question-main"><div class="Question-mainColumn"><div><div id="QuestionAnswers-answers" class="QuestionAnswers-answers" data-zop-feedlistmap="0,0,1,0" data-za-detail-view-path-module="ContentList" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><div class="Card AnswersNavWrapper"><div class="ListShortcut"><div class="List"><div class="List-header"><h4 class="List-headerText"><span>19 个回答</span></h4><div class="List-headerOptions"><div class="Popover"><button role="combobox" aria-expanded="false" type="button" id="Popover14-toggle" aria-haspopup="true" aria-owns="Popover14-content" class="Button Select-button Select-plainButton Button--plain">默认排序<span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Select Select-arrow" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M12 16.183l2.716-2.966a.757.757 0 0 1 1.064.001.738.738 0 0 1 0 1.052l-3.247 3.512a.758.758 0 0 1-1.064 0L8.22 14.27a.738.738 0 0 1 0-1.052.758.758 0 0 1 1.063 0L12 16.183zm0-9.365L9.284 9.782a.758.758 0 0 1-1.064 0 .738.738 0 0 1 0-1.052l3.248-3.512a.758.758 0 0 1 1.065 0L15.78 8.73a.738.738 0 0 1 0 1.052.757.757 0 0 1-1.063.001L12 6.818z" fill-rule="evenodd"></path></svg></span></button></div></div></div><div><div class=""><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="0" data-zop="{&quot;authorName&quot;:&quot;牛阿&quot;,&quot;itemId&quot;:123230350,&quot;title&quot;:&quot;如何用简单例子讲解 Q - learning 的具体过程？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="123230350" itemprop="acceptedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="0" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;123230350&quot;,&quot;upvote_num&quot;:702,&quot;comment_num&quot;:57,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;26408259&quot;,&quot;author_member_hash_id&quot;:&quot;88e1fdec320a09a31dcb2af3b72ef207&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="牛阿"><meta itemprop="image" content="https://pic1.zhimg.com/v2-f5e42994bff8137b8097655644e47b2d_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/Enhuiz"><meta itemprop="zhihu:followerCount" content="172"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover15-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover15-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/Enhuiz"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/v2-f5e42994bff8137b8097655644e47b2d_xs.jpg" srcset="https://pic1.zhimg.com/v2-f5e42994bff8137b8097655644e47b2d_l.jpg 2x" alt="牛阿"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover16-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover16-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/Enhuiz">牛阿</a></div></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button type="button" class="Button Button--plain">702 人赞同了该回答</button></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="702"><meta itemprop="url" content="https://www.zhihu.com/question/26408259/answer/123230350"><meta itemprop="dateCreated" content="2016-09-22T15:51:37.000Z"><meta itemprop="dateModified" content="2018-11-30T18:00:04.000Z"><meta itemprop="commentCount" content="57"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text" ecommerce="[object Object]"><p>2018年12月1日修改： <span><span class="UserLink"><div class="Popover"><div id="Popover33-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover33-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/b9cafd08ffbb359cfa313ab80e158a82" data-za-detail-view-id="1045">@Rahn</a></div></div></span></span> 的答案中提出了一种剪枝方法，可以加快模型的收敛。</p><p>2017年06月05日修改：最近重写了一遍代码，<a href="https://link.zhihu.com/?target=https%3A//enhuiz.github.io/flappybird-ql/" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">Flappy Bird Q-learning</a>。你可以在这里试着训练，想最大帧数下，一两分钟内就可以达到10+的分数。</p><hr><h2><b>问题分析</b> </h2><p>我们可以通过<b>强化学习</b>（reinforcement learning）来解决小鸟怎么飞这个问题。强化学习中有<b>状态</b>（state）、<b>动作</b>（action）、<b>奖赏</b>（reward）这三个要素。智能体（Agent，指小鸟）会根据当前状态来采取动作，并记录被反馈的奖赏，以便下次再到相同状态时能采取更优的动作。</p><p><b>状态的选择</b><br>在这个问题中，最直观的状态提取方法就是以游戏每一帧的画面为状态。但为了简化问题，这里我将选用<a href="https://link.zhihu.com/?target=https%3A//github.com/SarvagyaVaish/FlappyBirdRL" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">SarvagyaVaish</a>项目中提出的状态表达：取小鸟到下一组管子的水平距离和垂直距离差作为小鸟的状态。更准确地说， <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation" alt="[公式]" eeimg="1" data-formula="\Delta x "> 与<img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(1)" alt="[公式]" eeimg="1" data-formula="\Delta y ">的定义如下图所示：</p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/50/v2-6ccc74c071fd10520ad4190080447bee_hd.jpg" data-size="normal" data-rawwidth="302" data-rawheight="582" data-default-watermark-src="https://pic2.zhimg.com/50/v2-ca8ed340cc8bbada28cf3f2952288469_hd.jpg" class="content_image" width="302"/></noscript><img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/v2-6ccc74c071fd10520ad4190080447bee_hd.jpg" data-size="normal" data-rawwidth="302" data-rawheight="582" data-default-watermark-src="https://pic2.zhimg.com/50/v2-ca8ed340cc8bbada28cf3f2952288469_hd.jpg" class="content_image lazy" width="302" data-actualsrc="https://pic2.zhimg.com/50/v2-6ccc74c071fd10520ad4190080447bee_hd.jpg"><figcaption>（图片来自Flappy Bird RL by SarvagyaVaish）</figcaption></figure><p>对于每一个状态<img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(2)" alt="[公式]" eeimg="1" data-formula="(\Delta x,\Delta y)">，<img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(3)" alt="[公式]" eeimg="1" data-formula="\Delta x">为水平距离，<img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(4)" alt="[公式]" eeimg="1" data-formula="\Delta y">为垂直距离。</p><p><b>动作的选择</b><br>每一帧，小鸟只有两种动作可选：1.向上飞一下。2.什么都不做。</p><p><b>奖赏的选择</b><br>小鸟活着时，每一帧给予1的奖赏；若死亡，则给予-1000的奖赏。</p><h2><b>关于Q</b> </h2><p>提到Q-learning，我们需要先了解Q的含义。</p><p><b>Q</b>为<b>动作效用函数</b>（action-utility function），用于评价在特定状态下采取某个动作的优劣。它是<b>智能体的记忆</b>。</p><p>在这个问题中， 状态和动作的组合是有限的。所以我们可以把<b>Q</b>当做是一张表格。表中的每一行记录了状态<img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(5)" alt="[公式]" eeimg="1" data-formula="(\Delta x, \Delta y)">，选择不同动作（飞或不飞）时的奖赏：</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/50/v2-7378f0165d6e13b78778dbed00de6ada_hd.jpg" data-caption="" data-size="normal" data-rawwidth="328" data-rawheight="374" data-default-watermark-src="https://pic1.zhimg.com/50/v2-c4db20f142eeed8a814e1ab9dd423761_hd.jpg" class="content_image" width="328"/></noscript><img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/v2-7378f0165d6e13b78778dbed00de6ada_hd.jpg" data-caption="" data-size="normal" data-rawwidth="328" data-rawheight="374" data-default-watermark-src="https://pic1.zhimg.com/50/v2-c4db20f142eeed8a814e1ab9dd423761_hd.jpg" class="content_image lazy" width="328" data-actualsrc="https://pic4.zhimg.com/50/v2-7378f0165d6e13b78778dbed00de6ada_hd.jpg"></figure><p>这张表一共 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(6)" alt="[公式]" eeimg="1" data-formula="m \times n"> 行，表示 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(6)" alt="[公式]" eeimg="1" data-formula="m \times n"> 个状态，每个状态所对应的动作都有一个<b>效用值</b>。</p><p>理想状态下，在完成训练后，我们会获得一张完美的<b>Q</b>表格。我们希望只要小鸟根据当前位置查找到对应的行，选择<b>效用值较大</b>的动作作为当前帧的动作，就可以无限地存活。</p><h2><b>训练</b> </h2><p>下面的伪代码说明了我们如何训练，来得到一个尽量完美的Q表格。</p><div class="highlight"><pre><code class="language-text">初始化 Q = {};
while Q 未收敛：
    初始化小鸟的位置S，开始新一轮游戏
    while S != 死亡状态：
        使用策略π，获得动作a=π(S) 
        使用动作a进行游戏，获得小鸟的新位置S',与奖励R(S,a)
        Q[S,A] ← (1-α)*Q[S,A] + α*(R(S,a) + γ* max Q[S',a]) // 更新Q
        S ← S'</code></pre></div><p>其中有些值得注意的地方：</p><p>1. 使用策略π，获得动作a=π(S)</p><p>最直观易懂的策略π(S)是根据<b>Q</b>表格来选择效用最大的动作（若两个动作效用值一样，如初始时某位置处效用值都为0，那就选第一个动作）。</p><p>但这样的选择可能会使<b>Q</b>陷入局部最优：在位置 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(7)" alt="[公式]" eeimg="1" data-formula="S_0"> 处，在第一次选择了动作1（飞）并获取了 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(8)" alt="[公式]" eeimg="1" data-formula="r_1 &gt; 0"> 的奖赏后，算法将永远无法对动作2（不飞）进行更新，即使动作2最终会给出 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(9)" alt="[公式]" eeimg="1" data-formula="r_2 &gt; r_1"> 的奖赏。</p><p>改进的策略为<b>ε-greedy</b>方法：每个状态以<b>ε的概率进行探索</b>，此时将随机选取飞或不飞，而<b>剩下的1-ε的概率则进行开发</b>，即按上述方法，选取当前状态下效用值较大的动作。</p><p>2.更新<b>Q</b>表格</p><p><b>Q</b>表格将根据以下公式进行更新：</p><p><img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(10)" alt="[公式]" eeimg="1" data-formula="Q(S,A) \leftarrow (1-\alpha)Q(S,A) + \alpha[R(S, a) + \gamma\max\limits_aQ(S&#39;, a)]"></p><p>其中<b>α</b>为<b>学习速率</b>（learning rate），<b>γ</b>为<b>折扣因子</b>（discount factor）。根据公式可以看出，学习速率α越大，保留之前训练的效果就越少。折扣因子γ越大，<img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(11)" alt="[公式]" eeimg="1" data-formula="max_aQ(S&#39;, a)">所起到的作用就越大。但<img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(11)" alt="[公式]" eeimg="1" data-formula="max_aQ(S&#39;, a)">指什么呢？</p><p>考虑小鸟在对状态进行更新时，会关心到<b>眼前利益</b>（ <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(12)" alt="[公式]" eeimg="1" data-formula="R"> ），和<b>记忆中的利益</b>（<img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(13)" alt="[公式]" eeimg="1" data-formula="\max\limits_aQ(S&#39;, a)">）。</p><p><img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(11)" alt="[公式]" eeimg="1" data-formula="max_aQ(S&#39;, a)">是<b>记忆中的利益</b>。它是小鸟记忆里，新位置<img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(14)" alt="[公式]" eeimg="1" data-formula="S&#39;">能给出的最大效用值。如果小鸟在过去的游戏中于位置<img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(14)" alt="[公式]" eeimg="1" data-formula="S&#39;">的某个动作上吃过甜头（例如选择了某个动作之后获得了50的奖赏），这个公式就可以让它提早地得知这个消息，以便使下回再通过位置<img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(15)" alt="[公式]" eeimg="1" data-formula="S">时选择正确的动作继续进入这个吃甜头的位置<img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(14)" alt="[公式]" eeimg="1" data-formula="S&#39;">。</p><p><b>可以看出，γ越大，小鸟就会越重视以往经验，越小，小鸟只重视眼前利益（R）。</b></p><p>根据上面的伪代码，便可以写出Q-learning的代码了。</p><h2><b>成果</b> </h2><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/50/5a852507fabd96902e9939dd33571746_hd.jpg" data-size="normal" data-rawwidth="323" data-rawheight="567" class="content_image" width="323"/></noscript><img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/5a852507fabd96902e9939dd33571746_hd.jpg" data-size="normal" data-rawwidth="323" data-rawheight="567" class="content_image lazy" width="323" data-actualsrc="https://pic1.zhimg.com/50/5a852507fabd96902e9939dd33571746_hd.jpg"><figcaption>训练后的小鸟一直挂在那里可以飞到几千分。</figcaption></figure></span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/26408259/answer/123230350"><span data-tooltip="发布于 2016-09-22 16:51">编辑于 2018-11-30</span></a></div></div><div class="ContentItem-actions RichContent-actions"><span><button aria-label="赞同 702" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 702</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>57 条评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover38-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover38-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button><button data-zop-retract-question="true" type="button" class="Button ContentItem-action ContentItem-rightButton Button--plain"><span class="RichContent-collapsedText">收起</span><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--ArrowDown ContentItem-arrowIcon is-active" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M12 13L8.285 9.218a.758.758 0 0 0-1.064 0 .738.738 0 0 0 0 1.052l4.249 4.512a.758.758 0 0 0 1.064 0l4.246-4.512a.738.738 0 0 0 0-1.052.757.757 0 0 0-1.063 0L12.002 13z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="1" data-zop="{&quot;authorName&quot;:&quot;YE Y&quot;,&quot;itemId&quot;:86915117,&quot;title&quot;:&quot;如何用简单例子讲解 Q - learning 的具体过程？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="86915117" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="1" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;86915117&quot;,&quot;upvote_num&quot;:124,&quot;comment_num&quot;:10,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;26408259&quot;,&quot;author_member_hash_id&quot;:&quot;661cd655d168712a683ed76b5e5d1d03&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="YE Y"><meta itemprop="image" content="https://pic1.zhimg.com/v2-e3036b85796de307726b34472d0d126c_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/frombeijingwithlove"><meta itemprop="zhihu:followerCount" content="16216"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover18-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover18-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/frombeijingwithlove"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/v2-e3036b85796de307726b34472d0d126c_xs.jpg" srcset="https://pic1.zhimg.com/v2-e3036b85796de307726b34472d0d126c_l.jpg 2x" alt="YE Y"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover19-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover19-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/frombeijingwithlove">YE Y</a></div></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button type="button" class="Button Button--plain">124 人赞同了该回答</button></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="124"><meta itemprop="url" content="https://www.zhihu.com/question/26408259/answer/86915117"><meta itemprop="dateCreated" content="2016-02-18T09:45:34.000Z"><meta itemprop="dateModified" content="2016-02-18T09:45:34.000Z"><meta itemprop="commentCount" content="10"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text" ecommerce="[object Object]">个人看过的最简单的讲解Q-Learning过程的例子：<br><a href="https://link.zhihu.com/?target=http%3A//mnemstudio.org/path-finding-q-learning-tutorial.htm" class=" external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043"><span class="invisible">http://</span><span class="visible">mnemstudio.org/path-fin</span><span class="invisible">ding-q-learning-tutorial.htm</span><span class="ellipsis"></span></a><br>还有中文版翻译：<br><a href="https://link.zhihu.com/?target=http%3A//blog.csdn.net/itplus/article/details/9361915" class=" external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043"><span class="invisible">http://</span><span class="visible">blog.csdn.net/itplus/ar</span><span class="invisible">ticle/details/9361915</span><span class="ellipsis"></span></a></span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/26408259/answer/86915117"><span data-tooltip="发布于 2016-02-18 09:45">发布于 2016-02-18</span></a></div></div><div class="ContentItem-actions"><span><button aria-label="赞同 124" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 124</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>10 条评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover36-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover36-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button></div></div></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="2" data-zop="{&quot;authorName&quot;:&quot;覃含章&quot;,&quot;itemId&quot;:577272724,&quot;title&quot;:&quot;如何用简单例子讲解 Q - learning 的具体过程？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="577272724" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="2" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;577272724&quot;,&quot;upvote_num&quot;:36,&quot;comment_num&quot;:7,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;26408259&quot;,&quot;author_member_hash_id&quot;:&quot;866e63341ae3873b7a4ce0390767dc74&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="覃含章"><meta itemprop="image" content="https://pic4.zhimg.com/v2-8d60cb90a307d11d3daeaaff0dd051fa_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/qinhanzhang"><meta itemprop="zhihu:followerCount" content="15933"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover24-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover24-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/qinhanzhang"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/v2-8d60cb90a307d11d3daeaaff0dd051fa_xs.jpg" srcset="https://pic4.zhimg.com/v2-8d60cb90a307d11d3daeaaff0dd051fa_l.jpg 2x" alt="覃含章"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover25-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover25-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/qinhanzhang">覃含章</a></div></div><a class="UserLink-badge" data-tooltip="优秀回答者 · 已认证的个人" href="https://www.zhihu.com/question/48510028" target="_blank"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--BadgeCG" fill="currentColor" viewBox="0 0 24 24" width="18" height="18"><g fill="none" fill-rule="evenodd"><path fill="#0F88EB" d="M12.055 3.172c.21-.165.397-.344.555-.532l-.127.152c.891-1.065 2.319-1.056 3.195.027l-.125-.153c.872 1.08 2.696 1.856 4.083 1.733l-.197.017c1.383-.122 2.386.893 2.239 2.279l.021-.198c-.147 1.381.593 3.218 1.661 4.113l-.152-.127c1.065.891 1.056 2.319-.027 3.195l.154-.125c-1.08.872-1.856 2.696-1.734 4.084l-.017-.197c.123 1.382-.893 2.385-2.279 2.238l.198.021c-1.38-.147-3.218.593-4.113 1.661l.127-.152c-.891 1.065-2.319 1.057-3.195-.027l.125.154a3.716 3.716 0 0 0-.503-.506c.975-.77 2.422-1.25 3.559-1.13l-.198-.021c1.386.147 2.402-.856 2.279-2.238l.017.197c-.122-1.388.655-3.212 1.734-4.084l-.154.125c1.083-.876 1.092-2.304.027-3.195l.152.127c-1.068-.895-1.808-2.732-1.66-4.113l-.022.198c.147-1.386-.856-2.4-2.24-2.279l.198-.017c-1.159.103-2.622-.422-3.58-1.227z"></path><path fill="#FF9500" d="M19.21 10.483l.151.127c-1.069-.895-1.809-2.732-1.662-4.113l-.02.197c.146-1.386-.857-2.4-2.24-2.278l.197-.018c-1.387.124-3.21-.653-4.083-1.732l.125.153c-.876-1.083-2.304-1.092-3.195-.028l.127-.152c-.894 1.068-2.733 1.808-4.113 1.663l.198.02c-1.386-.147-2.4.857-2.279 2.24L2.4 6.363c.122 1.387-.655 3.21-1.734 4.084l.154-.126c-1.083.877-1.092 2.304-.027 3.194L.64 13.39c1.068.894 1.808 2.733 1.661 4.112l.021-.196c-.147 1.385.856 2.401 2.24 2.28l-.198.015c1.387-.122 3.211.655 4.083 1.734l-.124-.154c.184.228.396.397.62.53a1.89 1.89 0 0 0 1.972 0c.215-.127.421-.287.602-.503l-.127.152c.895-1.068 2.733-1.808 4.113-1.66l-.198-.022c1.387.147 2.402-.856 2.28-2.238l.016.197c-.122-1.389.655-3.212 1.734-4.084l-.154.124c1.083-.876 1.092-2.303.028-3.194"></path><path fill="#FFF" d="M14.946 11.082l-2.362 2.024.721 3.025c.128.534-.144.738-.617.45l-2.654-1.623L7.38 16.58c-.468.286-.746.09-.617-.449l.721-3.025-2.362-2.024c-.417-.357-.317-.68.236-.726l3.101-.248 1.194-2.872c.211-.507.55-.512.763 0l1.195 2.872 3.1.248c.547.044.657.365.236.726"></path></g></svg></span></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText"><span><span><a href="https://www.zhihu.com/people/qinhanzhang/creations/19559450">机器学习</a> </span>话题</span>的优秀回答者</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button type="button" class="Button Button--plain">36 人赞同了该回答</button></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="36"><meta itemprop="url" content="https://www.zhihu.com/question/26408259/answer/577272724"><meta itemprop="dateCreated" content="2019-01-19T07:26:16.000Z"><meta itemprop="dateModified" content="2019-01-23T17:39:21.000Z"><meta itemprop="commentCount" content="7"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text" ecommerce="[object Object]"><p>本回答来自我的知乎专栏上的文章。主要说一下Q-learning中运用的upper confidence bound/置信度的思想。主要的参考文献为：</p><p>Jin, C., Allen-Zhu, Z., Bubeck, S. and Jordan, M.I., 2018. "Is Q-learning provably efficient?."<i>Advances in Neural Information Processing Systems</i>. 2018.</p><hr><h2><b>一、Bandit算法回顾，MDP简单介绍</b></h2><p>首先，我们指出在bandit算法中主要有两类流行的算法，一类是贪心算法（如uniform exploration， <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(16)" alt="[公式]" eeimg="1" data-formula="\epsilon"> -greedy算法），还有一类是基于upper confidence bound的UCB算法。他们二者的具体差别在哪里呢？简单来说，就是他们二者对exploration和exploitation的分配思路不太一样。对这方面不太熟悉的同学，可见：</p><a target="_blank" href="https://zhuanlan.zhihu.com/p/53595666" data-draft-node="block" data-draft-type="link-card" data-image="https://pic3.zhimg.com/v2-6707412e90fd874e86b6a0b88c69fdfa_180x120.jpg" data-image-width="1280" data-image-height="720" class="LinkCard LinkCard--hasImage" data-za-detail-view-id="172"><span class="LinkCard-backdrop" style="background-image:url(https://pic3.zhimg.com/v2-6707412e90fd874e86b6a0b88c69fdfa_180x120.jpg)"></span><span class="LinkCard-content"><span class="LinkCard-text"><span class="LinkCard-title" data-text="true">覃含章：在线学习(MAB)与强化学习(RL)[2]：IID Bandit的一些算法</span><span class="LinkCard-meta"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--InsertLink" fill="currentColor" viewBox="0 0 24 24" width="17" height="17"><path d="M6.77 17.23c-.905-.904-.94-2.333-.08-3.193l3.059-3.06-1.192-1.19-3.059 3.058c-1.489 1.489-1.427 3.954.138 5.519s4.03 1.627 5.519.138l3.059-3.059-1.192-1.192-3.059 3.06c-.86.86-2.289.824-3.193-.08zm3.016-8.673l1.192 1.192 3.059-3.06c.86-.86 2.289-.824 3.193.08.905.905.94 2.334.08 3.194l-3.059 3.06 1.192 1.19 3.059-3.058c1.489-1.489 1.427-3.954-.138-5.519s-4.03-1.627-5.519-.138L9.786 8.557zm-1.023 6.68c.33.33.863.343 1.177.029l5.34-5.34c.314-.314.3-.846-.03-1.176-.33-.33-.862-.344-1.176-.03l-5.34 5.34c-.314.314-.3.846.03 1.177z" fill-rule="evenodd"></path></svg></span>zhuanlan.zhihu.com</span></span><span class="LinkCard-imageCell"><img class="LinkCard-image LinkCard-image--horizontal" alt="图标" src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/v2-6707412e90fd874e86b6a0b88c69fdfa_180x120.jpg"></span></span></a><p>话又说回来，在经典bandit算法当中，我们要做的无非就是通过一类pull arm的策略，尽快找到比较好的“arm”（reward较高的arm），然后尽可能多的去拉这些比较好的arm就是了。贪心算法无非就是永远以当前对每个arm的reward的估计直接作为依据，而UCB算法则是考虑了置信度的问题，因此考虑的是每个arm reward的置信区间的上界。</p><p>那么这个思想推广到更一般的马尔科夫决策过程问题中，可不可以呢？答案自然也是可以的。具体来说，我们考虑一般的episodic Markov decision process， <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(17)" alt="[公式]" eeimg="1" data-formula="\text{MDP}(\mathcal{S},\mathcal{A},H,\mathbb{P},r)"> 。其中 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(18)" alt="[公式]" eeimg="1" data-formula="\mathcal{S}"> 是所有状态（state）的集合，且状态数量有限， <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(19)" alt="[公式]" eeimg="1" data-formula="S:=|\mathcal{S}|."><img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(20)" alt="[公式]" eeimg="1" data-formula="\mathcal{A}"> 则是所有行动（action）的集合，行动数量也有限， <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(21)" alt="[公式]" eeimg="1" data-formula="A:=|\mathcal{A}|"> ，<img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(22)" alt="[公式]" eeimg="1" data-formula="H"> 则是每个episode中MDP状态转移的次数（因此这个MDP可以认为是finite horizon的）。 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(23)" alt="[公式]" eeimg="1" data-formula="\mathbb{P}"> 就是我们熟知的状态转移矩阵（注意因为这里我们的状态和行动空间都是有限的，所以这个矩阵的维度也是有限的），即我们可以定义 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(24)" alt="[公式]" eeimg="1" data-formula="\mathbb{P}_h(\cdot|x,a)"> 来表示在 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(25)" alt="[公式]" eeimg="1" data-formula="h\in [H]"> 步中选择action <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(26)" alt="[公式]" eeimg="1" data-formula="a"> 和处在state <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(27)" alt="[公式]" eeimg="1" data-formula="x"> 情况下转移到其它state的分布。类似的，我们定义 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(28)" alt="[公式]" eeimg="1" data-formula="r_h:\mathcal{S}\times \mathcal{A}\rightarrow [0,1]"> 为一个确定性的reward函数（与 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(29)" alt="[公式]" eeimg="1" data-formula="h,s,a"> 有关）。</p><p>注意，我们这边考虑的是一个episodic MDP，也就是说这个MDP允许被重复很多个episodes。这也很好理解，比如我们如果要训练AlphaGo，那么每个episode就可以看作一局棋，我们势必要重复下很多盘棋才可能对一个比较好的下棋“策略”（policy）有所了解，而如果只下一盘或者很少盘数的棋，我们可能都没有办法对于怎么下棋这件事情，和不同的下棋策略赢的几率，有一个好的exploration。</p><p>那么我们就发现有两个核心的概念：</p><ol><li>策略：policy的概念。在MDP中，我们定义一个policy <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(30)" alt="[公式]" eeimg="1" data-formula="\pi"> 是 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(22)" alt="[公式]" eeimg="1" data-formula="H"> 个函数的集合： <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(31)" alt="[公式]" eeimg="1" data-formula="\{\pi_h:\mathcal{S}\rightarrow  \mathcal{A}\}_{h\in [H]}"> 。也就是说，在每一步 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(32)" alt="[公式]" eeimg="1" data-formula="h"> ,在不同的state下我们应该选择哪个action。</li><li>价值函数：value function的概念。这里我们需要定义著名的 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(33)" alt="[公式]" eeimg="1" data-formula="V">  function和 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(34)" alt="[公式]" eeimg="1" data-formula="Q">  function。我们称 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(35)" alt="[公式]" eeimg="1" data-formula="V_h^{\pi}(x)"> 这个函数是在step <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(32)" alt="[公式]" eeimg="1" data-formula="h"> 和state <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(27)" alt="[公式]" eeimg="1" data-formula="x"> 上，根据policy <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(30)" alt="[公式]" eeimg="1" data-formula="\pi"> 所能获得的期望剩余reward。也就是说， <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(36)" alt="[公式]" eeimg="1" data-formula="V_h^{\pi}(x):=\mathbb{E}\left[  \sum_{h&#39;=h}^H r_{h&#39;}(x_{h&#39;},\pi_{h&#39;}(x_{h&#39;})) |x_h=x \right]"> 。至于 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(34)" alt="[公式]" eeimg="1" data-formula="Q"> function，它和 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(33)" alt="[公式]" eeimg="1" data-formula="V"> function主要的区别在于多一个自变量 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(26)" alt="[公式]" eeimg="1" data-formula="a"> ，即，我们称 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(37)" alt="[公式]" eeimg="1" data-formula="Q_h^{\pi}(x,a)"> 这个函数是在step <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(32)" alt="[公式]" eeimg="1" data-formula="h"> 和state <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(27)" alt="[公式]" eeimg="1" data-formula="x"> 上选择了action <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(26)" alt="[公式]" eeimg="1" data-formula="a"> 之后，根据policy <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(30)" alt="[公式]" eeimg="1" data-formula="\pi"> 所能获得的期望剩余reward。如果你对动态规划（dynamic programming）不熟悉，你可能会问，为什么要定义两个看起来很像的函数。一方面来说，这样子我们很方便就可以写出DP里著名的Bellman equation（或者说optimal induction，直接可以得到最优的policy的表达式）；另一方面来说， 我们也可以说 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(33)" alt="[公式]" eeimg="1" data-formula="V"> function就可以刻画DP本身的逻辑。而 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(34)" alt="[公式]" eeimg="1" data-formula="Q"> function则可以刻画我们实际算法做 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(34)" alt="[公式]" eeimg="1" data-formula="Q"> learning的作用对象。</li></ol><p>本节最后说明，在我们的epsiode MDP setting，每个episode一开始，我们可以不失一般性地认为 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(38)" alt="[公式]" eeimg="1" data-formula="x_1"> 是被某个adversary任意挑选（arbitrarily picked）的。</p><hr><h2><b>二、基于UCB算法的Q-learning</b></h2><p>我们注意到，因为只是考虑有限的状态空间和行动空间，所以最优policy是一定存在的（这是DP的经典结果）。那么用 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(33)" alt="[公式]" eeimg="1" data-formula="V"> function我们就可以刻画最优的policy <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(39)" alt="[公式]" eeimg="1" data-formula="\pi^*"> :即让 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(40)" alt="[公式]" eeimg="1" data-formula="V_h^*(x)=\sup_{\pi} V_h^{\pi}(x)"> 对所有 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(41)" alt="[公式]" eeimg="1" data-formula="x\in \mathcal{S},h\in [H]"> 的函数。因此，根据DP的最优迭代公式（Bellman equation），我们知道如下迭代成立：</p><p><img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(42)" alt="[公式]" eeimg="1" data-formula="\left\{\begin{array}{l} &amp; V_h^*(x) = \max\limits_{a\in \mathcal{A}}Q_h^*(x,a),\forall~x\in \mathcal{S},h\in [H], \\ &amp; Q_h^*(x,a) = (r_h+\mathbb{P}_h V_{h+1}^* )(x,a),\forall~x\in \mathcal{S},a\in \mathcal{A},h\in [H],\\ &amp; V_{H+1}^*(x)=0,\forall~x\in \mathcal{S}. \end{array}   \right."></p><p>注意这里我们用了简化的notation： <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(43)" alt="[公式]" eeimg="1" data-formula="[\mathbb{P}_h V_{h+1}](x,a):=\mathbb{E}_{x&#39;\sim \mathbb{P}(\cdot|x,a)}V_{h+1}(x&#39;)."> 如果你不熟悉DP，我们稍微来看一下这个迭代意味着什么。简单来说，核心是迭代的第二步，即最优的 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(34)" alt="[公式]" eeimg="1" data-formula="Q"> 函数的值应该是当前的reward <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(44)" alt="[公式]" eeimg="1" data-formula="r_h"> ，加上最优的 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(33)" alt="[公式]" eeimg="1" data-formula="V"> 函数从 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(45)" alt="[公式]" eeimg="1" data-formula="h+1"> 步开始的值（这个在DP里面也叫做cost-to-go）。这其实也是DP所谓的tail-optimality。结合迭代的第一步说了最优的 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(33)" alt="[公式]" eeimg="1" data-formula="V"> 函数值应该是 最优的 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(34)" alt="[公式]" eeimg="1" data-formula="Q"> 函数取得最好的action的值，即从后往前推，你在step <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(32)" alt="[公式]" eeimg="1" data-formula="h"> 的时候，最优的policy就是要最大化 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(34)" alt="[公式]" eeimg="1" data-formula="Q"> 函数的值，也就是要最大化当前的reward加上从 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(45)" alt="[公式]" eeimg="1" data-formula="h+1"> 步开始最优policy所带来的期望reward。这点，希望不熟悉DP的同学们好好体会。</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/50/v2-85a1c171703a581ee480e6e9537d494a_hd.jpg" data-caption="" data-size="normal" data-rawwidth="250" data-rawheight="137" class="content_image" width="250"/></noscript><span><div class="VagueImage content_image" data-src="https://pic3.zhimg.com/80/v2-85a1c171703a581ee480e6e9537d494a_hd.jpg" style="width: 250px; height: 137px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p>说明这一点更简单的例子见上图，即所谓的最短路（shortest path）问题。这个问题显然也可以看成是episodic MDP，在每个episode我们都想找到这个固定的图上的最短路，假设每两个节点之间我们事先不知道要花多少时间，而它符合某些随机变量的分布。那么，这个问题的最优policy显然就是满足tail optimality的，这是因为，如果我找到了一套走最短路的方法，那么很显然，即使我走到了途中某个节点，比如B，那么我从B走到F的最短路一定可以通过B-&gt;C + C-&gt;F的最短路（最优policy给出） 或者 B-&gt;D + D-&gt;F的最短路（最优policy给出）来比较得到。</p><p>那么说到这里，Q-learning的想法也就呼之欲出了。因为我们知道，如果我们能知道 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(46)" alt="[公式]" eeimg="1" data-formula="r_h,\mathbb{P}_h"> 的具体分布，我们马上就能求得 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(47)" alt="[公式]" eeimg="1" data-formula="Q^*,V^*"> ，也就能求出最优的policy。当然，实际情况中我们规定我们事先并不知道 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(46)" alt="[公式]" eeimg="1" data-formula="r_h,\mathbb{P}_h"> 的分布，我们能做的，只有在每个episode中的每步step，根据当前的state选择适当的action，并且根据我们选择的action随机（根据 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(48)" alt="[公式]" eeimg="1" data-formula="\mathbb{P}_h"> ）跳到下个state，并且观察到对应的reward的一个sample。那么，一个很自然的想法，当我们收集了越来越多这些sample之后，我们就可以比较好的去估计这个 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(34)" alt="[公式]" eeimg="1" data-formula="Q"> 函数，估计好了这个 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(34)" alt="[公式]" eeimg="1" data-formula="Q"> 函数之后，我们的policy其实就是去最大化这个估计的 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(34)" alt="[公式]" eeimg="1" data-formula="Q"> 函数。嗯，思路其实就这么简单。</p><p>所以核心问题就是如何去估计这个 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(34)" alt="[公式]" eeimg="1" data-formula="Q"> 函数。如果沿用 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(16)" alt="[公式]" eeimg="1" data-formula="\epsilon"> -greedy，自然意思就是直接用sample mean（对每个 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(49)" alt="[公式]" eeimg="1" data-formula="x,a"> 来说）。而如果要用UCB的话，我们就需要对 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(34)" alt="[公式]" eeimg="1" data-formula="Q"> 函数求置信区间。这个自然就比bandit里面求置信区间要复杂了，因为 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(34)" alt="[公式]" eeimg="1" data-formula="Q"> 现在可不是什么IID的随机变量了，而是跟MDP有关的一个更加复杂的随机变量。好在Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, Michael Jordan这几个人最近确定了准确的形式把UCB的思想套用了过来（见开头的参考文献）。</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/50/v2-6f912680fde2648dcedef448e2f62c93_hd.jpg" data-caption="" data-size="normal" data-rawwidth="1095" data-rawheight="372" class="origin_image zh-lightbox-thumb" width="1095" data-original="https://pic1.zhimg.com/v2-6f912680fde2648dcedef448e2f62c93_r.jpg"/></noscript><span><div class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic1.zhimg.com/80/v2-6f912680fde2648dcedef448e2f62c93_hd.jpg" style="width: 654px; height: 222.181px;"><div class="VagueImage-mask is-active"></div></div></span></figure><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/50/v2-99bdfb07d3d2138c482b9aaa741c726a_hd.jpg" data-caption="" data-size="normal" data-rawwidth="1112" data-rawheight="541" class="origin_image zh-lightbox-thumb" width="1112" data-original="https://pic4.zhimg.com/v2-99bdfb07d3d2138c482b9aaa741c726a_r.jpg"/></noscript><span><div class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic4.zhimg.com/80/v2-99bdfb07d3d2138c482b9aaa741c726a_hd.jpg" style="width: 654px; height: 318.178px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p>他们的算法有两个形式，但其实大同小异，区别在于通过不同的集中不等式所需要的不同形式的upper confidence bound。简单来说，Bernstein不等式相比Hoeffding不等式还有对二阶矩的精细控制（即Algorithm 2中的 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(50)" alt="[公式]" eeimg="1" data-formula="\sigma_h"> 项），所以他们的Algorithm 2比起Algorithm 1能得到更好的regret bound。</p><p>不过我们这里因为只是谈一下大概的思想，所以后面只会具体提及Algorithm 1相关的分析思路。毕竟大的思想其实是一样的。这边我们看Algorithm 1，思路就如前面所说，是很直接的，算法中第7行就是如何对Q函数进行估计，一共有两项组成，第一项可以看成是优化算法中的momentum项，第二项就是核心的UCB bound迭代。这里我们注意到置信区间的长度，忽略log项，大概是 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(51)" alt="[公式]" eeimg="1" data-formula="\tilde{O}\left(\sqrt{H^3/t}\right)"> 的， <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(52)" alt="[公式]" eeimg="1" data-formula="t"> 是 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(53)" alt="[公式]" eeimg="1" data-formula="(x_h,a_h)"> 目前有的sample数量。那么当然，这边 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(54)" alt="[公式]" eeimg="1" data-formula="\alpha_t"> 步长的选取也是非常关键的。几位大佬机智地发现把步长取成 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(55)" alt="[公式]" eeimg="1" data-formula="\alpha_t=O(H/t)"> ，而非传统一阶算法里常用的 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(56)" alt="[公式]" eeimg="1" data-formula="\alpha_t=1/t"> 就能得到满意的结果，意思就是单纯的uniform weight是不行的，而要给比较近的项更高的权重才可以。</p><hr><h2><b>三、一些分析</b></h2><p>好了，本节我们就稍微具体看下Algorithm 1为什么可以work。尤其比如说步长为什么要选成 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(57)" alt="[公式]" eeimg="1" data-formula="O(H/t)"> 。当然这边也不可能照搬所有细节，所以想学完整proof的同学还是建议直接去翻原paper。</p><p>这边先重申下regret的定义（其实是expected regret,或者按照上次的说法是pseudo regret），很简单，就是</p><p><img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(58)" alt="[公式]" eeimg="1" data-formula="\text{Regret}(K) = \sum_{k=1}^K [V_1^*(x_1^k)-V_1^{\pi_k}(x_1^k)]."></p><p>其中 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(59)" alt="[公式]" eeimg="1" data-formula="K"> 是episode的总数， <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(60)" alt="[公式]" eeimg="1" data-formula="\pi_k"> 就是我们算法在episode <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(61)" alt="[公式]" eeimg="1" data-formula="k"> 所用的policy， <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(62)" alt="[公式]" eeimg="1" data-formula="x_1^k"> 就是每个episode <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(61)" alt="[公式]" eeimg="1" data-formula="k"> 最开始初始的state。</p><p>这边需要用一个额外的notation来说。我们让 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(63)" alt="[公式]" eeimg="1" data-formula="(x_h^k,a_h^k)"> 表示在episode <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(61)" alt="[公式]" eeimg="1" data-formula="k"> 的step <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(32)" alt="[公式]" eeimg="1" data-formula="h"> 我们实际观察到的state和选择的action。然后如果我们让 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(64)" alt="[公式]" eeimg="1" data-formula="Q_h^k,V_h^k"> 表示episode <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(61)" alt="[公式]" eeimg="1" data-formula="k"> 刚开始的 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(65)" alt="[公式]" eeimg="1" data-formula="Q_h,V_h"> 函数估计。我们就注意到Algorithm 1的第七行其实可以写成：</p><p><img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(66)" alt="[公式]" eeimg="1" data-formula="Q_h^{k+1}(x,a)=\left\{\begin{array}{ll} (1-\alpha_t)Q_h^k(x,a)+\alpha_t(r_h(x,a)+V_{h+1}^k(x_{h+1}^k)+b_t) &amp; \text{if } (x,a)=(x_h^k,a_h^k)\\ Q_h^k(x,a) &amp; \text{otherwise} \end{array} \right.."></p><p>这个式子其实更能反应算法的本质：这也是Q-learning利用多个episode学习的关系式。对于分析来说，我们就希望能说明利用合适的步长 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(54)" alt="[公式]" eeimg="1" data-formula="\alpha_t"> ,我们这边估计出来的 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(67)" alt="[公式]" eeimg="1" data-formula="Q_h^k"> 相比 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(68)" alt="[公式]" eeimg="1" data-formula="Q_h^*"> 它的error不会累积地过于快。</p><p>对于步长，我们再定义这样一些量：</p><p><img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(69)" alt="[公式]" eeimg="1" data-formula="\alpha_t^0=\Pi_{j=1}^t(1-\alpha_j),\alpha_t^i=\alpha_i\Pi_{j={i+1}}^t(1-\alpha_j)"> ,</p><p>那么我们可以进一步把上面的式子展开，得到</p><p><img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(70)" alt="[公式]" eeimg="1" data-formula="Q_h^k(x,a)=\alpha_t^0H+\sum_{i=1}^t\alpha_t^i\left[ r_h(x,a)+V_{h+1}^{k_i}(x_{h+1}) +b_i \right]"> .</p><p>这边我们令 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(52)" alt="[公式]" eeimg="1" data-formula="t"> 到第k个episode为止，在每个episode的第h个step，观测到 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(71)" alt="[公式]" eeimg="1" data-formula="(x,a)"> 的次数， <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(72)" alt="[公式]" eeimg="1" data-formula="k_1,k_2,\ldots,k_t"> 则是之前在step <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(32)" alt="[公式]" eeimg="1" data-formula="h"> 选择action <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(26)" alt="[公式]" eeimg="1" data-formula="a"> 和处在state <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(27)" alt="[公式]" eeimg="1" data-formula="x"> 的episodes。那么我们就发现，其实是这个量 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(73)" alt="[公式]" eeimg="1" data-formula="\alpha_t^i"> （当然它由 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(54)" alt="[公式]" eeimg="1" data-formula="\alpha_t"> 决定）来反应我们的Q-learning算法（Algorithm 1）对之前的UCB bound的权重分配。</p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/50/v2-5e941130c8feb808997c4928c0193f8b_hd.jpg" data-caption="" data-size="normal" data-rawwidth="1037" data-rawheight="450" class="origin_image zh-lightbox-thumb" width="1037" data-original="https://pic2.zhimg.com/v2-5e941130c8feb808997c4928c0193f8b_r.jpg"/></noscript><span><div class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic2.zhimg.com/80/v2-5e941130c8feb808997c4928c0193f8b_hd.jpg" style="width: 654px; height: 283.799px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p>那么这边就有张图，画出了取不同 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(54)" alt="[公式]" eeimg="1" data-formula="\alpha_t"> 我们的 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(73)" alt="[公式]" eeimg="1" data-formula="\alpha_t^i"> 随次数增长的曲线。我们注意到相比我们最终选取的 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(74)" alt="[公式]" eeimg="1" data-formula="\alpha_t=\frac{H+1}{H+t}"> ， <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(75)" alt="[公式]" eeimg="1" data-formula="1/t"> 的步长完全是uniform分配的，不偏不倚，而 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(76)" alt="[公式]" eeimg="1" data-formula="1/\sqrt{t}"> 的步长则基本把所有权重都分配给了最近的15%左右的sample（对于分析来说，这个步长会导致很大的variance，导致过高的regret）。这么来看，我们选取的步长倒是显得比较中规中矩了~</p><p>具体来说，我们容易验证当 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(74)" alt="[公式]" eeimg="1" data-formula="\alpha_t=\frac{H+1}{H+t}"> ，我们有 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(77)" alt="[公式]" eeimg="1" data-formula="\sum_{t=i}^{\infty}\alpha_t^i=1+1/H"> 对所有正整数 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(78)" alt="[公式]" eeimg="1" data-formula="i"> 成立。而这也是每个episode我们能累计regret的阶，也就是说总共在这种步长下我们的regret累计最多也就是 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(79)" alt="[公式]" eeimg="1" data-formula="(1+1/H)^H"> 的阶，而这算是个常数了。</p><p>那么之后的analysis都基于前面的式子，简单来说我们需要能够用 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(80)" alt="[公式]" eeimg="1" data-formula="V_h^k-V_h^*"> 来bound住 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(81)" alt="[公式]" eeimg="1" data-formula="Q_h-Q_h^*"> ，然后利用DP的迭代我们就能推出 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(80)" alt="[公式]" eeimg="1" data-formula="V_h^k-V_h^*"> ，也就是regret的bound。具体的步骤其实也不算长，当然也不是说就可以一蹴而就，有兴趣的同学们可以自己先尝试推推看，然后再去看大佬在paper里的推法。</p><p>忽略log项，Algorithm 1可以得到 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(82)" alt="[公式]" eeimg="1" data-formula="\tilde{O}\left(\sqrt{H^4SAT}\right)"> 的regret bound（ <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(83)" alt="[公式]" eeimg="1" data-formula="T=KH"> 是总共的步数），而Algorithm 2因为用了更精细的集中不等式，可以改进到 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(84)" alt="[公式]" eeimg="1" data-formula="\tilde{O}\left(\sqrt{H^3SAT}\right)"> 的regret bound。</p><p>之后再提一下他们得到的其它一些结果。主要还有两个：</p><p>1.他们构造了一个MDP的例子并说明 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(85)" alt="[公式]" eeimg="1" data-formula="\epsilon-"> greedy算法在这个例子上的regret至少是exponential的(exponential in <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(22)" alt="[公式]" eeimg="1" data-formula="H"> )。这确实就说明在更一般的情形下<img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(85)" alt="[公式]" eeimg="1" data-formula="\epsilon-"> greedy和UCB算法在理论上的表现被进一步拉开了。。</p><p>2.他们也给了一个一般情形的lower bound，对任意算法，存在一个 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(22)" alt="[公式]" eeimg="1" data-formula="H"> epsiode的MDP，使得算法的regret至少是 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(86)" alt="[公式]" eeimg="1" data-formula="\Omega\left( H\sqrt{SAT}  \right)"> 的。所以我们看到他们的Algorithm 2只和这个lower bound差了 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(87)" alt="[公式]" eeimg="1" data-formula="\sqrt{H}"> ，已经非常接近了。接下来就看谁能把这个gap close了，呵呵。</p><p>最后多说一句，其实bandit文献里面经常需要自己去”造“一些特殊的不等式然后算法配合之达到比较好的regret bound，这往往都是因为只控制一阶矩不够精细，需要引入二阶矩的一些精确估计才ok。当然可能有些人会觉得这些纯属是在玩bound，不过在bandit领域里应该都算是共识了。。。</p></span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/26408259/answer/577272724"><span data-tooltip="发布于 2019-01-19 07:26">编辑于 2019-01-23</span></a></div></div><div><div class="ContentItem-actions Sticky RichContent-actions is-fixed is-bottom" style="width: 694px; bottom: 0px; left: 197.5px;"><span><button aria-label="赞同 36" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 36</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>7 条评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover37-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover37-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button><button data-zop-retract-question="true" type="button" class="Button ContentItem-action ContentItem-rightButton Button--plain"><span class="RichContent-collapsedText">收起</span><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--ArrowDown ContentItem-arrowIcon is-active" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M12 13L8.285 9.218a.758.758 0 0 0-1.064 0 .738.738 0 0 0 0 1.052l4.249 4.512a.758.758 0 0 0 1.064 0l4.246-4.512a.738.738 0 0 0 0-1.052.757.757 0 0 0-1.063 0L12.002 13z" fill-rule="evenodd"></path></svg></span></button></div><div class="Sticky--holder" style="position: static; top: auto; right: auto; bottom: auto; left: auto; display: flex; float: none; margin: 0px -20px -10px; height: 54px; width: 694px;"></div></div></div></div></div><div class="Pc-word"><div class="Pc-word-card"><a target="_blank" href="http://www.togocareer.com/haiguiqiuzhi.html?sourcezhihu&amp;id=PC-1120"><div class="Pc-word-card-brand"><div class="Pc-word-card-brand-wrapper"><img width="20" height="20" src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/v2-b02efb32af2aada63f09e5d0a275df75_250x250.jpeg" alt="logo"><span>Togocareer</span></div></div></a><div class="Pc-word-card-sign"><div class="Pc-word-card-sign-label">广告​<svg class="Icon Icon--triangle Pc-word-card-sign-svg" viewBox="0 0 24 24"><path d="M9.218 16.78a.737.737 0 0 0 1.052 0l4.512-4.249a.758.758 0 0 0 0-1.063L10.27 7.22a.737.737 0 0 0-1.052 0 .759.759 0 0 0-.001 1.063L13 12l-3.782 3.716a.758.758 0 0 0 0 1.063z" fill-rule="evenodd"></path></svg></div><div class="Pc-word-card-sign-popup Pc-word-card-sign-popup--isHidden"><span class="Pc-word-card-sign-popup-arrow"></span><div class="Pc-word-card-sign-popup-menu"><button type="button">不感兴趣</button><a target="_blank" rel="noopener noreferrer" href="https://www.zhihu.com/promotion-intro">知乎广告介绍</a></div></div></div><a target="_blank" href="http://www.togocareer.com/haiguiqiuzhi.html?sourcezhihu&amp;id=PC-1120"><h2 class="Pc-word-card-title">作为应届留学生，回国后通过哪些途径找工作的？各行业招聘情况如何</h2><div class="Pc-word-card-content  "><span>近期热门招聘公司：金融，咨询，五百强外资，互联网等各大行业，从零到卓越，全流程专业化求职，一站式服务留学生。</span><span class="Pc-word-card-content-cta  ">查看详情</span></div></a></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="4" data-zop="{&quot;authorName&quot;:&quot;貌似有道理&quot;,&quot;itemId&quot;:88120604,&quot;title&quot;:&quot;如何用简单例子讲解 Q - learning 的具体过程？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="88120604" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="4" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;88120604&quot;,&quot;upvote_num&quot;:96,&quot;comment_num&quot;:16,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;26408259&quot;,&quot;author_member_hash_id&quot;:&quot;6eb095526bca675193c7fe78e44351ed&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="貌似有道理"><meta itemprop="image" content="https://pic3.zhimg.com/v2-f24b2dd4ba64f45bad21882ca5be2efe_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/lee-manners"><meta itemprop="zhihu:followerCount" content="2909"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover27-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover27-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/lee-manners"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/v2-f24b2dd4ba64f45bad21882ca5be2efe_xs.jpg" srcset="https://pic3.zhimg.com/v2-f24b2dd4ba64f45bad21882ca5be2efe_l.jpg 2x" alt="貌似有道理"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover28-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover28-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/lee-manners">貌似有道理</a></div></div><a class="UserLink-badge" data-tooltip="已认证的个人" href="https://www.zhihu.com/question/48510028" target="_blank"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--BadgeCert" fill="currentColor" viewBox="0 0 24 24" width="18" height="18"><g fill="none" fill-rule="evenodd"><path fill="#0F88EB" d="M2.64 13.39c1.068.895 1.808 2.733 1.66 4.113l.022-.196c-.147 1.384.856 2.4 2.24 2.278l-.198.016c1.387-.122 3.21.655 4.083 1.734l-.125-.154c.876 1.084 2.304 1.092 3.195.027l-.127.152c.895-1.068 2.733-1.808 4.113-1.66l-.198-.022c1.386.147 2.402-.856 2.279-2.238l.017.197c-.122-1.388.655-3.212 1.734-4.084l-.154.125c1.083-.876 1.092-2.304.027-3.195l.152.127c-1.068-.895-1.808-2.732-1.66-4.113l-.022.198c.147-1.386-.856-2.4-2.24-2.279l.198-.017c-1.387.123-3.21-.654-4.083-1.733l.125.153c-.876-1.083-2.304-1.092-3.195-.027l.127-.152c-.895 1.068-2.733 1.808-4.113 1.662l.198.02c-1.386-.147-2.4.857-2.279 2.24L4.4 6.363c.122 1.387-.655 3.21-1.734 4.084l.154-.126c-1.083.878-1.092 2.304-.027 3.195l-.152-.127z"></path><path fill="#FFF" d="M9.78 15.728l-2.633-2.999s-.458-.705.242-1.362c.7-.657 1.328-.219 1.328-.219l1.953 2.132 4.696-4.931s.663-.348 1.299.198c.636.545.27 1.382.27 1.382s-3.466 3.858-5.376 5.782c-.98.93-1.778.017-1.778.017z"></path></g></svg></span></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText">约克大学 机器学习博士在读</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button type="button" class="Button Button--plain">96 人赞同了该回答</button></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="96"><meta itemprop="url" content="https://www.zhihu.com/question/26408259/answer/88120604"><meta itemprop="dateCreated" content="2016-02-25T19:04:45.000Z"><meta itemprop="dateModified" content="2016-02-25T19:04:45.000Z"><meta itemprop="commentCount" content="16"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text" ecommerce="[object Object]"><p>Q-learning 的估计本质是用Q(s,a)平均值来估计Q(s,a)。 如果把所有的数据收集全了，求平均值（那是MC方法） 还有一个好办法， 没多一条数据，更新一下平均值。这个就是Q-learning的思想来源。</p><figure><noscript><img src="https://pic3.zhimg.com/50/1ffd8c2a1d18022d6e6a629c417bf6f5_hd.jpg" data-rawwidth="524" data-rawheight="349" class="origin_image zh-lightbox-thumb" width="524" data-original="https://pic3.zhimg.com/1ffd8c2a1d18022d6e6a629c417bf6f5_r.jpg"/></noscript><span><div class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic3.zhimg.com/80/1ffd8c2a1d18022d6e6a629c417bf6f5_hd.jpg" style="width: 524px; height: 349px;"><div class="VagueImage-mask is-active"></div></div></span></figure><br>根据Q估计，就可以使用<br>策略做为policy. Agent 走的越多，得到样本越多，更新更多，Q-value估值就接近真实值。最后<b><u>依概率收敛到最优policy</u></b></span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/26408259/answer/88120604"><span data-tooltip="发布于 2016-02-25 19:04">发布于 2016-02-25</span></a></div></div><div class="ContentItem-actions RichContent-actions"><span><button aria-label="赞同 96" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 96</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>16 条评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover29-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover29-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button><button data-zop-retract-question="true" type="button" class="Button ContentItem-action ContentItem-rightButton Button--plain"><span class="RichContent-collapsedText">收起</span><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--ArrowDown ContentItem-arrowIcon is-active" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M12 13L8.285 9.218a.758.758 0 0 0-1.064 0 .738.738 0 0 0 0 1.052l4.249 4.512a.758.758 0 0 0 1.064 0l4.246-4.512a.738.738 0 0 0 0-1.052.757.757 0 0 0-1.063 0L12.002 13z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div class="List-item" tabindex="0"><div class="ContentItem AnswerItem" data-za-index="5" data-zop="{&quot;authorName&quot;:&quot;囧Bourne&quot;,&quot;itemId&quot;:467132543,&quot;title&quot;:&quot;如何用简单例子讲解 Q - learning 的具体过程？&quot;,&quot;type&quot;:&quot;answer&quot;}" name="467132543" itemprop="suggestedAnswer" itemtype="http://schema.org/Answer" itemscope="" data-za-detail-view-path-module="AnswerItem" data-za-detail-view-path-index="5" data-za-extra-module="{&quot;card&quot;:{&quot;has_image&quot;:false,&quot;has_video&quot;:false,&quot;content&quot;:{&quot;type&quot;:&quot;Answer&quot;,&quot;token&quot;:&quot;467132543&quot;,&quot;upvote_num&quot;:53,&quot;comment_num&quot;:1,&quot;publish_timestamp&quot;:null,&quot;parent_token&quot;:&quot;26408259&quot;,&quot;author_member_hash_id&quot;:&quot;9ecb1c10ea59e37946b80597cef515f4&quot;}}}"><div class="ContentItem-meta"><div class="AuthorInfo AnswerItem-authorInfo AnswerItem-authorInfo--related" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="囧Bourne"><meta itemprop="image" content="https://pic2.zhimg.com/v2-3ca8a9ff2ab03fabdc3c478721a41d16_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/jiong-jiong-shi"><meta itemprop="zhihu:followerCount" content="245"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover30-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover30-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/jiong-jiong-shi"><img class="Avatar AuthorInfo-avatar" width="38" height="38" src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/v2-3ca8a9ff2ab03fabdc3c478721a41d16_xs.jpg" srcset="https://pic2.zhimg.com/v2-3ca8a9ff2ab03fabdc3c478721a41d16_l.jpg 2x" alt="囧Bourne"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover31-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover31-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/jiong-jiong-shi">囧Bourne</a></div></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="ztext AuthorInfo-badgeText">Human-Robot Coexistence\ 人机共融、机器人与机器学习</div></div></div></div></div><div class="AnswerItem-extraInfo"><span class="Voters"><button type="button" class="Button Button--plain">53 人赞同了该回答</button></span></div></div><meta itemprop="image"><meta itemprop="upvoteCount" content="53"><meta itemprop="url" content="https://www.zhihu.com/question/26408259/answer/467132543"><meta itemprop="dateCreated" content="2018-08-12T08:08:20.000Z"><meta itemprop="dateModified" content="2018-08-27T03:18:30.000Z"><meta itemprop="commentCount" content="1"><div class="RichContent RichContent--unescapable"><div class="RichContent-inner"><span class="RichText ztext CopyrightRichText-richText" itemprop="text" ecommerce="[object Object]"><p>为了理清强化学习中最经典、最基础的算法——Q-learning，根据ADEPT的学习规律（Analogy / Diagram / Example / Plain / Technical Definition），本文努力用直观理解、数学方法、图形表达、简单例子和文字解释来展现其精髓之处。<b>区别于众多Q-learning讲解中的伪代码流程图，本文将提供可视化的算法流程图</b>帮助大家学习、对比Q-learning与Sarsa。</p><p>( 在此假设大家已了解TD Method， ε-greedy policy，off-policy 和 on-policy相关知识。想了解的童鞋也可在本文最后Reference或链接中学习） </p><h2>一、直观理解</h2><p>任何强化学习的问题都需要兼顾探索（Exploration）和利用（Exploitation），因此TD方法也对应有两种经典的衍生算法：Q-learning（off-policy）和Sarsa（on-policy）。基于off-policy思想的Q-learning，与Monte Carlo 方法中off-policy的灵魂思路是一致的，除了更新价值的步数不一样之外。在此引用笔者之前回答中关于off-policy的一个比喻。</p><div class="highlight"><pre><code class="language-text">     古时候，优秀的皇帝都秉持着“水能载舟 亦能覆舟”的思想，希望能多了解民间百姓的生活。皇帝可以选择通过微服出巡，亲自下凡了解百姓生活（On-policy），虽然眼见为实，但毕竟皇帝本人分身乏术，掌握情况不全；因此也可以派多个官员去了解情况，而皇帝本人则躺在酒池肉林里收听百官情报即可（Off-policy）。
（坏皇帝则派出“锦衣卫”_(´ཀ`」 ∠)_）</code></pre></div><p>不清楚off-policy的同学可以点击以下传送门：</p><a target="_blank" href="https://www.zhihu.com/question/57159315/answer/465865135" data-draft-node="block" data-draft-type="link-card" data-image="https://pic2.zhimg.com/v2-978881b6ca688d54e38afe7d04dd4f25_180x120.jpg" data-image-width="2702" data-image-height="1606" class="LinkCard LinkCard--hasImage"><span class="LinkCard-backdrop" style="background-image:url(https://pic2.zhimg.com/v2-978881b6ca688d54e38afe7d04dd4f25_180x120.jpg)"></span><span class="LinkCard-content"><span class="LinkCard-text"><span class="LinkCard-title" data-text="true">强化学习中on-policy 与off-policy有什么区别？</span><span class="LinkCard-meta"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--InsertLink" fill="currentColor" viewBox="0 0 24 24" width="17" height="17"><path d="M6.77 17.23c-.905-.904-.94-2.333-.08-3.193l3.059-3.06-1.192-1.19-3.059 3.058c-1.489 1.489-1.427 3.954.138 5.519s4.03 1.627 5.519.138l3.059-3.059-1.192-1.192-3.059 3.06c-.86.86-2.289.824-3.193-.08zm3.016-8.673l1.192 1.192 3.059-3.06c.86-.86 2.289-.824 3.193.08.905.905.94 2.334.08 3.194l-3.059 3.06 1.192 1.19 3.059-3.058c1.489-1.489 1.427-3.954-.138-5.519s-4.03-1.627-5.519-.138L9.786 8.557zm-1.023 6.68c.33.33.863.343 1.177.029l5.34-5.34c.314-.314.3-.846-.03-1.176-.33-.33-.862-.344-1.176-.03l-5.34 5.34c-.314.314-.3.846.03 1.177z" fill-rule="evenodd"></path></svg></span>www.zhihu.com</span></span><span class="LinkCard-imageCell"><img class="LinkCard-image LinkCard-image--horizontal" alt="图标" src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/v2-978881b6ca688d54e38afe7d04dd4f25_180x120.jpg"></span></span></a><p><b>疑难点在于：</b>对于Q-learning（off-policy），我们用来产生与环境互动的行为策略，既然其产生的样本数据是用来训练目标策略的，那为什么学习策略可以在某一程度上独立于行为策略呢？这就是一个细节处理问题，因此下面将一步一步剖析这种独立是如何产生。</p><h2>二、算法流程</h2><p>本文首先放出两张一目了然的流程图： Q-learning和Sarsa，为了可以直接于可视化流程图的比较之中领悟这两种方法的思路和差别。  </p><p><b>1. Sarsa</b></p><figure data-size="small"><noscript><img src="https://pic2.zhimg.com/50/v2-5aebc163f1e575aecfad9e4d1e4ce06a_hd.jpg" data-caption="" data-size="small" data-rawwidth="963" data-rawheight="1037" data-default-watermark-src="https://pic1.zhimg.com/50/v2-b19ecbf6c0f069f601d1275ea5835a5d_hd.jpg" class="origin_image zh-lightbox-thumb" width="963" data-original="https://pic2.zhimg.com/v2-5aebc163f1e575aecfad9e4d1e4ce06a_r.jpg"/></noscript><span><div class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic2.zhimg.com/80/v2-5aebc163f1e575aecfad9e4d1e4ce06a_hd.jpg" style="width: 261.594px; height: 281.696px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p>（已了解Sarsa的同学也不要轻易跳过，或者对比过后，你会有新的发现）</p><p>1.1)    一个回合（Episode）开始，随机选择（初始化）第一个状态<img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(88)" alt="[公式]" eeimg="1" data-formula="S_1">。并基于 ε-greedy策略在状态<img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(88)" alt="[公式]" eeimg="1" data-formula="S_1">中选择动作，有两种情况，一是有 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(89)" alt="[公式]" eeimg="1" data-formula="(1-ε)"> 的概率直接选择具有最大值Q的动作，二是有 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(90)" alt="[公式]" eeimg="1" data-formula="ε"> 概率随机选择 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(88)" alt="[公式]" eeimg="1" data-formula="S_1"> 下的任意动作（在第二种情况下每个动作的概率均为 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(91)" alt="[公式]" eeimg="1" data-formula="ε/|A_1|"> ,其中 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(92)" alt="[公式]" eeimg="1" data-formula="|A_1|"> 为 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(88)" alt="[公式]" eeimg="1" data-formula="S_1"> 下的动作总个数）。</p><p>1.2)    进入第一次循环(Repeat 1 / Step 1)：执行 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(93)" alt="[公式]" eeimg="1" data-formula="A_1"> 之后（与环境互动），观察下一个状态 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(94)" alt="[公式]" eeimg="1" data-formula="S_2"> ，并马上得到 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(94)" alt="[公式]" eeimg="1" data-formula="S_2"> 的即时回报 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(95)" alt="[公式]" eeimg="1" data-formula="R_2">。<b>此时，再次基于 ε-greedy策略，在状态</b><img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(94)" alt="[公式]" eeimg="1" data-formula="S_2"><b>中选择动作</b><img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(96)" alt="[公式]" eeimg="1" data-formula="A_2"><b>。得到 </b><img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(96)" alt="[公式]" eeimg="1" data-formula="A_2"><b> 后，即可进行Q函数的更新（Update），更新中的 </b><img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(97)" alt="[公式]" eeimg="1" data-formula="Q_k(S_2,A_2)"><b> 为ε-greedy策略下所随机选取的动作，这是与Q-learning的不同之处！</b>（下标 k 或 i 表示最近一次更新的Q值，是一个迭代序数而非时间步（step）序数，在此可先忽略。）</p><p>1.3)    不断循环第二步，直到终止状态。</p><p class="ztext-empty-paragraph"><br></p><p><b>2. Q-learning</b></p><figure data-size="small"><noscript><img src="https://pic4.zhimg.com/50/v2-db6c94e4a630b382b28d5bed3b096dc4_hd.jpg" data-caption="" data-size="small" data-rawwidth="977" data-rawheight="1059" data-default-watermark-src="https://pic4.zhimg.com/50/v2-5b256c09644635e5d86159c37b7b013b_hd.jpg" class="origin_image zh-lightbox-thumb" width="977" data-original="https://pic4.zhimg.com/v2-db6c94e4a630b382b28d5bed3b096dc4_r.jpg"/></noscript><span><div class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic4.zhimg.com/80/v2-db6c94e4a630b382b28d5bed3b096dc4_hd.jpg" style="width: 261.594px; height: 283.55px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p>2.1)    一个回合（Episode）开始，随机选择（初始化）第一个状态<img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(88)" alt="[公式]" eeimg="1" data-formula="S_1">。</p><p>2.2)    进入第一次循环（Repeat 1 / Step 1）：首先基于 ε-greedy策略在状态<img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(88)" alt="[公式]" eeimg="1" data-formula="S_1">中选择动作。选择并执行 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(93)" alt="[公式]" eeimg="1" data-formula="A_1"> 之后（与环境互动），观察下一个状态 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(94)" alt="[公式]" eeimg="1" data-formula="S_2"> ，并得到 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(94)" alt="[公式]" eeimg="1" data-formula="S_2"> 的即时回报 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(95)" alt="[公式]" eeimg="1" data-formula="R_2">。此时，<b>立即进行Q函数的更新（Update），更新中的 </b><img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(98)" alt="[公式]" eeimg="1" data-formula="max_aQ_k(S_2,a_2)"><b> 为我们人为直接选择 </b><img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(94)" alt="[公式]" eeimg="1" data-formula="S_2"><b> 下所有动作中具有最大Q值的动作，这就是与Saras根本区别！</b></p><p>2.3)    更新完毕后，进入第二次循环：基于 ε-greedy策略，在状态<img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(94)" alt="[公式]" eeimg="1" data-formula="S_2">中选择动作与环境互动（此前在状态<img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(94)" alt="[公式]" eeimg="1" data-formula="S_2">时候并未采取动作与环境互动）。值得注意的是，我们在循环1中更新（Update）时所选取 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(99)" alt="[公式]" eeimg="1" data-formula="S_2 "> 的动作 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(100)" alt="[公式]" eeimg="1" data-formula="a_2"> 是唯一的(人为强制选择)，即最具有最大价值Q的动作<img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(98)" alt="[公式]" eeimg="1" data-formula="max_aQ_k(S_2,a_2)">；而循环2中作为需要与环境互动的第二次动作 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(96)" alt="[公式]" eeimg="1" data-formula="A_2">  则是基于ε-greedy策略（即在此时究竟选取 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(98)" alt="[公式]" eeimg="1" data-formula="max_aQ_k(S_2,a_2)"> 对应的动作 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(100)" alt="[公式]" eeimg="1" data-formula="a_2"> 还是其他动作完全根据是随机选择，听天由命吧 0.0 ）！<b>因此，基于ε-greedy策略，与环境互动、做学习训练时做动作选择的决策（在off-policy中这被称为行为策略）与Sarsa是一致的。</b></p><h2>3. 细节</h2><p>不少学童鞋对这两幅伪码图中的动作符号存疑：为什么动作的表示有时候为大写的A，有时候为小写的a？</p><p><i>（引用R. S. Sutton与A.G. Barto于2018年1月1日发布的《Reinforcement learning: An introduction》第二版）：</i></p><figure data-size="small"><noscript><img src="https://pic2.zhimg.com/50/v2-70b370d3c271f1d643534001396cbcd0_hd.jpg" data-size="small" data-rawwidth="1721" data-rawheight="605" class="origin_image zh-lightbox-thumb" width="1721" data-original="https://pic2.zhimg.com/v2-70b370d3c271f1d643534001396cbcd0_r.jpg"/></noscript><span><div class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic2.zhimg.com/80/v2-70b370d3c271f1d643534001396cbcd0_hd.jpg" style="width: 261.594px; height: 91.9607px;"><div class="VagueImage-mask is-active"></div></div></span><figcaption>Pseudo code of Sarsa</figcaption></figure><figure data-size="small"><noscript><img src="https://pic1.zhimg.com/50/v2-ce61618678907272d040ae9f80794b0d_hd.jpg" data-size="small" data-rawwidth="1707" data-rawheight="556" class="origin_image zh-lightbox-thumb" width="1707" data-original="https://pic1.zhimg.com/v2-ce61618678907272d040ae9f80794b0d_r.jpg"/></noscript><span><div class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic1.zhimg.com/80/v2-ce61618678907272d040ae9f80794b0d_hd.jpg" style="width: 261.594px; height: 85.2058px;"><div class="VagueImage-mask is-active"></div></div></span><figcaption>Pseudo code of Q-learning</figcaption></figure><p>大写的A表示集合，比如 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(93)" alt="[公式]" eeimg="1" data-formula="A_1"> 则表示 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(88)" alt="[公式]" eeimg="1" data-formula="S_1"> 下的所有动作，而 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(101)" alt="[公式]" eeimg="1" data-formula="a_1"> 则表示具体的一个动作，它们之间的关系为：<img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(102)" alt="[公式]" eeimg="1" data-formula="a_1\in A_1">。回到流程图中，可以发现出现a都在Q-learning的update公式中，这是因为我们在更新时，人为指定选择具有最大值Q的a，这是具有确定性的事件（Deterministic）。而在Q-learning中与环境互动的环节、在Sarsa中更新Q值的环节与环境互动的环节时，动作的选择是随机的（ ε-greedy），因此所有动作都有可能被选中，只不过是具有最大值Q的动作<img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(103)" alt="[公式]" eeimg="1" data-formula="a = \left \{ a| max_{a\in A} Q(S,A)\right \}">被选中的概率大。</p><p>此时我们可以清楚知道Sutton书中的伪代码的全部含义啦^_^！</p><h2>三、Q-learning如何实现更加有效的探索？</h2><p>清楚整个流程之后，我们来具体看看，Q-learning到底是怎么实现有意义的探索，如何在环境中发掘出更有价值的动作？（即一个当前估值（evaluate）不高但潜力巨大的动作的逆袭之路）</p><figure data-size="small"><noscript><img src="https://pic4.zhimg.com/50/v2-2d3fc034d95d825950bdd1dfa3bb6ce1_hd.jpg" data-caption="" data-size="small" data-rawwidth="998" data-rawheight="904" class="origin_image zh-lightbox-thumb" width="998" data-original="https://pic4.zhimg.com/v2-2d3fc034d95d825950bdd1dfa3bb6ce1_r.jpg"/></noscript><span><div class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic4.zhimg.com/80/v2-2d3fc034d95d825950bdd1dfa3bb6ce1_hd.jpg" style="width: 261.594px; height: 236.955px;"><div class="VagueImage-mask is-active"></div></div></span></figure><p>在这个例子中，我们将更新黄色状态的动作价值<img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(104)" alt="[公式]" eeimg="1" data-formula="Q(S=yellow,A)">。</p><p class="ztext-empty-paragraph"><br></p><p>图a，假设已知黄色状态下只有两个动作可选：灰动作和黑动作，并且在第k-1次更新 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(104)" alt="[公式]" eeimg="1" data-formula="Q(S=yellow,A)"> 时，（ <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(105)" alt="[公式]" eeimg="1" data-formula="Q_{1}(S_t,a_t)"> 通过人为设置获取，不纳入更新迭代次数k），已知灰动作价值比黑动作大，即有</p><p><img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(106)" alt="[公式]" eeimg="1" data-formula="Q_{m}(yellow,gray)&gt;Q_{n}(yellow,black )"></p><p>其中m,n为对应动作价值已迭代更新的次数，k为 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(107)" alt="[公式]" eeimg="1" data-formula="Q_{k}(yellow,action)"> 更新的次数，所以有m+n= k。</p><p class="ztext-empty-paragraph"><br></p><p>图b，在某个回合（episode）中，在时间步为t的时候（time step = t），所处状态为黄色，又已知灰动作价值比黑动作大，即基于 ε-greedy的行为策略选择动作，会出现情况①或②：</p><p>①  有 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(108)" alt="[公式]" eeimg="1" data-formula="(1- ε )+ ε /2=1- ε /2"> 的可能性选择当前最大价值Q的灰动作 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(109)" alt="[公式]" eeimg="1" data-formula="a_n">：<img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(110)" alt="[公式]" eeimg="1" data-formula="Q_{m+1}(s_t,a_t)=Q_m(s_t,a_t)+\alpha[R_{t+1}+maxQ_i(s_{t+1},A_{t+1})-Q_m(s_t,a_t)]"></p><p>而另一黑动作没有更新。</p><p>其中， <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(111)" alt="[公式]" eeimg="1" data-formula="s_t=yellow"> 。</p><p>②  有 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(112)" alt="[公式]" eeimg="1" data-formula=" ε /2">  的可能性选择当前较小价值Q的黑动作 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(113)" alt="[公式]" eeimg="1" data-formula="a_n&#39;">：</p><p><img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(114)" alt="[公式]" eeimg="1" data-formula="Q_{n+1}(s_t,a_t&#39;)=Q_n(s_t,a_t&#39;)+\alpha[R_{t+1}+maxQ_j(s_{t+1}&#39;,A_{t+1}&#39;)-Q_n(s_t,a_t&#39;)]"></p><p> 而另一灰动作没有更新</p><p>其中， <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(111)" alt="[公式]" eeimg="1" data-formula="s_t=yellow"> 。</p><p class="ztext-empty-paragraph"><br></p><p>在此假设图b发生②的情况，则 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(115)" alt="[公式]" eeimg="1" data-formula="s_{t+1}&#39;=green"> 。</p><p class="ztext-empty-paragraph"><br></p><p>图c通过选取 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(116)" alt="[公式]" eeimg="1" data-formula="green"> 下的最大价值动作 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(117)" alt="[公式]" eeimg="1" data-formula="a_j(red)"> 来更新图f的目标策略。</p><p class="ztext-empty-paragraph"><br></p><p>在第k+1次更新中，我们通过取最大值（即greedy思想），选取Q值最大的动作来更新目标策略 <img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(30)" alt="[公式]" eeimg="1" data-formula="\pi"> （target policy）：</p><p><img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(118)" alt="[公式]" eeimg="1" data-formula="Q_{k+1}^{\pi}(S=yellow,A)=max\left \{Q_{m}(S,gray),Q_{n+1}(S,black)\right \}"></p><p>比如，基于上述已发生的，再发生图f的情况，则在下一次更新<img src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/equation(119)" alt="[公式]" eeimg="1" data-formula="Q_{k+1}^{\pi}(S=yellow,A)">时，黄色状态下的黑动作变为最优动作（颠覆了灰色动作有最大Q值的地位）。</p><p class="ztext-empty-paragraph"><br></p><p>（实际上无论发生情况①或是②，黄色状态下的灰动作与黑动作的价值的大小关系都可能发生变化！！）</p><p class="ztext-empty-paragraph"><br></p><h2>四、另一个栗子</h2><p>在此举一个非常直观的例子来帮助我们认识一下Q-learning和Sarsa的实际应用效果的区别。</p><p>在下面栅格化的小世界中，绿色区域为草地，在上面每移动一格子就会扣1分，而踏入黑色区域的悬崖（chasm），会扣一百分，并且回到起始点S (Start)。我们希望能学习到一条得分最高的路径到达终点T (Terminal)。分别使用Sarsa和Q-learning进行学习。结果如图所示，红色为相应算法的最优路径。</p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/50/v2-28d7363a9a1c00f029ef5ce4fb29e362_hd.jpg" data-size="normal" data-rawwidth="564" data-rawheight="685" class="origin_image zh-lightbox-thumb" width="564" data-original="https://pic2.zhimg.com/v2-28d7363a9a1c00f029ef5ce4fb29e362_r.jpg"/></noscript><span><div class="VagueImage origin_image zh-lightbox-thumb" data-src="https://pic2.zhimg.com/80/v2-28d7363a9a1c00f029ef5ce4fb29e362_hd.jpg" style="width: 564px; height: 685px;"><div class="VagueImage-mask is-active"></div></div></span><figcaption>Sarsa vs Q-learning</figcaption></figure><p>可以看到，Q-learning寻找到一条全局最优的路径，因为虽然Q-learning的行为策略（behavior）是基于 ε-greedy策略，但其目标策略（target policy）只考虑最优行为；而Sarsa只能找到一条次优路径，这条路径在直观上更加安全，这是因为Sarsa（其目标策略和行为策略为同一策略）考虑了所有动作的可能性（ ε-greedy），当靠近悬崖时，由于会有一定概率选择往悬崖走一步，从而使得这些悬崖边路的价值更低。</p><p class="ztext-empty-paragraph"><br></p><h2>五、总结</h2><p>Q-learning虽然具有学习到全局最优的能力，但是其收敛慢；而Sarsa虽然学习效果不如Q-learning，但是其收敛快，直观简单。因此，对于不同的问题，我们需要有所斟酌。</p><p class="ztext-empty-paragraph"><br></p><h2>Reference</h2><ol><li>Watkins C J C H, Dayan P. Technical Note: Q-Learning[J]. Machine Learning, 8(3-4):279-292, 1992.</li><li>R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. IEEE Transactions on Neural Networks, 9(5):1054–1054, 2018.</li></ol><p><b>声明：本图文未经作者允许，谢绝转载。</b></p><p><a href="https://www.zhihu.com/question/57159315/answer/465865135" class="internal">囧Bourne：强化学习中on-policy 与off-policy有什么区别？</a></p></span></div><div><div class="ContentItem-time"><a target="_blank" href="https://www.zhihu.com/question/26408259/answer/467132543"><span data-tooltip="发布于 2018-08-12 09:08">编辑于 2018-08-27</span></a></div></div><div class="ContentItem-actions RichContent-actions"><span><button aria-label="赞同 53" type="button" class="Button VoteButton VoteButton--up"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 53</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>1 条评论</button><div class="Popover ShareMenu ContentItem-action"><div class="ShareMenu-toggler" id="Popover32-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover32-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>感谢</button><button data-zop-retract-question="true" type="button" class="Button ContentItem-action ContentItem-rightButton Button--plain"><span class="RichContent-collapsedText">收起</span><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--ArrowDown ContentItem-arrowIcon is-active" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M12 13L8.285 9.218a.758.758 0 0 0-1.064 0 .738.738 0 0 0 0 1.052l4.249 4.512a.758.758 0 0 0 1.064 0l4.246-4.512a.738.738 0 0 0 0-1.052.757.757 0 0 0-1.063 0L12.002 13z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div></div></div></div></div></div></div></div></div></div><div class="Question-sideColumn Question-sideColumn--sticky" data-za-detail-view-path-module="RightSideBar" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><div><div class="Sticky is-fixed" style="width: 296px; top: -374px; left: 901.5px;"><div class="Card AppBanner"><a class="AppBanner-link" href="http://zhi.hu/BDXoI"><div class="AppBanner-layout"><img class="AppBanner-qrcode" src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/sidebar-download-qrcode.7caef4dd.png" alt="QR Code of Downloading Zhihu App"><div class="AppBanner-content"><div class="AppBanner-title">下载知乎客户端</div><div class="AppBanner-description">与世界分享知识、经验和见解</div></div></div></a></div><div class="Card"></div><div class="Card" data-za-detail-view-path-module="RelatedQuestions" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><div class="Card-header SimilarQuestions-title"><div class="Card-headerText">相关问题</div></div><div class="Card-section SimilarQuestions-list"><div class="SimilarQuestions-item" itemprop="zhihu:similarQuestion" itemtype="http://schema.org/Question" itemscope="" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;41477987&quot;}}}"><meta itemprop="name" content="MDP马尔可夫决策过程中的值迭代和策略迭代感觉并没有本质区别？"><meta itemprop="url" content="https://www.zhihu.com/question/41477987"><meta itemprop="answerCount" content="10"><meta itemprop="zhihu:followerCount" content="259"><a target="_blank" type="button" class="Button Button--plain" href="https://www.zhihu.com/question/41477987">MDP马尔可夫决策过程中的值迭代和策略迭代感觉并没有本质区别？</a> 10 个回答</div><div class="SimilarQuestions-item" itemprop="zhihu:similarQuestion" itemtype="http://schema.org/Question" itemscope="" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;313863142&quot;}}}"><meta itemprop="name" content="如何评价优化算法 AdaBound?"><meta itemprop="url" content="https://www.zhihu.com/question/313863142"><meta itemprop="answerCount" content="18"><meta itemprop="zhihu:followerCount" content="539"><a target="_blank" type="button" class="Button Button--plain" href="https://www.zhihu.com/question/313863142">如何评价优化算法 AdaBound?</a> 18 个回答</div><div class="SimilarQuestions-item" itemprop="zhihu:similarQuestion" itemtype="http://schema.org/Question" itemscope="" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;56082058&quot;}}}"><meta itemprop="name" content="优化算法的一次迭代可以等价于神经网络的一层吗？"><meta itemprop="url" content="https://www.zhihu.com/question/56082058"><meta itemprop="answerCount" content="7"><meta itemprop="zhihu:followerCount" content="105"><a target="_blank" type="button" class="Button Button--plain" href="https://www.zhihu.com/question/56082058">优化算法的一次迭代可以等价于神经网络的一层吗？</a> 7 个回答</div><div class="SimilarQuestions-item" itemprop="zhihu:similarQuestion" itemtype="http://schema.org/Question" itemscope="" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;264955924&quot;}}}"><meta itemprop="name" content="包含两个部分相减的目标函数有什么好的方法让其中一个部分优先得到优化？"><meta itemprop="url" content="https://www.zhihu.com/question/264955924"><meta itemprop="answerCount" content="10"><meta itemprop="zhihu:followerCount" content="42"><a target="_blank" type="button" class="Button Button--plain" href="https://www.zhihu.com/question/264955924">包含两个部分相减的目标函数有什么好的方法让其中一个部分优先得到优化？</a> 10 个回答</div><div class="SimilarQuestions-item" itemprop="zhihu:similarQuestion" itemtype="http://schema.org/Question" itemscope="" data-za-detail-view-path-module="QuestionItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Question&quot;,&quot;token&quot;:&quot;28168332&quot;}}}"><meta itemprop="name" content="遗传算法能不能优化算法？"><meta itemprop="url" content="https://www.zhihu.com/question/28168332"><meta itemprop="answerCount" content="7"><meta itemprop="zhihu:followerCount" content="161"><a target="_blank" type="button" class="Button Button--plain" href="https://www.zhihu.com/question/28168332">遗传算法能不能优化算法？</a> 7 个回答</div></div></div><div class="Card" data-za-detail-view-path-module="ContentList" data-za-detail-view-path-module_name="相关推荐" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><div class="Card-header RelatedCommodities-title"><div class="Card-headerText">相关推荐</div></div><div class="Card-section RelatedCommodities-list"><a target="_blank" href="https://www.zhihu.com/lives/909108706266988544" type="button" class="Button RelatedCommodities-item Button--plain" data-za-detail-view-path-module="LiveItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Live&quot;,&quot;id&quot;:&quot;909108706266988544&quot;,&quot;author_member_hash_id&quot;:&quot;d98018aa7a13a0464305252d641e42b5&quot;}}}"><img class="RelatedCommodities-image" src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/v2-fffc9efd35336b2f23aa62d7a1e4cafa_250x0.jpg" alt="live"><div class="RelatedCommodities-content"><div class="RelatedCommodities-subject RelatedCommodities-subject-two" data-tooltip="深度学习平台开发实践及未来发展">深度学习平台开发实践及未来发展</div><div class="RelatedCommodities-meta"><div class="RelatedCommodities-scoreWrapper"><div class="Rating"><svg viewBox="0 0 15 15" class="Icon Icon--rating" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#ffab2e" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z">   </path></g></g></svg><svg viewBox="0 0 15 15" class="Icon Icon--rating" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#ffab2e" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z">   </path></g></g></svg><svg viewBox="0 0 15 15" class="Icon Icon--rating" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#ffab2e" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z">   </path></g></g></svg><svg viewBox="0 0 15 15" class="Icon Icon--rating" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#ffab2e" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z">   </path></g></g></svg><svg viewBox="0 0 15 15" class="Icon Icon--rating" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#ffab2e" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z">   </path></g></g></svg></div>473 人参与</div></div></div></a><a target="_blank" href="https://www.zhihu.com/lives/952125519370657792" type="button" class="Button RelatedCommodities-item Button--plain" data-za-detail-view-path-module="LiveItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Live&quot;,&quot;id&quot;:&quot;952125519370657792&quot;,&quot;author_member_hash_id&quot;:&quot;64fb64a673f7ec3df5470e8d368b1cb6&quot;}}}"><img class="RelatedCommodities-image" src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/v2-3cea28d030d574fa2ccc7bbc02dfd8a1_250x0.jpg" alt="live"><div class="RelatedCommodities-content"><div class="RelatedCommodities-subject RelatedCommodities-subject-two" data-tooltip="机器学习与深度学习学习路线入门到精通">机器学习与深度学习学习路线入门到精通</div><div class="RelatedCommodities-meta"><div class="RelatedCommodities-scoreWrapper"><div class="Rating"><svg viewBox="0 0 15 15" class="Icon Icon--rating" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#ffab2e" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z">   </path></g></g></svg><svg viewBox="0 0 15 15" class="Icon Icon--rating" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#ffab2e" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z">   </path></g></g></svg><svg viewBox="0 0 15 15" class="Icon Icon--rating" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#ffab2e" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z">   </path></g></g></svg><svg width="15" height="15" viewBox="0 0 15 15" class="Icon Icon--ratingHalf" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#d7d8d9" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z"></path>   </g></g></svg><svg viewBox="0 0 15 15" class="Icon Icon--ratingNone" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#d7d8d9" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#d7d8d9" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z"></path>   </g></g></svg></div>155 人参与</div></div></div></a><a target="_blank" href="https://www.zhihu.com/lives/967356309683793920" type="button" class="Button RelatedCommodities-item Button--plain" data-za-detail-view-path-module="LiveItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Live&quot;,&quot;id&quot;:&quot;967356309683793920&quot;,&quot;author_member_hash_id&quot;:&quot;64fb64a673f7ec3df5470e8d368b1cb6&quot;}}}"><img class="RelatedCommodities-image" src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/v2-3cea28d030d574fa2ccc7bbc02dfd8a1_250x0.jpg" alt="live"><div class="RelatedCommodities-content"><div class="RelatedCommodities-subject RelatedCommodities-subject-two" data-tooltip="语音识别之循环神经网络RNN与LSTM">语音识别之循环神经网络RNN与LSTM</div><div class="RelatedCommodities-meta"><div class="RelatedCommodities-scoreWrapper"><div class="Rating"><svg viewBox="0 0 15 15" class="Icon Icon--rating" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#ffab2e" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z">   </path></g></g></svg><svg viewBox="0 0 15 15" class="Icon Icon--rating" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#ffab2e" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z">   </path></g></g></svg><svg viewBox="0 0 15 15" class="Icon Icon--rating" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#ffab2e" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z">   </path></g></g></svg><svg width="15" height="15" viewBox="0 0 15 15" class="Icon Icon--ratingHalf" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#d7d8d9" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#ffab2e" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z"></path>   </g></g></svg><svg viewBox="0 0 15 15" class="Icon Icon--ratingNone" width="15" height="15" aria-hidden="true" style="margin-left: 0px; height: 15px; width: 15px;"><title></title><g><g fill-rule="evenodd">     <path fill="#d7d8d9" d="M10.925 14.302c.173.13.408.13.58-.002.174-.13.244-.362.175-.572l-1.323-4.296 3.435-2.456c.175-.13.25-.36.185-.572-.064-.212-.253-.357-.468-.36H9.275L7.96 1.754c-.064-.21-.21-.354-.46-.354-.14 0-1.027 3.53-.988 6.32.04 2.788.98 3.85.98 3.85l3.433 2.732z"></path>     <path fill="#d7d8d9" d="M7.5 1.4c-.25 0-.41.144-.474.354l-1.318 4.29H1.49c-.214.003-.403.148-.467.36-.065.212.01.442.185.572l3.42 2.463-1.307 4.286c-.066.21.004.44.176.572.172.13.407.132.58.003l3.42-2.734L7.5 1.4z"></path>   </g></g></svg></div>93 人参与</div></div></div></a></div></div><div class="Card"></div><footer class="Footer"><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://liukanshan.zhihu.com/">刘看山</a><span class="Footer-dot"></span><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://www.zhihu.com/question/19581624">知乎指南</a><span class="Footer-dot"></span><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://www.zhihu.com/terms">知乎协议</a><span class="Footer-dot"></span><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://www.zhihu.com/term/privacy">知乎隐私保护指引</a><br><a class="Footer-item" target="_blank" href="https://www.zhihu.com/app">应用</a><span class="Footer-dot"></span><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://app.mokahr.com/apply/zhihu">工作</a><span class="Footer-dot"></span><button type="button" class="Button OrgCreateButton">申请开通知乎机构号</button><br><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="https://zhuanlan.zhihu.com/p/51068775">侵权举报</a><span class="Footer-dot"></span><a class="Footer-item" target="_blank" rel="noopener noreferrer" href="http://www.12377.cn/">网上有害信息举报专区</a><br><span class="Footer-item">违法和不良信息举报：010-82716601</span><br><a class="Footer-item" target="_blank" href="https://www.zhihu.com/jubao">儿童色情信息举报专区</a><br><a class="Footer-item" target="_blank" href="https://www.zhihu.com/certificates">证照中心</a><br><a class="Footer-item" target="_blank" href="https://www.zhihu.com/contact">联系我们</a><span> © 2019 知乎</span></footer></div><div class="Sticky--holder" style="position: static; top: auto; right: auto; bottom: auto; left: auto; display: block; float: none; margin: 0px; height: 958px;"></div></div></div></div></div></main><div class="CornerButtons"><div class="CornerAnimayedFlex"><button data-tooltip="建议反馈" data-tooltip-position="left" data-tooltip-will-hide-on-click="true" aria-label="建议反馈" type="button" class="Button CornerButton Button--plain"><svg class="Zi Zi--Feedback" title="建议反馈" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M19.99 6.99L18 5s-1-1-2-1H8C7 4 6 5 6 5L4 7S3 8 3 9v9s0 2 2.002 2H19c2 0 2-2 2-2V9c0-1-1.01-2.01-1.01-2.01zM16.5 5.5L19 8H5l2.5-2.5h9zm-2 5.5s.5 0 .5.5-.5.5-.5.5h-5s-.5 0-.5-.5.5-.5.5-.5h5z"></path></svg></button></div><div class="CornerAnimayedFlex"><button data-tooltip="回到顶部" data-tooltip-position="left" data-tooltip-will-hide-on-click="true" aria-label="回到顶部" type="button" class="Button CornerButton Button--plain"><svg class="Zi Zi--BackToTop" title="回到顶部" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M16.036 19.59a1 1 0 0 1-.997.995H9.032a.996.996 0 0 1-.997-.996v-7.005H5.03c-1.1 0-1.36-.633-.578-1.416L11.33 4.29a1.003 1.003 0 0 1 1.412 0l6.878 6.88c.782.78.523 1.415-.58 1.415h-3.004v7.005z"></path></svg></button></div></div></div></div><script id="js-clientConfig" type="text/json">{"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","zhuanlanHost":"zhuanlan.zhihu.com"}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"question\u002Fget\u002F":false,"question\u002FgetAnswers\u002F26408259":false}},"entities":{"users":{},"questions":{"26408259":{"type":"question","id":26408259,"title":"如何用简单例子讲解 Q - learning 的具体过程？","questionType":"normal","created":1414653732,"updatedTime":1472707763,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F26408259","isMuted":false,"isNormal":true,"isEditable":false,"adminClosedComment":false,"hasPublishingDraft":false,"answerCount":19,"visitCount":147660,"commentCount":0,"followerCount":1337,"collapsedAnswerCount":0,"excerpt":"Q-learning如何在探索和经验之间进行平衡？Q-learning每次迭代都沿当前Q值最高的路径前进吗？","commentPermission":"all","detail":"Q-learning如何在探索和经验之间进行平衡？Q-learning每次迭代都沿当前Q值最高的路径前进吗？","editableDetail":"Q-learning如何在探索和经验之间进行平衡？Q-learning每次迭代都沿当前Q值最高的路径前进吗？","status":{"isLocked":false,"isClose":false,"isEvaluate":false,"isSuggest":false},"relationship":{"isAuthor":false,"isFollowing":false,"isAnonymous":false,"canLock":false,"canStickAnswers":false,"canCollapseAnswers":false},"topics":[{"id":"19553510","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19553510","name":"算法","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002F2e6276881_is.jpg"},{"id":"19559450","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19559450","name":"机器学习","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fd3dd87a0feae0a3db82973157eee89c0_is.jpg"},{"id":"19590194","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19590194","name":"无监督学习","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-5d9ee9aa6bcfd8c3e9187d781baa4d12_is.jpg"},{"id":"19813032","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19813032","name":"深度学习（Deep Learning）","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002F5d3c206139ca2124997418db09b0bb11_is.jpg"},{"id":"20039099","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20039099","name":"强化学习 (Reinforcement Learning)","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-f128c9fb5c0d027bf31d47b9a55756d5_is.jpg"}],"author":{"id":"0","urlToken":"","name":"匿名用户","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Faadd7b895_is.jpg","avatarUrlTemplate":"https:\u002F\u002Fpic1.zhimg.com\u002Faadd7b895_{size}.jpg","isOrg":false,"type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002F0","userType":"people","headline":"","badge":[],"gender":1,"isAdvertiser":false,"followerCount":0,"isFollowing":false,"isFollowed":false,"isCelebrity":false,"isBlocking":false,"isBlocked":false,"isPrivacy":false},"canComment":{"status":true,"reason":""},"reviewInfo":{"type":"","tips":"","editTips":"","isReviewing":false},"relatedCards":[],"muteInfo":{"type":""},"showAuthor":false}},"answers":{"86915117":{"id":86915117,"type":"answer","answerType":"normal","question":{"type":"question","id":26408259,"title":"如何用简单例子讲解 Q - learning 的具体过程？","questionType":"normal","created":1414653732,"updatedTime":1472707763,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F26408259","relationship":{}},"author":{"id":"661cd655d168712a683ed76b5e5d1d03","urlToken":"frombeijingwithlove","name":"YE Y","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e3036b85796de307726b34472d0d126c_is.jpg","avatarUrlTemplate":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-e3036b85796de307726b34472d0d126c_{size}.jpg","isOrg":false,"type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002F661cd655d168712a683ed76b5e5d1d03","userType":"people","headline":"","badge":[],"gender":1,"isAdvertiser":false,"followerCount":16216,"isFollowed":false,"isPrivacy":false},"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fanswers\u002F86915117","isCollapsed":false,"createdTime":1455788734,"updatedTime":1455788734,"extras":"","isCopyable":true,"isNormal":true,"voteupCount":124,"commentCount":10,"isSticky":false,"adminClosedComment":false,"commentPermission":"all","canComment":{"reason":"","status":true},"reshipmentSettings":"allowed","content":"个人看过的最简单的讲解Q-Learning过程的例子：\u003Cbr\u002F\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fmnemstudio.org\u002Fpath-finding-q-learning-tutorial.htm\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttp:\u002F\u002F\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Emnemstudio.org\u002Fpath-fin\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003Eding-q-learning-tutorial.htm\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003Cbr\u002F\u003E还有中文版翻译：\u003Cbr\u002F\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fblog.csdn.net\u002Fitplus\u002Farticle\u002Fdetails\u002F9361915\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttp:\u002F\u002F\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Eblog.csdn.net\u002Fitplus\u002Far\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003Eticle\u002Fdetails\u002F9361915\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E","editableContent":"","excerpt":"个人看过的最简单的讲解Q-Learning过程的例子： http:\u002F\u002Fmnemstudio.org\u002Fpath-finding-q-learning-tutorial.htm 还有中文版翻译： http:\u002F\u002Fblog.csdn.net\u002Fitplus\u002Farticle\u002Fdetails\u002F9361915","collapsedBy":"nobody","collapseReason":"","annotationAction":null,"markInfos":[],"relevantInfo":{"isRelevant":false,"relevantType":"","relevantText":""},"suggestEdit":{"reason":"","status":false,"tip":"","title":"","unnormalDetails":{"status":"","description":"","reason":"","reasonId":0,"note":""},"url":""},"isLabeled":false,"rewardInfo":{"canOpenReward":false,"isRewardable":false,"rewardMemberCount":0,"rewardTotalMoney":0,"tagline":""},"relationship":{"isAuthor":false,"isAuthorized":false,"isNothelp":false,"isThanked":false,"isRecognized":false,"voting":0,"upvotedFollowees":[]}},"88120604":{"id":88120604,"type":"answer","answerType":"normal","question":{"type":"question","id":26408259,"title":"如何用简单例子讲解 Q - learning 的具体过程？","questionType":"normal","created":1414653732,"updatedTime":1472707763,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F26408259","relationship":{}},"author":{"id":"6eb095526bca675193c7fe78e44351ed","urlToken":"lee-manners","name":"貌似有道理","avatarUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-f24b2dd4ba64f45bad21882ca5be2efe_is.jpg","avatarUrlTemplate":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-f24b2dd4ba64f45bad21882ca5be2efe_{size}.jpg","isOrg":false,"type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002F6eb095526bca675193c7fe78e44351ed","userType":"people","headline":"一入AI深似海，妹子从此是路人","badge":[{"type":"identity","description":"约克大学 机器学习博士在读","topics":[]}],"gender":1,"isAdvertiser":false,"followerCount":2909,"isFollowed":false,"isPrivacy":false},"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fanswers\u002F88120604","isCollapsed":false,"createdTime":1456427085,"updatedTime":1456427085,"extras":"","isCopyable":false,"isNormal":true,"voteupCount":96,"commentCount":16,"isSticky":false,"adminClosedComment":false,"commentPermission":"all","canComment":{"reason":"","status":true},"reshipmentSettings":"disallowed","content":"Q-learning 的估计本质是用Q(s,a)平均值来估计Q(s,a)。 如果把所有的数据收集全了，求平均值（那是MC方法） 还有一个好办法， 没多一条数据，更新一下平均值。这个就是Q-learning的思想来源。\u003Cbr\u002F\u003E\u003Cbr\u002F\u003E\u003Cfigure\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002F1ffd8c2a1d18022d6e6a629c417bf6f5_hd.jpg\" data-rawwidth=\"524\" data-rawheight=\"349\" class=\"origin_image zh-lightbox-thumb\" width=\"524\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002F1ffd8c2a1d18022d6e6a629c417bf6f5_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;524&#39; height=&#39;349&#39;&gt;&lt;\u002Fsvg&gt;\" data-rawwidth=\"524\" data-rawheight=\"349\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"524\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002F1ffd8c2a1d18022d6e6a629c417bf6f5_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002F1ffd8c2a1d18022d6e6a629c417bf6f5_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cbr\u002F\u003E根据Q估计，就可以使用\u003Cbr\u002F\u003E策略做为policy. Agent 走的越多，得到样本越多，更新更多，Q-value估值就接近真实值。最后\u003Cb\u003E\u003Cu\u003E依概率收敛到最优policy\u003C\u002Fu\u003E\u003C\u002Fb\u003E","editableContent":"","excerpt":"Q-learning 的估计本质是用Q(s,a)平均值来估计Q(s,a)。 如果把所有的数据收集全了，求平均值（那是MC方法） 还有一个好办法， 没多一条数据，更新一下平均值。这个就是Q-learning的思想来源。 [图片] 根据Q估计，就可以使用 策略做为policy. Agent 走的越多，得到样本越多，更新更多，Q-value估值就接近真实值。最后依概率收敛到最优policy","collapsedBy":"nobody","collapseReason":"","annotationAction":null,"markInfos":[],"relevantInfo":{"isRelevant":false,"relevantType":"","relevantText":""},"suggestEdit":{"reason":"","status":false,"tip":"","title":"","unnormalDetails":{"status":"","description":"","reason":"","reasonId":0,"note":""},"url":""},"isLabeled":false,"rewardInfo":{"canOpenReward":false,"isRewardable":false,"rewardMemberCount":0,"rewardTotalMoney":0,"tagline":""},"relationship":{"isAuthor":false,"isAuthorized":false,"isNothelp":false,"isThanked":false,"isRecognized":false,"voting":0,"upvotedFollowees":[]}},"123230350":{"id":123230350,"type":"answer","answerType":"normal","question":{"type":"question","id":26408259,"title":"如何用简单例子讲解 Q - learning 的具体过程？","questionType":"normal","created":1414653732,"updatedTime":1472707763,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F26408259","relationship":{}},"author":{"id":"88e1fdec320a09a31dcb2af3b72ef207","urlToken":"Enhuiz","name":"牛阿","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f5e42994bff8137b8097655644e47b2d_is.jpg","avatarUrlTemplate":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f5e42994bff8137b8097655644e47b2d_{size}.jpg","isOrg":false,"type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002F88e1fdec320a09a31dcb2af3b72ef207","userType":"people","headline":"","badge":[],"gender":1,"isAdvertiser":false,"followerCount":172,"isFollowed":false,"isPrivacy":false},"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fanswers\u002F123230350","isCollapsed":false,"createdTime":1474559497,"updatedTime":1543600804,"extras":"","isCopyable":true,"isNormal":true,"voteupCount":702,"commentCount":57,"isSticky":false,"adminClosedComment":false,"commentPermission":"all","canComment":{"reason":"","status":true},"reshipmentSettings":"allowed","content":"\u003Cp\u003E2018年12月1日修改： \u003Ca class=\"member_mention\" href=\"https:\u002F\u002Fwww.zhihu.com\u002Fpeople\u002Fb9cafd08ffbb359cfa313ab80e158a82\" data-hash=\"b9cafd08ffbb359cfa313ab80e158a82\" data-hovercard=\"p$b$b9cafd08ffbb359cfa313ab80e158a82\"\u003E@Rahn\u003C\u002Fa\u003E 的答案中提出了一种剪枝方法，可以加快模型的收敛。\u003C\u002Fp\u003E\u003Cp\u003E2017年06月05日修改：最近重写了一遍代码，\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fenhuiz.github.io\u002Fflappybird-ql\u002F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EFlappy Bird Q-learning\u003C\u002Fa\u003E。你可以在这里试着训练，想最大帧数下，一两分钟内就可以达到10+的分数。\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Ch2\u003E\u003Cb\u003E问题分析\u003C\u002Fb\u003E \u003C\u002Fh2\u003E\u003Cp\u003E我们可以通过\u003Cb\u003E强化学习\u003C\u002Fb\u003E（reinforcement learning）来解决小鸟怎么飞这个问题。强化学习中有\u003Cb\u003E状态\u003C\u002Fb\u003E（state）、\u003Cb\u003E动作\u003C\u002Fb\u003E（action）、\u003Cb\u003E奖赏\u003C\u002Fb\u003E（reward）这三个要素。智能体（Agent，指小鸟）会根据当前状态来采取动作，并记录被反馈的奖赏，以便下次再到相同状态时能采取更优的动作。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E状态的选择\u003C\u002Fb\u003E\u003Cbr\u002F\u003E在这个问题中，最直观的状态提取方法就是以游戏每一帧的画面为状态。但为了简化问题，这里我将选用\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fgithub.com\u002FSarvagyaVaish\u002FFlappyBirdRL\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003ESarvagyaVaish\u003C\u002Fa\u003E项目中提出的状态表达：取小鸟到下一组管子的水平距离和垂直距离差作为小鸟的状态。更准确地说， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5CDelta+x+\" alt=\"\\Delta x \" eeimg=\"1\"\u002F\u003E 与\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5CDelta+y+\" alt=\"\\Delta y \" eeimg=\"1\"\u002F\u003E的定义如下图所示：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-6ccc74c071fd10520ad4190080447bee_hd.jpg\" data-size=\"normal\" data-rawwidth=\"302\" data-rawheight=\"582\" data-default-watermark-src=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-ca8ed340cc8bbada28cf3f2952288469_hd.jpg\" class=\"content_image\" width=\"302\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;302&#39; height=&#39;582&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"302\" data-rawheight=\"582\" data-default-watermark-src=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-ca8ed340cc8bbada28cf3f2952288469_hd.jpg\" class=\"content_image lazy\" width=\"302\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-6ccc74c071fd10520ad4190080447bee_hd.jpg\"\u002F\u003E\u003Cfigcaption\u003E（图片来自Flappy Bird RL by SarvagyaVaish）\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E对于每一个状态\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28%5CDelta+x%2C%5CDelta+y%29\" alt=\"(\\Delta x,\\Delta y)\" eeimg=\"1\"\u002F\u003E，\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5CDelta+x\" alt=\"\\Delta x\" eeimg=\"1\"\u002F\u003E为水平距离，\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5CDelta+y\" alt=\"\\Delta y\" eeimg=\"1\"\u002F\u003E为垂直距离。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E动作的选择\u003C\u002Fb\u003E\u003Cbr\u002F\u003E每一帧，小鸟只有两种动作可选：1.向上飞一下。2.什么都不做。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E奖赏的选择\u003C\u002Fb\u003E\u003Cbr\u002F\u003E小鸟活着时，每一帧给予1的奖赏；若死亡，则给予-1000的奖赏。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E关于Q\u003C\u002Fb\u003E \u003C\u002Fh2\u003E\u003Cp\u003E提到Q-learning，我们需要先了解Q的含义。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003EQ\u003C\u002Fb\u003E为\u003Cb\u003E动作效用函数\u003C\u002Fb\u003E（action-utility function），用于评价在特定状态下采取某个动作的优劣。它是\u003Cb\u003E智能体的记忆\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Cp\u003E在这个问题中， 状态和动作的组合是有限的。所以我们可以把\u003Cb\u003EQ\u003C\u002Fb\u003E当做是一张表格。表中的每一行记录了状态\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28%5CDelta+x%2C+%5CDelta+y%29\" alt=\"(\\Delta x, \\Delta y)\" eeimg=\"1\"\u002F\u003E，选择不同动作（飞或不飞）时的奖赏：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-7378f0165d6e13b78778dbed00de6ada_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"328\" data-rawheight=\"374\" data-default-watermark-src=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-c4db20f142eeed8a814e1ab9dd423761_hd.jpg\" class=\"content_image\" width=\"328\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;328&#39; height=&#39;374&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"328\" data-rawheight=\"374\" data-default-watermark-src=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-c4db20f142eeed8a814e1ab9dd423761_hd.jpg\" class=\"content_image lazy\" width=\"328\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-7378f0165d6e13b78778dbed00de6ada_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E这张表一共 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=m+%5Ctimes+n\" alt=\"m \\times n\" eeimg=\"1\"\u002F\u003E 行，表示 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=m+%5Ctimes+n\" alt=\"m \\times n\" eeimg=\"1\"\u002F\u003E 个状态，每个状态所对应的动作都有一个\u003Cb\u003E效用值\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Cp\u003E理想状态下，在完成训练后，我们会获得一张完美的\u003Cb\u003EQ\u003C\u002Fb\u003E表格。我们希望只要小鸟根据当前位置查找到对应的行，选择\u003Cb\u003E效用值较大\u003C\u002Fb\u003E的动作作为当前帧的动作，就可以无限地存活。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E训练\u003C\u002Fb\u003E \u003C\u002Fh2\u003E\u003Cp\u003E下面的伪代码说明了我们如何训练，来得到一个尽量完美的Q表格。\u003C\u002Fp\u003E\u003Cdiv class=\"highlight\"\u003E\u003Cpre\u003E\u003Ccode class=\"language-text\"\u003E初始化 Q = {};\nwhile Q 未收敛：\n    初始化小鸟的位置S，开始新一轮游戏\n    while S != 死亡状态：\n        使用策略π，获得动作a=π(S) \n        使用动作a进行游戏，获得小鸟的新位置S&#39;,与奖励R(S,a)\n        Q[S,A] ← (1-α)*Q[S,A] + α*(R(S,a) + γ* max Q[S&#39;,a]) \u002F\u002F 更新Q\n        S ← S&#39;\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003C\u002Fdiv\u003E\u003Cp\u003E其中有些值得注意的地方：\u003C\u002Fp\u003E\u003Cp\u003E1. 使用策略π，获得动作a=π(S)\u003C\u002Fp\u003E\u003Cp\u003E最直观易懂的策略π(S)是根据\u003Cb\u003EQ\u003C\u002Fb\u003E表格来选择效用最大的动作（若两个动作效用值一样，如初始时某位置处效用值都为0，那就选第一个动作）。\u003C\u002Fp\u003E\u003Cp\u003E但这样的选择可能会使\u003Cb\u003EQ\u003C\u002Fb\u003E陷入局部最优：在位置 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_0\" alt=\"S_0\" eeimg=\"1\"\u002F\u003E 处，在第一次选择了动作1（飞）并获取了 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=r_1+%3E+0\" alt=\"r_1 &gt; 0\" eeimg=\"1\"\u002F\u003E 的奖赏后，算法将永远无法对动作2（不飞）进行更新，即使动作2最终会给出 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=r_2+%3E+r_1\" alt=\"r_2 &gt; r_1\" eeimg=\"1\"\u002F\u003E 的奖赏。\u003C\u002Fp\u003E\u003Cp\u003E改进的策略为\u003Cb\u003Eε-greedy\u003C\u002Fb\u003E方法：每个状态以\u003Cb\u003Eε的概率进行探索\u003C\u002Fb\u003E，此时将随机选取飞或不飞，而\u003Cb\u003E剩下的1-ε的概率则进行开发\u003C\u002Fb\u003E，即按上述方法，选取当前状态下效用值较大的动作。\u003C\u002Fp\u003E\u003Cp\u003E2.更新\u003Cb\u003EQ\u003C\u002Fb\u003E表格\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003EQ\u003C\u002Fb\u003E表格将根据以下公式进行更新：\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%28S%2CA%29+%5Cleftarrow+%281-%5Calpha%29Q%28S%2CA%29+%2B+%5Calpha%5BR%28S%2C+a%29+%2B+%5Cgamma%5Cmax%5Climits_aQ%28S%27%2C+a%29%5D\" alt=\"Q(S,A) \\leftarrow (1-\\alpha)Q(S,A) + \\alpha[R(S, a) + \\gamma\\max\\limits_aQ(S&#39;, a)]\" eeimg=\"1\"\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E其中\u003Cb\u003Eα\u003C\u002Fb\u003E为\u003Cb\u003E学习速率\u003C\u002Fb\u003E（learning rate），\u003Cb\u003Eγ\u003C\u002Fb\u003E为\u003Cb\u003E折扣因子\u003C\u002Fb\u003E（discount factor）。根据公式可以看出，学习速率α越大，保留之前训练的效果就越少。折扣因子γ越大，\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=max_aQ%28S%27%2C+a%29\" alt=\"max_aQ(S&#39;, a)\" eeimg=\"1\"\u002F\u003E所起到的作用就越大。但\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=max_aQ%28S%27%2C+a%29\" alt=\"max_aQ(S&#39;, a)\" eeimg=\"1\"\u002F\u003E指什么呢？\u003C\u002Fp\u003E\u003Cp\u003E考虑小鸟在对状态进行更新时，会关心到\u003Cb\u003E眼前利益\u003C\u002Fb\u003E（ \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=R\" alt=\"R\" eeimg=\"1\"\u002F\u003E ），和\u003Cb\u003E记忆中的利益\u003C\u002Fb\u003E（\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmax%5Climits_aQ%28S%27%2C+a%29\" alt=\"\\max\\limits_aQ(S&#39;, a)\" eeimg=\"1\"\u002F\u003E）。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=max_aQ%28S%27%2C+a%29\" alt=\"max_aQ(S&#39;, a)\" eeimg=\"1\"\u002F\u003E是\u003Cb\u003E记忆中的利益\u003C\u002Fb\u003E。它是小鸟记忆里，新位置\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S%27\" alt=\"S&#39;\" eeimg=\"1\"\u002F\u003E能给出的最大效用值。如果小鸟在过去的游戏中于位置\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S%27\" alt=\"S&#39;\" eeimg=\"1\"\u002F\u003E的某个动作上吃过甜头（例如选择了某个动作之后获得了50的奖赏），这个公式就可以让它提早地得知这个消息，以便使下回再通过位置\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S\" alt=\"S\" eeimg=\"1\"\u002F\u003E时选择正确的动作继续进入这个吃甜头的位置\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S%27\" alt=\"S&#39;\" eeimg=\"1\"\u002F\u003E。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E可以看出，γ越大，小鸟就会越重视以往经验，越小，小鸟只重视眼前利益（R）。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E根据上面的伪代码，便可以写出Q-learning的代码了。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E成果\u003C\u002Fb\u003E \u003C\u002Fh2\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002F5a852507fabd96902e9939dd33571746_hd.jpg\" data-size=\"normal\" data-rawwidth=\"323\" data-rawheight=\"567\" class=\"content_image\" width=\"323\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;323&#39; height=&#39;567&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"323\" data-rawheight=\"567\" class=\"content_image lazy\" width=\"323\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002F5a852507fabd96902e9939dd33571746_hd.jpg\"\u002F\u003E\u003Cfigcaption\u003E训练后的小鸟一直挂在那里可以飞到几千分。\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E","editableContent":"","excerpt":"2018年12月1日修改： @Rahn 的答案中提出了一种剪枝方法，可以加快模型的收敛。2017年06月05日修改：最近重写了一遍代码， Flappy Bird Q-learning 。你可以在这里试着训练，想最大帧数下，一两分钟内就可以达到10+的分数。问题分析 我们可以通过 强化学习（reinforcement learning）来解决小鸟怎么飞这个问题。强化学习中有状态（state）、动作（action）、奖赏（reward）这三个要素。智能体（Agent，指小鸟）会根据当前状态来采…","collapsedBy":"nobody","collapseReason":"","annotationAction":null,"markInfos":[],"relevantInfo":{"isRelevant":false,"relevantType":"","relevantText":""},"suggestEdit":{"reason":"","status":false,"tip":"","title":"","unnormalDetails":{"status":"","description":"","reason":"","reasonId":0,"note":""},"url":""},"isLabeled":false,"rewardInfo":{"canOpenReward":false,"isRewardable":false,"rewardMemberCount":0,"rewardTotalMoney":0,"tagline":""},"relationship":{"isAuthor":false,"isAuthorized":false,"isNothelp":false,"isThanked":false,"isRecognized":false,"voting":0,"upvotedFollowees":[]}},"467132543":{"id":467132543,"type":"answer","answerType":"normal","question":{"type":"question","id":26408259,"title":"如何用简单例子讲解 Q - learning 的具体过程？","questionType":"normal","created":1414653732,"updatedTime":1472707763,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F26408259","relationship":{}},"author":{"id":"9ecb1c10ea59e37946b80597cef515f4","urlToken":"jiong-jiong-shi","name":"囧Bourne","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-3ca8a9ff2ab03fabdc3c478721a41d16_is.jpg","avatarUrlTemplate":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-3ca8a9ff2ab03fabdc3c478721a41d16_{size}.jpg","isOrg":false,"type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002F9ecb1c10ea59e37946b80597cef515f4","userType":"people","headline":"Human-Robot Coexistence\\ 人机共融、机器人与机器学习","badge":[],"gender":1,"isAdvertiser":false,"followerCount":245,"isFollowed":false,"isPrivacy":false},"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fanswers\u002F467132543","isCollapsed":false,"createdTime":1534061300,"updatedTime":1535339910,"extras":"","isCopyable":false,"isNormal":true,"voteupCount":53,"commentCount":1,"isSticky":false,"adminClosedComment":false,"commentPermission":"all","canComment":{"reason":"","status":true},"reshipmentSettings":"disallowed","content":"\u003Cp\u003E为了理清强化学习中最经典、最基础的算法——Q-learning，根据ADEPT的学习规律（Analogy \u002F Diagram \u002F Example \u002F Plain \u002F Technical Definition），本文努力用直观理解、数学方法、图形表达、简单例子和文字解释来展现其精髓之处。\u003Cb\u003E区别于众多Q-learning讲解中的伪代码流程图，本文将提供可视化的算法流程图\u003C\u002Fb\u003E帮助大家学习、对比Q-learning与Sarsa。\u003C\u002Fp\u003E\u003Cp\u003E( 在此假设大家已了解TD Method， ε-greedy policy，off-policy 和 on-policy相关知识。想了解的童鞋也可在本文最后Reference或链接中学习） \u003C\u002Fp\u003E\u003Ch2\u003E一、直观理解\u003C\u002Fh2\u003E\u003Cp\u003E任何强化学习的问题都需要兼顾探索（Exploration）和利用（Exploitation），因此TD方法也对应有两种经典的衍生算法：Q-learning（off-policy）和Sarsa（on-policy）。基于off-policy思想的Q-learning，与Monte Carlo 方法中off-policy的灵魂思路是一致的，除了更新价值的步数不一样之外。在此引用笔者之前回答中关于off-policy的一个比喻。\u003C\u002Fp\u003E\u003Cdiv class=\"highlight\"\u003E\u003Cpre\u003E\u003Ccode class=\"language-text\"\u003E     古时候，优秀的皇帝都秉持着“水能载舟 亦能覆舟”的思想，希望能多了解民间百姓的生活。皇帝可以选择通过微服出巡，亲自下凡了解百姓生活（On-policy），虽然眼见为实，但毕竟皇帝本人分身乏术，掌握情况不全；因此也可以派多个官员去了解情况，而皇帝本人则躺在酒池肉林里收听百官情报即可（Off-policy）。\n（坏皇帝则派出“锦衣卫”_(´ཀ`」 ∠)_）\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003C\u002Fdiv\u003E\u003Cp\u003E不清楚off-policy的同学可以点击以下传送门：\u003C\u002Fp\u003E\u003Ca href=\"https:\u002F\u002Fwww.zhihu.com\u002Fquestion\u002F57159315\u002Fanswer\u002F465865135\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-978881b6ca688d54e38afe7d04dd4f25_180x120.jpg\" data-image-width=\"2702\" data-image-height=\"1606\" class=\"internal\"\u003E强化学习中on-policy 与off-policy有什么区别？\u003C\u002Fa\u003E\u003Cp\u003E\u003Cb\u003E疑难点在于：\u003C\u002Fb\u003E对于Q-learning（off-policy），我们用来产生与环境互动的行为策略，既然其产生的样本数据是用来训练目标策略的，那为什么学习策略可以在某一程度上独立于行为策略呢？这就是一个细节处理问题，因此下面将一步一步剖析这种独立是如何产生。\u003C\u002Fp\u003E\u003Ch2\u003E二、算法流程\u003C\u002Fh2\u003E\u003Cp\u003E本文首先放出两张一目了然的流程图： Q-learning和Sarsa，为了可以直接于可视化流程图的比较之中领悟这两种方法的思路和差别。  \u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E1. Sarsa\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"small\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-5aebc163f1e575aecfad9e4d1e4ce06a_hd.jpg\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"963\" data-rawheight=\"1037\" data-default-watermark-src=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-b19ecbf6c0f069f601d1275ea5835a5d_hd.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"963\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5aebc163f1e575aecfad9e4d1e4ce06a_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;963&#39; height=&#39;1037&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"963\" data-rawheight=\"1037\" data-default-watermark-src=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-b19ecbf6c0f069f601d1275ea5835a5d_hd.jpg\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"963\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5aebc163f1e575aecfad9e4d1e4ce06a_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-5aebc163f1e575aecfad9e4d1e4ce06a_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E（已了解Sarsa的同学也不要轻易跳过，或者对比过后，你会有新的发现）\u003C\u002Fp\u003E\u003Cp\u003E1.1)    一个回合（Episode）开始，随机选择（初始化）第一个状态\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_1\" alt=\"S_1\" eeimg=\"1\"\u002F\u003E。并基于 ε-greedy策略在状态\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_1\" alt=\"S_1\" eeimg=\"1\"\u002F\u003E中选择动作，有两种情况，一是有 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%281-%CE%B5%29\" alt=\"(1-ε)\" eeimg=\"1\"\u002F\u003E 的概率直接选择具有最大值Q的动作，二是有 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%CE%B5\" alt=\"ε\" eeimg=\"1\"\u002F\u003E 概率随机选择 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_1\" alt=\"S_1\" eeimg=\"1\"\u002F\u003E 下的任意动作（在第二种情况下每个动作的概率均为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%CE%B5%2F%7CA_1%7C\" alt=\"ε\u002F|A_1|\" eeimg=\"1\"\u002F\u003E ,其中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%7CA_1%7C\" alt=\"|A_1|\" eeimg=\"1\"\u002F\u003E 为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_1\" alt=\"S_1\" eeimg=\"1\"\u002F\u003E 下的动作总个数）。\u003C\u002Fp\u003E\u003Cp\u003E1.2)    进入第一次循环(Repeat 1 \u002F Step 1)：执行 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A_1\" alt=\"A_1\" eeimg=\"1\"\u002F\u003E 之后（与环境互动），观察下一个状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_2\" alt=\"S_2\" eeimg=\"1\"\u002F\u003E ，并马上得到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_2\" alt=\"S_2\" eeimg=\"1\"\u002F\u003E 的即时回报 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=R_2\" alt=\"R_2\" eeimg=\"1\"\u002F\u003E。\u003Cb\u003E此时，再次基于 ε-greedy策略，在状态\u003C\u002Fb\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_2\" alt=\"S_2\" eeimg=\"1\"\u002F\u003E\u003Cb\u003E中选择动作\u003C\u002Fb\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A_2\" alt=\"A_2\" eeimg=\"1\"\u002F\u003E\u003Cb\u003E。得到 \u003C\u002Fb\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A_2\" alt=\"A_2\" eeimg=\"1\"\u002F\u003E\u003Cb\u003E 后，即可进行Q函数的更新（Update），更新中的 \u003C\u002Fb\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q_k%28S_2%2CA_2%29\" alt=\"Q_k(S_2,A_2)\" eeimg=\"1\"\u002F\u003E\u003Cb\u003E 为ε-greedy策略下所随机选取的动作，这是与Q-learning的不同之处！\u003C\u002Fb\u003E（下标 k 或 i 表示最近一次更新的Q值，是一个迭代序数而非时间步（step）序数，在此可先忽略。）\u003C\u002Fp\u003E\u003Cp\u003E1.3)    不断循环第二步，直到终止状态。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E2. Q-learning\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"small\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-db6c94e4a630b382b28d5bed3b096dc4_hd.jpg\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"977\" data-rawheight=\"1059\" data-default-watermark-src=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-5b256c09644635e5d86159c37b7b013b_hd.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"977\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-db6c94e4a630b382b28d5bed3b096dc4_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;977&#39; height=&#39;1059&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"977\" data-rawheight=\"1059\" data-default-watermark-src=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-5b256c09644635e5d86159c37b7b013b_hd.jpg\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"977\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-db6c94e4a630b382b28d5bed3b096dc4_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-db6c94e4a630b382b28d5bed3b096dc4_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E2.1)    一个回合（Episode）开始，随机选择（初始化）第一个状态\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_1\" alt=\"S_1\" eeimg=\"1\"\u002F\u003E。\u003C\u002Fp\u003E\u003Cp\u003E2.2)    进入第一次循环（Repeat 1 \u002F Step 1）：首先基于 ε-greedy策略在状态\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_1\" alt=\"S_1\" eeimg=\"1\"\u002F\u003E中选择动作。选择并执行 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A_1\" alt=\"A_1\" eeimg=\"1\"\u002F\u003E 之后（与环境互动），观察下一个状态 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_2\" alt=\"S_2\" eeimg=\"1\"\u002F\u003E ，并得到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_2\" alt=\"S_2\" eeimg=\"1\"\u002F\u003E 的即时回报 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=R_2\" alt=\"R_2\" eeimg=\"1\"\u002F\u003E。此时，\u003Cb\u003E立即进行Q函数的更新（Update），更新中的 \u003C\u002Fb\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=max_aQ_k%28S_2%2Ca_2%29\" alt=\"max_aQ_k(S_2,a_2)\" eeimg=\"1\"\u002F\u003E\u003Cb\u003E 为我们人为直接选择 \u003C\u002Fb\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_2\" alt=\"S_2\" eeimg=\"1\"\u002F\u003E\u003Cb\u003E 下所有动作中具有最大Q值的动作，这就是与Saras根本区别！\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E2.3)    更新完毕后，进入第二次循环：基于 ε-greedy策略，在状态\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_2\" alt=\"S_2\" eeimg=\"1\"\u002F\u003E中选择动作与环境互动（此前在状态\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_2\" alt=\"S_2\" eeimg=\"1\"\u002F\u003E时候并未采取动作与环境互动）。值得注意的是，我们在循环1中更新（Update）时所选取 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_2+\" alt=\"S_2 \" eeimg=\"1\"\u002F\u003E 的动作 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a_2\" alt=\"a_2\" eeimg=\"1\"\u002F\u003E 是唯一的(人为强制选择)，即最具有最大价值Q的动作\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=max_aQ_k%28S_2%2Ca_2%29\" alt=\"max_aQ_k(S_2,a_2)\" eeimg=\"1\"\u002F\u003E；而循环2中作为需要与环境互动的第二次动作 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A_2\" alt=\"A_2\" eeimg=\"1\"\u002F\u003E  则是基于ε-greedy策略（即在此时究竟选取 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=max_aQ_k%28S_2%2Ca_2%29\" alt=\"max_aQ_k(S_2,a_2)\" eeimg=\"1\"\u002F\u003E 对应的动作 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a_2\" alt=\"a_2\" eeimg=\"1\"\u002F\u003E 还是其他动作完全根据是随机选择，听天由命吧 0.0 ）！\u003Cb\u003E因此，基于ε-greedy策略，与环境互动、做学习训练时做动作选择的决策（在off-policy中这被称为行为策略）与Sarsa是一致的。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Ch2\u003E3. 细节\u003C\u002Fh2\u003E\u003Cp\u003E不少学童鞋对这两幅伪码图中的动作符号存疑：为什么动作的表示有时候为大写的A，有时候为小写的a？\u003C\u002Fp\u003E\u003Cp\u003E\u003Ci\u003E（引用R. S. Sutton与A.G. Barto于2018年1月1日发布的《Reinforcement learning: An introduction》第二版）：\u003C\u002Fi\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"small\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-70b370d3c271f1d643534001396cbcd0_hd.jpg\" data-size=\"small\" data-rawwidth=\"1721\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb\" width=\"1721\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-70b370d3c271f1d643534001396cbcd0_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1721&#39; height=&#39;605&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"small\" data-rawwidth=\"1721\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1721\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-70b370d3c271f1d643534001396cbcd0_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-70b370d3c271f1d643534001396cbcd0_hd.jpg\"\u002F\u003E\u003Cfigcaption\u003EPseudo code of Sarsa\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"small\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-ce61618678907272d040ae9f80794b0d_hd.jpg\" data-size=\"small\" data-rawwidth=\"1707\" data-rawheight=\"556\" class=\"origin_image zh-lightbox-thumb\" width=\"1707\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-ce61618678907272d040ae9f80794b0d_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1707&#39; height=&#39;556&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"small\" data-rawwidth=\"1707\" data-rawheight=\"556\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1707\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-ce61618678907272d040ae9f80794b0d_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-ce61618678907272d040ae9f80794b0d_hd.jpg\"\u002F\u003E\u003Cfigcaption\u003EPseudo code of Q-learning\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E大写的A表示集合，比如 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A_1\" alt=\"A_1\" eeimg=\"1\"\u002F\u003E 则表示 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_1\" alt=\"S_1\" eeimg=\"1\"\u002F\u003E 下的所有动作，而 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a_1\" alt=\"a_1\" eeimg=\"1\"\u002F\u003E 则表示具体的一个动作，它们之间的关系为：\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a_1%5Cin+A_1\" alt=\"a_1\\in A_1\" eeimg=\"1\"\u002F\u003E。回到流程图中，可以发现出现a都在Q-learning的update公式中，这是因为我们在更新时，人为指定选择具有最大值Q的a，这是具有确定性的事件（Deterministic）。而在Q-learning中与环境互动的环节、在Sarsa中更新Q值的环节与环境互动的环节时，动作的选择是随机的（ ε-greedy），因此所有动作都有可能被选中，只不过是具有最大值Q的动作\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a+%3D+%5Cleft+%5C%7B+a%7C+max_%7Ba%5Cin+A%7D+Q%28S%2CA%29%5Cright+%5C%7D\" alt=\"a = \\left \\{ a| max_{a\\in A} Q(S,A)\\right \\}\" eeimg=\"1\"\u002F\u003E被选中的概率大。\u003C\u002Fp\u003E\u003Cp\u003E此时我们可以清楚知道Sutton书中的伪代码的全部含义啦^_^！\u003C\u002Fp\u003E\u003Ch2\u003E三、Q-learning如何实现更加有效的探索？\u003C\u002Fh2\u003E\u003Cp\u003E清楚整个流程之后，我们来具体看看，Q-learning到底是怎么实现有意义的探索，如何在环境中发掘出更有价值的动作？（即一个当前估值（evaluate）不高但潜力巨大的动作的逆袭之路）\u003C\u002Fp\u003E\u003Cfigure data-size=\"small\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-2d3fc034d95d825950bdd1dfa3bb6ce1_hd.jpg\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"998\" data-rawheight=\"904\" class=\"origin_image zh-lightbox-thumb\" width=\"998\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2d3fc034d95d825950bdd1dfa3bb6ce1_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;998&#39; height=&#39;904&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"998\" data-rawheight=\"904\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"998\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2d3fc034d95d825950bdd1dfa3bb6ce1_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-2d3fc034d95d825950bdd1dfa3bb6ce1_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E在这个例子中，我们将更新黄色状态的动作价值\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%28S%3Dyellow%2CA%29\" alt=\"Q(S=yellow,A)\" eeimg=\"1\"\u002F\u003E。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E图a，假设已知黄色状态下只有两个动作可选：灰动作和黑动作，并且在第k-1次更新 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%28S%3Dyellow%2CA%29\" alt=\"Q(S=yellow,A)\" eeimg=\"1\"\u002F\u003E 时，（ \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q_%7B1%7D%28S_t%2Ca_t%29\" alt=\"Q_{1}(S_t,a_t)\" eeimg=\"1\"\u002F\u003E 通过人为设置获取，不纳入更新迭代次数k），已知灰动作价值比黑动作大，即有\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q_%7Bm%7D%28yellow%2Cgray%29%3EQ_%7Bn%7D%28yellow%2Cblack+%29\" alt=\"Q_{m}(yellow,gray)&gt;Q_{n}(yellow,black )\" eeimg=\"1\"\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E其中m,n为对应动作价值已迭代更新的次数，k为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q_%7Bk%7D%28yellow%2Caction%29\" alt=\"Q_{k}(yellow,action)\" eeimg=\"1\"\u002F\u003E 更新的次数，所以有m+n= k。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E图b，在某个回合（episode）中，在时间步为t的时候（time step = t），所处状态为黄色，又已知灰动作价值比黑动作大，即基于 ε-greedy的行为策略选择动作，会出现情况①或②：\u003C\u002Fp\u003E\u003Cp\u003E①  有 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%281-+%CE%B5+%29%2B+%CE%B5+%2F2%3D1-+%CE%B5+%2F2\" alt=\"(1- ε )+ ε \u002F2=1- ε \u002F2\" eeimg=\"1\"\u002F\u003E 的可能性选择当前最大价值Q的灰动作 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a_n\" alt=\"a_n\" eeimg=\"1\"\u002F\u003E：\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q_%7Bm%2B1%7D%28s_t%2Ca_t%29%3DQ_m%28s_t%2Ca_t%29%2B%5Calpha%5BR_%7Bt%2B1%7D%2BmaxQ_i%28s_%7Bt%2B1%7D%2CA_%7Bt%2B1%7D%29-Q_m%28s_t%2Ca_t%29%5D\" alt=\"Q_{m+1}(s_t,a_t)=Q_m(s_t,a_t)+\\alpha[R_{t+1}+maxQ_i(s_{t+1},A_{t+1})-Q_m(s_t,a_t)]\" eeimg=\"1\"\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E而另一黑动作没有更新。\u003C\u002Fp\u003E\u003Cp\u003E其中， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s_t%3Dyellow\" alt=\"s_t=yellow\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cp\u003E②  有 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+%CE%B5+%2F2\" alt=\" ε \u002F2\" eeimg=\"1\"\u002F\u003E  的可能性选择当前较小价值Q的黑动作 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a_n%27\" alt=\"a_n&#39;\" eeimg=\"1\"\u002F\u003E：\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q_%7Bn%2B1%7D%28s_t%2Ca_t%27%29%3DQ_n%28s_t%2Ca_t%27%29%2B%5Calpha%5BR_%7Bt%2B1%7D%2BmaxQ_j%28s_%7Bt%2B1%7D%27%2CA_%7Bt%2B1%7D%27%29-Q_n%28s_t%2Ca_t%27%29%5D\" alt=\"Q_{n+1}(s_t,a_t&#39;)=Q_n(s_t,a_t&#39;)+\\alpha[R_{t+1}+maxQ_j(s_{t+1}&#39;,A_{t+1}&#39;)-Q_n(s_t,a_t&#39;)]\" eeimg=\"1\"\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E 而另一灰动作没有更新\u003C\u002Fp\u003E\u003Cp\u003E其中， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s_t%3Dyellow\" alt=\"s_t=yellow\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E在此假设图b发生②的情况，则 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s_%7Bt%2B1%7D%27%3Dgreen\" alt=\"s_{t+1}&#39;=green\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E图c通过选取 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=green\" alt=\"green\" eeimg=\"1\"\u002F\u003E 下的最大价值动作 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a_j%28red%29\" alt=\"a_j(red)\" eeimg=\"1\"\u002F\u003E 来更新图f的目标策略。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E在第k+1次更新中，我们通过取最大值（即greedy思想），选取Q值最大的动作来更新目标策略 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E （target policy）：\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q_%7Bk%2B1%7D%5E%7B%5Cpi%7D%28S%3Dyellow%2CA%29%3Dmax%5Cleft+%5C%7BQ_%7Bm%7D%28S%2Cgray%29%2CQ_%7Bn%2B1%7D%28S%2Cblack%29%5Cright+%5C%7D\" alt=\"Q_{k+1}^{\\pi}(S=yellow,A)=max\\left \\{Q_{m}(S,gray),Q_{n+1}(S,black)\\right \\}\" eeimg=\"1\"\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E比如，基于上述已发生的，再发生图f的情况，则在下一次更新\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q_%7Bk%2B1%7D%5E%7B%5Cpi%7D%28S%3Dyellow%2CA%29\" alt=\"Q_{k+1}^{\\pi}(S=yellow,A)\" eeimg=\"1\"\u002F\u003E时，黄色状态下的黑动作变为最优动作（颠覆了灰色动作有最大Q值的地位）。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E（实际上无论发生情况①或是②，黄色状态下的灰动作与黑动作的价值的大小关系都可能发生变化！！）\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003E四、另一个栗子\u003C\u002Fh2\u003E\u003Cp\u003E在此举一个非常直观的例子来帮助我们认识一下Q-learning和Sarsa的实际应用效果的区别。\u003C\u002Fp\u003E\u003Cp\u003E在下面栅格化的小世界中，绿色区域为草地，在上面每移动一格子就会扣1分，而踏入黑色区域的悬崖（chasm），会扣一百分，并且回到起始点S (Start)。我们希望能学习到一条得分最高的路径到达终点T (Terminal)。分别使用Sarsa和Q-learning进行学习。结果如图所示，红色为相应算法的最优路径。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-28d7363a9a1c00f029ef5ce4fb29e362_hd.jpg\" data-size=\"normal\" data-rawwidth=\"564\" data-rawheight=\"685\" class=\"origin_image zh-lightbox-thumb\" width=\"564\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-28d7363a9a1c00f029ef5ce4fb29e362_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;564&#39; height=&#39;685&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"564\" data-rawheight=\"685\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"564\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-28d7363a9a1c00f029ef5ce4fb29e362_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-28d7363a9a1c00f029ef5ce4fb29e362_hd.jpg\"\u002F\u003E\u003Cfigcaption\u003ESarsa vs Q-learning\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E可以看到，Q-learning寻找到一条全局最优的路径，因为虽然Q-learning的行为策略（behavior）是基于 ε-greedy策略，但其目标策略（target policy）只考虑最优行为；而Sarsa只能找到一条次优路径，这条路径在直观上更加安全，这是因为Sarsa（其目标策略和行为策略为同一策略）考虑了所有动作的可能性（ ε-greedy），当靠近悬崖时，由于会有一定概率选择往悬崖走一步，从而使得这些悬崖边路的价值更低。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003E五、总结\u003C\u002Fh2\u003E\u003Cp\u003EQ-learning虽然具有学习到全局最优的能力，但是其收敛慢；而Sarsa虽然学习效果不如Q-learning，但是其收敛快，直观简单。因此，对于不同的问题，我们需要有所斟酌。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003EReference\u003C\u002Fh2\u003E\u003Col\u003E\u003Cli\u003EWatkins C J C H, Dayan P. Technical Note: Q-Learning[J]. Machine Learning, 8(3-4):279-292, 1992.\u003C\u002Fli\u003E\u003Cli\u003ER. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. IEEE Transactions on Neural Networks, 9(5):1054–1054, 2018.\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cp\u003E\u003Cb\u003E声明：本图文未经作者允许，谢绝转载。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fwww.zhihu.com\u002Fquestion\u002F57159315\u002Fanswer\u002F465865135\" class=\"internal\"\u003E囧Bourne：强化学习中on-policy 与off-policy有什么区别？\u003C\u002Fa\u003E\u003C\u002Fp\u003E","editableContent":"","excerpt":"为了理清强化学习中最经典、最基础的算法——Q-learning，根据ADEPT的学习规律（Analogy \u002F Diagram \u002F Example \u002F Plain \u002F Technical Definition），本文努力用直观理解、数学方法、图形表达、简单例子和文字解释来展现其精髓之处。 区别于众多Q-learning讲解中的伪代码流程图，本文将提供可视化的算法流程图帮助大家学习、对比Q-learning与Sarsa。( 在此假设大家已了解TD Method， ε-greedy policy，off-policy 和 on-policy相关…","collapsedBy":"nobody","collapseReason":"","annotationAction":null,"markInfos":[],"relevantInfo":{"isRelevant":false,"relevantType":"","relevantText":""},"suggestEdit":{"reason":"","status":false,"tip":"","title":"","unnormalDetails":{"status":"","description":"","reason":"","reasonId":0,"note":""},"url":""},"isLabeled":false,"rewardInfo":{"canOpenReward":false,"isRewardable":true,"rewardMemberCount":0,"rewardTotalMoney":0,"tagline":"予人玫瑰，手有余香"},"relationship":{"isAuthor":false,"isAuthorized":false,"isNothelp":false,"isThanked":false,"isRecognized":false,"voting":0,"upvotedFollowees":[]}},"577272724":{"id":577272724,"type":"answer","answerType":"normal","question":{"type":"question","id":26408259,"title":"如何用简单例子讲解 Q - learning 的具体过程？","questionType":"normal","created":1414653732,"updatedTime":1472707763,"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F26408259","relationship":{}},"author":{"id":"866e63341ae3873b7a4ce0390767dc74","urlToken":"qinhanzhang","name":"覃含章","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-8d60cb90a307d11d3daeaaff0dd051fa_is.jpg","avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-8d60cb90a307d11d3daeaaff0dd051fa_{size}.jpg","isOrg":false,"type":"people","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fpeople\u002F866e63341ae3873b7a4ce0390767dc74","userType":"people","headline":"Optimization &amp; STAT Learning researcher","badge":[{"type":"identity","description":"计算科学与工程博士在读","topics":[]},{"type":"best_answerer","description":"优秀回答者","topics":[{"id":"19559450","type":"topic","url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19559450","name":"机器学习","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fd3dd87a0feae0a3db82973157eee89c0_is.jpg"}]}],"gender":1,"isAdvertiser":false,"followerCount":15933,"isFollowed":false,"isPrivacy":false},"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fanswers\u002F577272724","isCollapsed":false,"createdTime":1547882776,"updatedTime":1548265161,"extras":"","isCopyable":true,"isNormal":true,"voteupCount":36,"commentCount":7,"isSticky":false,"adminClosedComment":false,"commentPermission":"all","canComment":{"reason":"","status":true},"reshipmentSettings":"allowed","content":"\u003Cp\u003E本回答来自我的知乎专栏上的文章。主要说一下Q-learning中运用的upper confidence bound\u002F置信度的思想。主要的参考文献为：\u003C\u002Fp\u003E\u003Cp\u003EJin, C., Allen-Zhu, Z., Bubeck, S. and Jordan, M.I., 2018. &#34;Is Q-learning provably efficient?.&#34;\u003Ci\u003EAdvances in Neural Information Processing Systems\u003C\u002Fi\u003E. 2018.\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Ch2\u003E\u003Cb\u003E一、Bandit算法回顾，MDP简单介绍\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E首先，我们指出在bandit算法中主要有两类流行的算法，一类是贪心算法（如uniform exploration， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E -greedy算法），还有一类是基于upper confidence bound的UCB算法。他们二者的具体差别在哪里呢？简单来说，就是他们二者对exploration和exploitation的分配思路不太一样。对这方面不太熟悉的同学，可见：\u003C\u002Fp\u003E\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F53595666\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-6707412e90fd874e86b6a0b88c69fdfa_180x120.jpg\" data-image-width=\"1280\" data-image-height=\"720\" class=\"internal\"\u003E覃含章：在线学习(MAB)与强化学习(RL)[2]：IID Bandit的一些算法\u003C\u002Fa\u003E\u003Cp\u003E话又说回来，在经典bandit算法当中，我们要做的无非就是通过一类pull arm的策略，尽快找到比较好的“arm”（reward较高的arm），然后尽可能多的去拉这些比较好的arm就是了。贪心算法无非就是永远以当前对每个arm的reward的估计直接作为依据，而UCB算法则是考虑了置信度的问题，因此考虑的是每个arm reward的置信区间的上界。\u003C\u002Fp\u003E\u003Cp\u003E那么这个思想推广到更一般的马尔科夫决策过程问题中，可不可以呢？答案自然也是可以的。具体来说，我们考虑一般的episodic Markov decision process， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctext%7BMDP%7D%28%5Cmathcal%7BS%7D%2C%5Cmathcal%7BA%7D%2CH%2C%5Cmathbb%7BP%7D%2Cr%29\" alt=\"\\text{MDP}(\\mathcal{S},\\mathcal{A},H,\\mathbb{P},r)\" eeimg=\"1\"\u002F\u003E 。其中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BS%7D\" alt=\"\\mathcal{S}\" eeimg=\"1\"\u002F\u003E 是所有状态（state）的集合，且状态数量有限， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S%3A%3D%7C%5Cmathcal%7BS%7D%7C.\" alt=\"S:=|\\mathcal{S}|.\" eeimg=\"1\"\u002F\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathcal%7BA%7D\" alt=\"\\mathcal{A}\" eeimg=\"1\"\u002F\u003E 则是所有行动（action）的集合，行动数量也有限， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A%3A%3D%7C%5Cmathcal%7BA%7D%7C\" alt=\"A:=|\\mathcal{A}|\" eeimg=\"1\"\u002F\u003E ，\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=H\" alt=\"H\" eeimg=\"1\"\u002F\u003E 则是每个episode中MDP状态转移的次数（因此这个MDP可以认为是finite horizon的）。 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbb%7BP%7D\" alt=\"\\mathbb{P}\" eeimg=\"1\"\u002F\u003E 就是我们熟知的状态转移矩阵（注意因为这里我们的状态和行动空间都是有限的，所以这个矩阵的维度也是有限的），即我们可以定义 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbb%7BP%7D_h%28%5Ccdot%7Cx%2Ca%29\" alt=\"\\mathbb{P}_h(\\cdot|x,a)\" eeimg=\"1\"\u002F\u003E 来表示在 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=h%5Cin+%5BH%5D\" alt=\"h\\in [H]\" eeimg=\"1\"\u002F\u003E 步中选择action \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a\" alt=\"a\" eeimg=\"1\"\u002F\u003E 和处在state \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=x\" alt=\"x\" eeimg=\"1\"\u002F\u003E 情况下转移到其它state的分布。类似的，我们定义 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=r_h%3A%5Cmathcal%7BS%7D%5Ctimes+%5Cmathcal%7BA%7D%5Crightarrow+%5B0%2C1%5D\" alt=\"r_h:\\mathcal{S}\\times \\mathcal{A}\\rightarrow [0,1]\" eeimg=\"1\"\u002F\u003E 为一个确定性的reward函数（与 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=h%2Cs%2Ca\" alt=\"h,s,a\" eeimg=\"1\"\u002F\u003E 有关）。\u003C\u002Fp\u003E\u003Cp\u003E注意，我们这边考虑的是一个episodic MDP，也就是说这个MDP允许被重复很多个episodes。这也很好理解，比如我们如果要训练AlphaGo，那么每个episode就可以看作一局棋，我们势必要重复下很多盘棋才可能对一个比较好的下棋“策略”（policy）有所了解，而如果只下一盘或者很少盘数的棋，我们可能都没有办法对于怎么下棋这件事情，和不同的下棋策略赢的几率，有一个好的exploration。\u003C\u002Fp\u003E\u003Cp\u003E那么我们就发现有两个核心的概念：\u003C\u002Fp\u003E\u003Col\u003E\u003Cli\u003E策略：policy的概念。在MDP中，我们定义一个policy \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=H\" alt=\"H\" eeimg=\"1\"\u002F\u003E 个函数的集合： \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5C%7B%5Cpi_h%3A%5Cmathcal%7BS%7D%5Crightarrow++%5Cmathcal%7BA%7D%5C%7D_%7Bh%5Cin+%5BH%5D%7D\" alt=\"\\{\\pi_h:\\mathcal{S}\\rightarrow  \\mathcal{A}\\}_{h\\in [H]}\" eeimg=\"1\"\u002F\u003E 。也就是说，在每一步 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=h\" alt=\"h\" eeimg=\"1\"\u002F\u003E ,在不同的state下我们应该选择哪个action。\u003C\u002Fli\u003E\u003Cli\u003E价值函数：value function的概念。这里我们需要定义著名的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V\" alt=\"V\" eeimg=\"1\"\u002F\u003E  function和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q\" alt=\"Q\" eeimg=\"1\"\u002F\u003E  function。我们称 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_h%5E%7B%5Cpi%7D%28x%29\" alt=\"V_h^{\\pi}(x)\" eeimg=\"1\"\u002F\u003E 这个函数是在step \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=h\" alt=\"h\" eeimg=\"1\"\u002F\u003E 和state \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=x\" alt=\"x\" eeimg=\"1\"\u002F\u003E 上，根据policy \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 所能获得的期望剩余reward。也就是说， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_h%5E%7B%5Cpi%7D%28x%29%3A%3D%5Cmathbb%7BE%7D%5Cleft%5B++%5Csum_%7Bh%27%3Dh%7D%5EH+r_%7Bh%27%7D%28x_%7Bh%27%7D%2C%5Cpi_%7Bh%27%7D%28x_%7Bh%27%7D%29%29+%7Cx_h%3Dx+%5Cright%5D\" alt=\"V_h^{\\pi}(x):=\\mathbb{E}\\left[  \\sum_{h&#39;=h}^H r_{h&#39;}(x_{h&#39;},\\pi_{h&#39;}(x_{h&#39;})) |x_h=x \\right]\" eeimg=\"1\"\u002F\u003E 。至于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q\" alt=\"Q\" eeimg=\"1\"\u002F\u003E function，它和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V\" alt=\"V\" eeimg=\"1\"\u002F\u003E function主要的区别在于多一个自变量 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a\" alt=\"a\" eeimg=\"1\"\u002F\u003E ，即，我们称 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q_h%5E%7B%5Cpi%7D%28x%2Ca%29\" alt=\"Q_h^{\\pi}(x,a)\" eeimg=\"1\"\u002F\u003E 这个函数是在step \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=h\" alt=\"h\" eeimg=\"1\"\u002F\u003E 和state \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=x\" alt=\"x\" eeimg=\"1\"\u002F\u003E 上选择了action \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a\" alt=\"a\" eeimg=\"1\"\u002F\u003E 之后，根据policy \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"\u002F\u003E 所能获得的期望剩余reward。如果你对动态规划（dynamic programming）不熟悉，你可能会问，为什么要定义两个看起来很像的函数。一方面来说，这样子我们很方便就可以写出DP里著名的Bellman equation（或者说optimal induction，直接可以得到最优的policy的表达式）；另一方面来说， 我们也可以说 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V\" alt=\"V\" eeimg=\"1\"\u002F\u003E function就可以刻画DP本身的逻辑。而 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q\" alt=\"Q\" eeimg=\"1\"\u002F\u003E function则可以刻画我们实际算法做 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q\" alt=\"Q\" eeimg=\"1\"\u002F\u003E learning的作用对象。\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cp\u003E本节最后说明，在我们的epsiode MDP setting，每个episode一开始，我们可以不失一般性地认为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=x_1\" alt=\"x_1\" eeimg=\"1\"\u002F\u003E 是被某个adversary任意挑选（arbitrarily picked）的。\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Ch2\u003E\u003Cb\u003E二、基于UCB算法的Q-learning\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E我们注意到，因为只是考虑有限的状态空间和行动空间，所以最优policy是一定存在的（这是DP的经典结果）。那么用 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V\" alt=\"V\" eeimg=\"1\"\u002F\u003E function我们就可以刻画最优的policy \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%5E%2A\" alt=\"\\pi^*\" eeimg=\"1\"\u002F\u003E :即让 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_h%5E%2A%28x%29%3D%5Csup_%7B%5Cpi%7D+V_h%5E%7B%5Cpi%7D%28x%29\" alt=\"V_h^*(x)=\\sup_{\\pi} V_h^{\\pi}(x)\" eeimg=\"1\"\u002F\u003E 对所有 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=x%5Cin+%5Cmathcal%7BS%7D%2Ch%5Cin+%5BH%5D\" alt=\"x\\in \\mathcal{S},h\\in [H]\" eeimg=\"1\"\u002F\u003E 的函数。因此，根据DP的最优迭代公式（Bellman equation），我们知道如下迭代成立：\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bl%7D+%26+V_h%5E%2A%28x%29+%3D+%5Cmax%5Climits_%7Ba%5Cin+%5Cmathcal%7BA%7D%7DQ_h%5E%2A%28x%2Ca%29%2C%5Cforall~x%5Cin+%5Cmathcal%7BS%7D%2Ch%5Cin+%5BH%5D%2C+%5C%5C+%26+Q_h%5E%2A%28x%2Ca%29+%3D+%28r_h%2B%5Cmathbb%7BP%7D_h+V_%7Bh%2B1%7D%5E%2A+%29%28x%2Ca%29%2C%5Cforall~x%5Cin+%5Cmathcal%7BS%7D%2Ca%5Cin+%5Cmathcal%7BA%7D%2Ch%5Cin+%5BH%5D%2C%5C%5C+%26+V_%7BH%2B1%7D%5E%2A%28x%29%3D0%2C%5Cforall~x%5Cin+%5Cmathcal%7BS%7D.+%5Cend%7Barray%7D+++%5Cright.\" alt=\"\\left\\{\\begin{array}{l} &amp; V_h^*(x) = \\max\\limits_{a\\in \\mathcal{A}}Q_h^*(x,a),\\forall~x\\in \\mathcal{S},h\\in [H], \\\\ &amp; Q_h^*(x,a) = (r_h+\\mathbb{P}_h V_{h+1}^* )(x,a),\\forall~x\\in \\mathcal{S},a\\in \\mathcal{A},h\\in [H],\\\\ &amp; V_{H+1}^*(x)=0,\\forall~x\\in \\mathcal{S}. \\end{array}   \\right.\" eeimg=\"1\"\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E注意这里我们用了简化的notation： \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5B%5Cmathbb%7BP%7D_h+V_%7Bh%2B1%7D%5D%28x%2Ca%29%3A%3D%5Cmathbb%7BE%7D_%7Bx%27%5Csim+%5Cmathbb%7BP%7D%28%5Ccdot%7Cx%2Ca%29%7DV_%7Bh%2B1%7D%28x%27%29.\" alt=\"[\\mathbb{P}_h V_{h+1}](x,a):=\\mathbb{E}_{x&#39;\\sim \\mathbb{P}(\\cdot|x,a)}V_{h+1}(x&#39;).\" eeimg=\"1\"\u002F\u003E 如果你不熟悉DP，我们稍微来看一下这个迭代意味着什么。简单来说，核心是迭代的第二步，即最优的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q\" alt=\"Q\" eeimg=\"1\"\u002F\u003E 函数的值应该是当前的reward \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=r_h\" alt=\"r_h\" eeimg=\"1\"\u002F\u003E ，加上最优的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V\" alt=\"V\" eeimg=\"1\"\u002F\u003E 函数从 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=h%2B1\" alt=\"h+1\" eeimg=\"1\"\u002F\u003E 步开始的值（这个在DP里面也叫做cost-to-go）。这其实也是DP所谓的tail-optimality。结合迭代的第一步说了最优的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V\" alt=\"V\" eeimg=\"1\"\u002F\u003E 函数值应该是 最优的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q\" alt=\"Q\" eeimg=\"1\"\u002F\u003E 函数取得最好的action的值，即从后往前推，你在step \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=h\" alt=\"h\" eeimg=\"1\"\u002F\u003E 的时候，最优的policy就是要最大化 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q\" alt=\"Q\" eeimg=\"1\"\u002F\u003E 函数的值，也就是要最大化当前的reward加上从 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=h%2B1\" alt=\"h+1\" eeimg=\"1\"\u002F\u003E 步开始最优policy所带来的期望reward。这点，希望不熟悉DP的同学们好好体会。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-85a1c171703a581ee480e6e9537d494a_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"250\" data-rawheight=\"137\" class=\"content_image\" width=\"250\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;250&#39; height=&#39;137&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"250\" data-rawheight=\"137\" class=\"content_image lazy\" width=\"250\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002F50\u002Fv2-85a1c171703a581ee480e6e9537d494a_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E说明这一点更简单的例子见上图，即所谓的最短路（shortest path）问题。这个问题显然也可以看成是episodic MDP，在每个episode我们都想找到这个固定的图上的最短路，假设每两个节点之间我们事先不知道要花多少时间，而它符合某些随机变量的分布。那么，这个问题的最优policy显然就是满足tail optimality的，这是因为，如果我找到了一套走最短路的方法，那么很显然，即使我走到了途中某个节点，比如B，那么我从B走到F的最短路一定可以通过B-&gt;C + C-&gt;F的最短路（最优policy给出） 或者 B-&gt;D + D-&gt;F的最短路（最优policy给出）来比较得到。\u003C\u002Fp\u003E\u003Cp\u003E那么说到这里，Q-learning的想法也就呼之欲出了。因为我们知道，如果我们能知道 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=r_h%2C%5Cmathbb%7BP%7D_h\" alt=\"r_h,\\mathbb{P}_h\" eeimg=\"1\"\u002F\u003E 的具体分布，我们马上就能求得 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%5E%2A%2CV%5E%2A\" alt=\"Q^*,V^*\" eeimg=\"1\"\u002F\u003E ，也就能求出最优的policy。当然，实际情况中我们规定我们事先并不知道 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=r_h%2C%5Cmathbb%7BP%7D_h\" alt=\"r_h,\\mathbb{P}_h\" eeimg=\"1\"\u002F\u003E 的分布，我们能做的，只有在每个episode中的每步step，根据当前的state选择适当的action，并且根据我们选择的action随机（根据 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmathbb%7BP%7D_h\" alt=\"\\mathbb{P}_h\" eeimg=\"1\"\u002F\u003E ）跳到下个state，并且观察到对应的reward的一个sample。那么，一个很自然的想法，当我们收集了越来越多这些sample之后，我们就可以比较好的去估计这个 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q\" alt=\"Q\" eeimg=\"1\"\u002F\u003E 函数，估计好了这个 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q\" alt=\"Q\" eeimg=\"1\"\u002F\u003E 函数之后，我们的policy其实就是去最大化这个估计的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q\" alt=\"Q\" eeimg=\"1\"\u002F\u003E 函数。嗯，思路其实就这么简单。\u003C\u002Fp\u003E\u003Cp\u003E所以核心问题就是如何去估计这个 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q\" alt=\"Q\" eeimg=\"1\"\u002F\u003E 函数。如果沿用 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"\u002F\u003E -greedy，自然意思就是直接用sample mean（对每个 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=x%2Ca\" alt=\"x,a\" eeimg=\"1\"\u002F\u003E 来说）。而如果要用UCB的话，我们就需要对 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q\" alt=\"Q\" eeimg=\"1\"\u002F\u003E 函数求置信区间。这个自然就比bandit里面求置信区间要复杂了，因为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q\" alt=\"Q\" eeimg=\"1\"\u002F\u003E 现在可不是什么IID的随机变量了，而是跟MDP有关的一个更加复杂的随机变量。好在Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, Michael Jordan这几个人最近确定了准确的形式把UCB的思想套用了过来（见开头的参考文献）。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-6f912680fde2648dcedef448e2f62c93_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1095\" data-rawheight=\"372\" class=\"origin_image zh-lightbox-thumb\" width=\"1095\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-6f912680fde2648dcedef448e2f62c93_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1095&#39; height=&#39;372&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1095\" data-rawheight=\"372\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1095\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-6f912680fde2648dcedef448e2f62c93_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002F50\u002Fv2-6f912680fde2648dcedef448e2f62c93_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-99bdfb07d3d2138c482b9aaa741c726a_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1112\" data-rawheight=\"541\" class=\"origin_image zh-lightbox-thumb\" width=\"1112\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-99bdfb07d3d2138c482b9aaa741c726a_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1112&#39; height=&#39;541&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1112\" data-rawheight=\"541\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1112\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-99bdfb07d3d2138c482b9aaa741c726a_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002F50\u002Fv2-99bdfb07d3d2138c482b9aaa741c726a_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E他们的算法有两个形式，但其实大同小异，区别在于通过不同的集中不等式所需要的不同形式的upper confidence bound。简单来说，Bernstein不等式相比Hoeffding不等式还有对二阶矩的精细控制（即Algorithm 2中的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csigma_h\" alt=\"\\sigma_h\" eeimg=\"1\"\u002F\u003E 项），所以他们的Algorithm 2比起Algorithm 1能得到更好的regret bound。\u003C\u002Fp\u003E\u003Cp\u003E不过我们这里因为只是谈一下大概的思想，所以后面只会具体提及Algorithm 1相关的分析思路。毕竟大的思想其实是一样的。这边我们看Algorithm 1，思路就如前面所说，是很直接的，算法中第7行就是如何对Q函数进行估计，一共有两项组成，第一项可以看成是优化算法中的momentum项，第二项就是核心的UCB bound迭代。这里我们注意到置信区间的长度，忽略log项，大概是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctilde%7BO%7D%5Cleft%28%5Csqrt%7BH%5E3%2Ft%7D%5Cright%29\" alt=\"\\tilde{O}\\left(\\sqrt{H^3\u002Ft}\\right)\" eeimg=\"1\"\u002F\u003E 的， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=t\" alt=\"t\" eeimg=\"1\"\u002F\u003E 是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28x_h%2Ca_h%29\" alt=\"(x_h,a_h)\" eeimg=\"1\"\u002F\u003E 目前有的sample数量。那么当然，这边 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Calpha_t\" alt=\"\\alpha_t\" eeimg=\"1\"\u002F\u003E 步长的选取也是非常关键的。几位大佬机智地发现把步长取成 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Calpha_t%3DO%28H%2Ft%29\" alt=\"\\alpha_t=O(H\u002Ft)\" eeimg=\"1\"\u002F\u003E ，而非传统一阶算法里常用的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Calpha_t%3D1%2Ft\" alt=\"\\alpha_t=1\u002Ft\" eeimg=\"1\"\u002F\u003E 就能得到满意的结果，意思就是单纯的uniform weight是不行的，而要给比较近的项更高的权重才可以。\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Ch2\u003E\u003Cb\u003E三、一些分析\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp\u003E好了，本节我们就稍微具体看下Algorithm 1为什么可以work。尤其比如说步长为什么要选成 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=O%28H%2Ft%29\" alt=\"O(H\u002Ft)\" eeimg=\"1\"\u002F\u003E 。当然这边也不可能照搬所有细节，所以想学完整proof的同学还是建议直接去翻原paper。\u003C\u002Fp\u003E\u003Cp\u003E这边先重申下regret的定义（其实是expected regret,或者按照上次的说法是pseudo regret），很简单，就是\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctext%7BRegret%7D%28K%29+%3D+%5Csum_%7Bk%3D1%7D%5EK+%5BV_1%5E%2A%28x_1%5Ek%29-V_1%5E%7B%5Cpi_k%7D%28x_1%5Ek%29%5D.\" alt=\"\\text{Regret}(K) = \\sum_{k=1}^K [V_1^*(x_1^k)-V_1^{\\pi_k}(x_1^k)].\" eeimg=\"1\"\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E其中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=K\" alt=\"K\" eeimg=\"1\"\u002F\u003E 是episode的总数， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi_k\" alt=\"\\pi_k\" eeimg=\"1\"\u002F\u003E 就是我们算法在episode \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=k\" alt=\"k\" eeimg=\"1\"\u002F\u003E 所用的policy， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=x_1%5Ek\" alt=\"x_1^k\" eeimg=\"1\"\u002F\u003E 就是每个episode \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=k\" alt=\"k\" eeimg=\"1\"\u002F\u003E 最开始初始的state。\u003C\u002Fp\u003E\u003Cp\u003E这边需要用一个额外的notation来说。我们让 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28x_h%5Ek%2Ca_h%5Ek%29\" alt=\"(x_h^k,a_h^k)\" eeimg=\"1\"\u002F\u003E 表示在episode \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=k\" alt=\"k\" eeimg=\"1\"\u002F\u003E 的step \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=h\" alt=\"h\" eeimg=\"1\"\u002F\u003E 我们实际观察到的state和选择的action。然后如果我们让 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q_h%5Ek%2CV_h%5Ek\" alt=\"Q_h^k,V_h^k\" eeimg=\"1\"\u002F\u003E 表示episode \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=k\" alt=\"k\" eeimg=\"1\"\u002F\u003E 刚开始的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q_h%2CV_h\" alt=\"Q_h,V_h\" eeimg=\"1\"\u002F\u003E 函数估计。我们就注意到Algorithm 1的第七行其实可以写成：\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q_h%5E%7Bk%2B1%7D%28x%2Ca%29%3D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bll%7D+%281-%5Calpha_t%29Q_h%5Ek%28x%2Ca%29%2B%5Calpha_t%28r_h%28x%2Ca%29%2BV_%7Bh%2B1%7D%5Ek%28x_%7Bh%2B1%7D%5Ek%29%2Bb_t%29+%26+%5Ctext%7Bif+%7D+%28x%2Ca%29%3D%28x_h%5Ek%2Ca_h%5Ek%29%5C%5C+Q_h%5Ek%28x%2Ca%29+%26+%5Ctext%7Botherwise%7D+%5Cend%7Barray%7D+%5Cright..\" alt=\"Q_h^{k+1}(x,a)=\\left\\{\\begin{array}{ll} (1-\\alpha_t)Q_h^k(x,a)+\\alpha_t(r_h(x,a)+V_{h+1}^k(x_{h+1}^k)+b_t) &amp; \\text{if } (x,a)=(x_h^k,a_h^k)\\\\ Q_h^k(x,a) &amp; \\text{otherwise} \\end{array} \\right..\" eeimg=\"1\"\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E这个式子其实更能反应算法的本质：这也是Q-learning利用多个episode学习的关系式。对于分析来说，我们就希望能说明利用合适的步长 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Calpha_t\" alt=\"\\alpha_t\" eeimg=\"1\"\u002F\u003E ,我们这边估计出来的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q_h%5Ek\" alt=\"Q_h^k\" eeimg=\"1\"\u002F\u003E 相比 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q_h%5E%2A\" alt=\"Q_h^*\" eeimg=\"1\"\u002F\u003E 它的error不会累积地过于快。\u003C\u002Fp\u003E\u003Cp\u003E对于步长，我们再定义这样一些量：\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Calpha_t%5E0%3D%5CPi_%7Bj%3D1%7D%5Et%281-%5Calpha_j%29%2C%5Calpha_t%5Ei%3D%5Calpha_i%5CPi_%7Bj%3D%7Bi%2B1%7D%7D%5Et%281-%5Calpha_j%29\" alt=\"\\alpha_t^0=\\Pi_{j=1}^t(1-\\alpha_j),\\alpha_t^i=\\alpha_i\\Pi_{j={i+1}}^t(1-\\alpha_j)\" eeimg=\"1\"\u002F\u003E ,\u003C\u002Fp\u003E\u003Cp\u003E那么我们可以进一步把上面的式子展开，得到\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q_h%5Ek%28x%2Ca%29%3D%5Calpha_t%5E0H%2B%5Csum_%7Bi%3D1%7D%5Et%5Calpha_t%5Ei%5Cleft%5B+r_h%28x%2Ca%29%2BV_%7Bh%2B1%7D%5E%7Bk_i%7D%28x_%7Bh%2B1%7D%29+%2Bb_i+%5Cright%5D\" alt=\"Q_h^k(x,a)=\\alpha_t^0H+\\sum_{i=1}^t\\alpha_t^i\\left[ r_h(x,a)+V_{h+1}^{k_i}(x_{h+1}) +b_i \\right]\" eeimg=\"1\"\u002F\u003E .\u003C\u002Fp\u003E\u003Cp\u003E这边我们令 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=t\" alt=\"t\" eeimg=\"1\"\u002F\u003E 到第k个episode为止，在每个episode的第h个step，观测到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28x%2Ca%29\" alt=\"(x,a)\" eeimg=\"1\"\u002F\u003E 的次数， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=k_1%2Ck_2%2C%5Cldots%2Ck_t\" alt=\"k_1,k_2,\\ldots,k_t\" eeimg=\"1\"\u002F\u003E 则是之前在step \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=h\" alt=\"h\" eeimg=\"1\"\u002F\u003E 选择action \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=a\" alt=\"a\" eeimg=\"1\"\u002F\u003E 和处在state \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=x\" alt=\"x\" eeimg=\"1\"\u002F\u003E 的episodes。那么我们就发现，其实是这个量 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Calpha_t%5Ei\" alt=\"\\alpha_t^i\" eeimg=\"1\"\u002F\u003E （当然它由 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Calpha_t\" alt=\"\\alpha_t\" eeimg=\"1\"\u002F\u003E 决定）来反应我们的Q-learning算法（Algorithm 1）对之前的UCB bound的权重分配。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-5e941130c8feb808997c4928c0193f8b_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1037\" data-rawheight=\"450\" class=\"origin_image zh-lightbox-thumb\" width=\"1037\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5e941130c8feb808997c4928c0193f8b_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1037&#39; height=&#39;450&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1037\" data-rawheight=\"450\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1037\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-5e941130c8feb808997c4928c0193f8b_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002F50\u002Fv2-5e941130c8feb808997c4928c0193f8b_hd.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E那么这边就有张图，画出了取不同 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Calpha_t\" alt=\"\\alpha_t\" eeimg=\"1\"\u002F\u003E 我们的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Calpha_t%5Ei\" alt=\"\\alpha_t^i\" eeimg=\"1\"\u002F\u003E 随次数增长的曲线。我们注意到相比我们最终选取的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Calpha_t%3D%5Cfrac%7BH%2B1%7D%7BH%2Bt%7D\" alt=\"\\alpha_t=\\frac{H+1}{H+t}\" eeimg=\"1\"\u002F\u003E ， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=1%2Ft\" alt=\"1\u002Ft\" eeimg=\"1\"\u002F\u003E 的步长完全是uniform分配的，不偏不倚，而 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=1%2F%5Csqrt%7Bt%7D\" alt=\"1\u002F\\sqrt{t}\" eeimg=\"1\"\u002F\u003E 的步长则基本把所有权重都分配给了最近的15%左右的sample（对于分析来说，这个步长会导致很大的variance，导致过高的regret）。这么来看，我们选取的步长倒是显得比较中规中矩了~\u003C\u002Fp\u003E\u003Cp\u003E具体来说，我们容易验证当 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Calpha_t%3D%5Cfrac%7BH%2B1%7D%7BH%2Bt%7D\" alt=\"\\alpha_t=\\frac{H+1}{H+t}\" eeimg=\"1\"\u002F\u003E ，我们有 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csum_%7Bt%3Di%7D%5E%7B%5Cinfty%7D%5Calpha_t%5Ei%3D1%2B1%2FH\" alt=\"\\sum_{t=i}^{\\infty}\\alpha_t^i=1+1\u002FH\" eeimg=\"1\"\u002F\u003E 对所有正整数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=i\" alt=\"i\" eeimg=\"1\"\u002F\u003E 成立。而这也是每个episode我们能累计regret的阶，也就是说总共在这种步长下我们的regret累计最多也就是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%281%2B1%2FH%29%5EH\" alt=\"(1+1\u002FH)^H\" eeimg=\"1\"\u002F\u003E 的阶，而这算是个常数了。\u003C\u002Fp\u003E\u003Cp\u003E那么之后的analysis都基于前面的式子，简单来说我们需要能够用 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_h%5Ek-V_h%5E%2A\" alt=\"V_h^k-V_h^*\" eeimg=\"1\"\u002F\u003E 来bound住 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q_h-Q_h%5E%2A\" alt=\"Q_h-Q_h^*\" eeimg=\"1\"\u002F\u003E ，然后利用DP的迭代我们就能推出 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V_h%5Ek-V_h%5E%2A\" alt=\"V_h^k-V_h^*\" eeimg=\"1\"\u002F\u003E ，也就是regret的bound。具体的步骤其实也不算长，当然也不是说就可以一蹴而就，有兴趣的同学们可以自己先尝试推推看，然后再去看大佬在paper里的推法。\u003C\u002Fp\u003E\u003Cp\u003E忽略log项，Algorithm 1可以得到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctilde%7BO%7D%5Cleft%28%5Csqrt%7BH%5E4SAT%7D%5Cright%29\" alt=\"\\tilde{O}\\left(\\sqrt{H^4SAT}\\right)\" eeimg=\"1\"\u002F\u003E 的regret bound（ \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=T%3DKH\" alt=\"T=KH\" eeimg=\"1\"\u002F\u003E 是总共的步数），而Algorithm 2因为用了更精细的集中不等式，可以改进到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctilde%7BO%7D%5Cleft%28%5Csqrt%7BH%5E3SAT%7D%5Cright%29\" alt=\"\\tilde{O}\\left(\\sqrt{H^3SAT}\\right)\" eeimg=\"1\"\u002F\u003E 的regret bound。\u003C\u002Fp\u003E\u003Cp\u003E之后再提一下他们得到的其它一些结果。主要还有两个：\u003C\u002Fp\u003E\u003Cp\u003E1.他们构造了一个MDP的例子并说明 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon-\" alt=\"\\epsilon-\" eeimg=\"1\"\u002F\u003E greedy算法在这个例子上的regret至少是exponential的(exponential in \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=H\" alt=\"H\" eeimg=\"1\"\u002F\u003E )。这确实就说明在更一般的情形下\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cepsilon-\" alt=\"\\epsilon-\" eeimg=\"1\"\u002F\u003E greedy和UCB算法在理论上的表现被进一步拉开了。。\u003C\u002Fp\u003E\u003Cp\u003E2.他们也给了一个一般情形的lower bound，对任意算法，存在一个 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=H\" alt=\"H\" eeimg=\"1\"\u002F\u003E epsiode的MDP，使得算法的regret至少是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5COmega%5Cleft%28+H%5Csqrt%7BSAT%7D++%5Cright%29\" alt=\"\\Omega\\left( H\\sqrt{SAT}  \\right)\" eeimg=\"1\"\u002F\u003E 的。所以我们看到他们的Algorithm 2只和这个lower bound差了 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csqrt%7BH%7D\" alt=\"\\sqrt{H}\" eeimg=\"1\"\u002F\u003E ，已经非常接近了。接下来就看谁能把这个gap close了，呵呵。\u003C\u002Fp\u003E\u003Cp\u003E最后多说一句，其实bandit文献里面经常需要自己去”造“一些特殊的不等式然后算法配合之达到比较好的regret bound，这往往都是因为只控制一阶矩不够精细，需要引入二阶矩的一些精确估计才ok。当然可能有些人会觉得这些纯属是在玩bound，不过在bandit领域里应该都算是共识了。。。\u003C\u002Fp\u003E","editableContent":"","excerpt":"本回答来自我的知乎专栏上的文章。主要说一下Q-learning中运用的upper confidence bound\u002F置信度的思想。主要的参考文献为： Jin, C., Allen-Zhu, Z., Bubeck, S. and Jordan, M.I., 2018. &#34;Is Q-learning provably efficient?.&#34; Advances in Neural Information Processing Systems. 2018.一、Bandit算法回顾，MDP简单介绍首先，我们指出在bandit算法中主要有两类流行的算法，一类是贪心算法（如uniform exploration， [公式] …","collapsedBy":"nobody","collapseReason":"","annotationAction":null,"markInfos":[],"relevantInfo":{"isRelevant":false,"relevantType":"","relevantText":""},"suggestEdit":{"reason":"","status":false,"tip":"","title":"","unnormalDetails":{"status":"","description":"","reason":"","reasonId":0,"note":""},"url":""},"isLabeled":false,"rewardInfo":{"canOpenReward":false,"isRewardable":true,"rewardMemberCount":0,"rewardTotalMoney":0,"tagline":"cute learning"},"relationship":{"isAuthor":false,"isAuthorized":false,"isNothelp":false,"isThanked":false,"isRecognized":false,"voting":0,"upvotedFollowees":[]}}},"articles":{},"columns":{},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{}},"currentUser":"","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false}},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{}},"env":{"ab":{"config":{"experiments":[{"expId":"launch-us_foltopic_user-10","expPrefix":"us_foltopic_user","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false}],"params":[{"id":"zr_es_update","type":"String","value":"0","chainId":"_all_"},{"id":"zr_search_xgb","type":"String","value":"0","chainId":"_all_"},{"id":"se_billboardsearch","type":"String","value":"0","chainId":"_all_"},{"id":"se_featured","type":"String","value":"1","chainId":"_all_"},{"id":"se_ri","type":"String","value":"0","chainId":"_all_"},{"id":"se_zu_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"soc_bignew","type":"String","value":"1","chainId":"_all_"},{"id":"top_hotcommerce","type":"String","value":"1","chainId":"_all_"},{"id":"top_v_album","type":"String","value":"1","chainId":"_all_"},{"id":"ug_fw_answ_aut_1","type":"String","value":"0","chainId":"_all_"},{"id":"ls_videoad","type":"String","value":"0","chainId":"_all_"},{"id":"qa_test","type":"String","value":"0","chainId":"_all_"},{"id":"se_expired_ob","type":"String","value":"0","chainId":"_all_"},{"id":"se_payconsult","type":"String","value":"0","chainId":"_all_"},{"id":"ug_zero_follow","type":"String","value":"0","chainId":"_all_"},{"id":"pf_fuceng","type":"String","value":"1","chainId":"_all_"},{"id":"se_subtext","type":"String","value":"0","chainId":"_all_"},{"id":"top_new_feed","type":"String","value":"5","chainId":"_all_"},{"id":"zr_ebook_chapter","type":"String","value":"0","chainId":"_all_"},{"id":"tp_m_intro_re_topic","type":"String","value":"1","chainId":"_all_"},{"id":"top_ydyq","type":"String","value":"X","chainId":"_all_"},{"id":"ug_goodcomment_0","type":"String","value":"1","chainId":"_all_"},{"id":"web_answerlist_ad","type":"String","value":"0"},{"id":"pf_newguide_vertical","type":"String","value":"0","chainId":"_all_"},{"id":"se_ltr_v002","type":"String","value":"1","chainId":"_all_"},{"id":"se_ltr_v010","type":"String","value":"0","chainId":"_all_"},{"id":"se_waterfall","type":"String","value":"0","chainId":"_all_"},{"id":"ug_follow_answerer_0","type":"String","value":"0","chainId":"_all_"},{"id":"web_wx_block","type":"String","value":"2"},{"id":"gue_new_special_page","type":"String","value":"0"},{"id":"se_colorfultab","type":"String","value":"1","chainId":"_all_"},{"id":"se_preset_tech","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_lastread","type":"String","value":"0","chainId":"_all_"},{"id":"soc_special","type":"String","value":"0","chainId":"_all_"},{"id":"zr_album_exp","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sticky_android","type":"String","value":"0","chainId":"_all_"},{"id":"se_auto_syn","type":"String","value":"0","chainId":"_all_"},{"id":"se_college","type":"String","value":"default","chainId":"_all_"},{"id":"top_native_answer","type":"String","value":"1","chainId":"_all_"},{"id":"top_test_4_liguangyi","type":"String","value":"1","chainId":"_all_"},{"id":"top_recall_exp_v2","type":"String","value":"1","chainId":"_all_"},{"id":"li_album3_ab","type":"String","value":"0","chainId":"_all_"},{"id":"li_hot_score_ab","type":"String","value":"0","chainId":"_all_"},{"id":"pf_foltopic_usernum","type":"String","value":"50","chainId":"_all_"},{"id":"top_reason","type":"String","value":"1","chainId":"_all_"},{"id":"se_ios_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"se_whitelist","type":"String","value":"0","chainId":"_all_"},{"id":"web_column_auto_invite","type":"String","value":"0"},{"id":"zr_art_rec","type":"String","value":"base","chainId":"_all_"},{"id":"gue_anonymous","type":"String","value":"show"},{"id":"li_ebook_detail","type":"String","value":"1","chainId":"_all_"},{"id":"li_tjys_ec_ab","type":"String","value":"0","chainId":"_all_"},{"id":"qa_answerlist_ad","type":"String","value":"0","chainId":"_all_"},{"id":"gue_zhuantikapian","type":"String","value":"0"},{"id":"se_title_only","type":"String","value":"0","chainId":"_all_"},{"id":"se_zu_recommend","type":"String","value":"0","chainId":"_all_"},{"id":"zr_infinity_a_u","type":"String","value":"close","chainId":"_all_"},{"id":"se_terminate","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_metacard","type":"String","value":"1","chainId":"_all_"},{"id":"ug_follow_answerer","type":"String","value":"0","chainId":"_all_"},{"id":"zr_album_chapter_exp","type":"String","value":"0","chainId":"_all_"},{"id":"se_topicdirect","type":"String","value":"2","chainId":"_all_"},{"id":"tp_sft","type":"String","value":"a","chainId":"_all_"},{"id":"web_sem_ab","type":"String","value":"0"},{"id":"se_agency","type":"String","value":" 0","chainId":"_all_"},{"id":"se_limit","type":"String","value":"0","chainId":"_all_"},{"id":"se_page_limit_20","type":"String","value":"1","chainId":"_all_"},{"id":"se_p_slideshow","type":"String","value":"0","chainId":"_all_"},{"id":"soc_update","type":"String","value":"1","chainId":"_all_"},{"id":"web_question_invite","type":"String","value":"B"},{"id":"zr_km_slot_style","type":"String","value":"event_card","chainId":"_all_"},{"id":"se_likebutton","type":"String","value":"0","chainId":"_all_"},{"id":"se_time_threshold","type":"String","value":"0","chainId":"_all_"},{"id":"top_quality","type":"String","value":"0","chainId":"_all_"},{"id":"li_price_test","type":"String","value":"1","chainId":"_all_"},{"id":"li_qa_cover","type":"String","value":"old","chainId":"_all_"},{"id":"pf_creator_card","type":"String","value":"1","chainId":"_all_"},{"id":"se_backsearch","type":"String","value":"0","chainId":"_all_"},{"id":"tp_header_style","type":"String","value":"1","chainId":"_all_"},{"id":"tp_meta_card","type":"String","value":"0","chainId":"_all_"},{"id":"pf_noti_entry_num","type":"String","value":"0","chainId":"_all_"},{"id":"se_mobileweb","type":"String","value":"1","chainId":"_all_"},{"id":"se_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"se_topic_pu","type":"String","value":"0","chainId":"_all_"},{"id":"se_lottery","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_xgb_model","type":"String","value":"old","chainId":"_all_"},{"id":"ug_follow_topic_1","type":"String","value":"2","chainId":"_all_"},{"id":"se_webtimebox","type":"String","value":"0","chainId":"_all_"},{"id":"top_gr_ab","type":"String","value":"0","chainId":"_all_"},{"id":"se_rr","type":"String","value":"0","chainId":"_all_"},{"id":"li_album_liutongab","type":"String","value":"0","chainId":"_all_"},{"id":"li_android_vip","type":"String","value":"0","chainId":"_all_"},{"id":"pf_feed","type":"String","value":"1","chainId":"_all_"},{"id":"se_movietab","type":"String","value":"0","chainId":"_all_"},{"id":"se_time_score","type":"String","value":"1","chainId":"_all_"},{"id":"top_ebook","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_hotctr","type":"String","value":"1","chainId":"_all_"},{"id":"web_n_web_msg","type":"String","value":"0"},{"id":"se_search_feed","type":"String","value":"N","chainId":"_all_"},{"id":"top_universalebook","type":"String","value":"1","chainId":"_all_"},{"id":"ls_fmp4","type":"String","value":"0","chainId":"_all_"},{"id":"se_famous","type":"String","value":"1","chainId":"_all_"},{"id":"se_new_topic","type":"String","value":"0","chainId":"_all_"},{"id":"se_payconsult_click","type":"String","value":"0","chainId":"_all_"},{"id":"li_qa_new_cover","type":"String","value":"0","chainId":"_all_"},{"id":"se_site_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"ug_newtag","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_metacard_top","type":"String","value":"top","chainId":"_all_"},{"id":"tsp_childbillboard","type":"String","value":"1","chainId":"_all_"},{"id":"web_answer_list_ad","type":"String","value":"1"},{"id":"li_ts_sample","type":"String","value":"old","chainId":"_all_"},{"id":"se_pyc_click2","type":"String","value":"1","chainId":"_all_"},{"id":"se_webrs","type":"String","value":"1","chainId":"_all_"},{"id":"top_rank","type":"String","value":"0","chainId":"_all_"},{"id":"ug_zero_follow_0","type":"String","value":"0","chainId":"_all_"},{"id":"web_heifetz_grow_ad","type":"String","value":"1"},{"id":"zr_rel_search","type":"String","value":"base","chainId":"_all_"},{"id":"se_topic_express","type":"String","value":"0","chainId":"_all_"},{"id":"se_webmajorob","type":"String","value":"0","chainId":"_all_"},{"id":"top_recall_exp_v1","type":"String","value":"1","chainId":"_all_"},{"id":"tp_sft_v2","type":"String","value":" a","chainId":"_all_"},{"id":"web_answer_update","type":"String","value":"0"},{"id":"li_se_new_fields","type":"String","value":"0","chainId":"_all_"},{"id":"se_ad_index","type":"String","value":"10","chainId":"_all_"},{"id":"se_amovietab","type":"String","value":"0","chainId":"_all_"},{"id":"top_recall_deep_user","type":"String","value":"1","chainId":"_all_"},{"id":"tp_qa_toast","type":"String","value":"1","chainId":"_all_"},{"id":"zr_video_recall","type":"String","value":"current_recall","chainId":"_all_"},{"id":"ls_new_upload","type":"String","value":"0","chainId":"_all_"},{"id":"top_root","type":"String","value":"0","chainId":"_all_"},{"id":"zr_ans_rec","type":"String","value":"gbrank","chainId":"_all_"},{"id":"zr_video_rank","type":"String","value":"current_rank","chainId":"_all_"},{"id":"se_college_cm","type":"String","value":"0","chainId":"_all_"},{"id":"se_wannasearch","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_style","type":"String","value":"base","chainId":"_all_"},{"id":"zr_km_answer","type":"String","value":"open_cvr","chainId":"_all_"},{"id":"zr_video_rank_nn","type":"String","value":"current_rank","chainId":"_all_"},{"id":"se_timebox_num","type":"String","value":"3","chainId":"_all_"},{"id":"se_websearch","type":"String","value":"3","chainId":"_all_"},{"id":"soc_bigone","type":"String","value":"0","chainId":"_all_"},{"id":"top_vipconsume","type":"String","value":"1","chainId":"_all_"}],"chains":[{"chainId":"_all_"}]},"triggers":{}},"userAgent":{"Edge":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":false,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"isWebView":false,"origin":"Mozilla\u002F5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F75.0.3770.100 Safari\u002F537.36"},"ctx":{"path":"\u002Fquestion\u002F26408259"},"trafficSource":"production","edition":{"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false},"theme":"light","enableShortcut":true,"referer":"https:\u002F\u002Fwww.google.com\u002F","conf":{},"ipInfo":{},"logged":false,"tdkInfo":{}},"me":{"accountInfoLoadStatus":{},"organizationProfileStatus":{},"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"active":{"sendDigitsError":null,"activeConfirmSucceeded":null,"activeConfirmError":null},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false,"captchaBase64String":null,"captchaValidationMessage":null,"loginCaptchaExpires":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"question":{"followers":{},"concernedFollowers":{},"answers":{"26408259":{"isFetching":false,"isDrained":false,"ids":[123230350,86915117,577272724,88120604,467132543],"newIds":[123230350,86915117,577272724,88120604,467132543],"totals":19,"isPrevDrained":true,"previous":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F26408259\u002Fanswers?include=data%5B%2A%5D.is_normal%2Cadmin_closed_comment%2Creward_info%2Cis_collapsed%2Cannotation_action%2Cannotation_detail%2Ccollapse_reason%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Ccreated_time%2Cupdated_time%2Creview_info%2Crelevant_info%2Cquestion%2Cexcerpt%2Crelationship.is_authorized%2Cis_author%2Cvoting%2Cis_thanked%2Cis_nothelp%2Cis_labeled%2Cis_recognized%2Cpaid_info%2Cpaid_info_content%3Bdata%5B%2A%5D.mark_infos%5B%2A%5D.url%3Bdata%5B%2A%5D.author.follower_count%2Cbadge%5B%2A%5D.topics&limit=5&offset=0&platform=desktop&sort_by=default","next":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Fquestions\u002F26408259\u002Fanswers?include=data%5B%2A%5D.is_normal%2Cadmin_closed_comment%2Creward_info%2Cis_collapsed%2Cannotation_action%2Cannotation_detail%2Ccollapse_reason%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Ccreated_time%2Cupdated_time%2Creview_info%2Crelevant_info%2Cquestion%2Cexcerpt%2Crelationship.is_authorized%2Cis_author%2Cvoting%2Cis_thanked%2Cis_nothelp%2Cis_labeled%2Cis_recognized%2Cpaid_info%2Cpaid_info_content%3Bdata%5B%2A%5D.mark_infos%5B%2A%5D.url%3Bdata%5B%2A%5D.author.follower_count%2Cbadge%5B%2A%5D.topics&limit=5&offset=5&platform=desktop&sort_by=default"}},"hiddenAnswers":{},"updatedAnswers":{},"collapsedAnswers":{},"notificationAnswers":{},"invitedQuestions":{"total":{"count":null,"isEnd":false,"isLoading":false,"questions":[]},"followees":{"count":null,"isEnd":false,"isLoading":false,"questions":[]}},"laterQuestions":{"count":null,"globalWriteAnimate":false,"isEnd":false,"isLoading":false,"questions":[]},"waitingQuestions":{"hot":{"isEnd":false,"isLoading":false,"questions":[]},"value":{"isEnd":false,"isLoading":false,"questions":[]},"newest":{"isEnd":false,"isLoading":false,"questions":[]},"easy":{"isEnd":false,"isLoading":false,"questions":[]}},"invitationCandidates":{},"inviters":{},"invitees":{},"similarQuestions":{},"relatedCommodities":{},"recommendReadings":{},"bio":{},"brand":{},"permission":{},"adverts":{"3":{"ad":{"adVerb":"","brand":{"id":0,"logo":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b02efb32af2aada63f09e5d0a275df75_250x250.jpeg","name":"Togocareer"},"category":1,"clickTracks":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?ar=0.800580626531413&nt=0&idi=2006&tu=http%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi.html%3Fsourcezhihu%26id%3DPC-1120&ed=CjEEfh4wM315RmIGUjN_UUYkCCsBbm4heEF_XlJoeB0PdQF5CnZicn4RawQXMTYNWXYPbFkqPn18EWVXAGlqQAJ4DHkPdWpmLUxvVQdmdQZcY1o-BXN8IyEYZ1cXMjwNWXENbFQ2Z3C4D1cZGFWAww%3D%3D&ui=137.43.169.248&pdi=1523617890848616&ts=1562625615&pf=4&ut=92341debfd514f78b8af95260264b2f7&au=4280"],"closeTrack":"https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?nt=0&idi=2006&ui=137.43.169.248&ar=0.800580626531413&ut=92341debfd514f78b8af95260264b2f7&pdi=1523617890848616&au=4280&tu=http%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi.html%3Fsourcezhihu%26id%3DPC-1120&ts=1562625615&ed=CjEEfR4wM315RmIGUjN_UUYkCCsBbm4heEF_XlJoeB0PdQF5CnZicn4RawQXMTYNWXYPbFkqPn18EWVXAGlqQAJ4DHkPdWpmLUxvVQdmdQZcY1o-BXN8IyEYZ1cXMjwNWXENbFQ2Z3BWPb_6RfrC-Q%3D%3D&pf=4","closeTracks":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?nt=0&idi=2006&ui=137.43.169.248&ar=0.800580626531413&ut=92341debfd514f78b8af95260264b2f7&pdi=1523617890848616&au=4280&tu=http%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi.html%3Fsourcezhihu%26id%3DPC-1120&ts=1562625615&ed=CjEEfR4wM315RmIGUjN_UUYkCCsBbm4heEF_XlJoeB0PdQF5CnZicn4RawQXMTYNWXYPbFkqPn18EWVXAGlqQAJ4DHkPdWpmLUxvVQdmdQZcY1o-BXN8IyEYZ1cXMjwNWXENbFQ2Z3BWPb_6RfrC-Q%3D%3D&pf=4"],"conversionTracks":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?nt=0&ui=137.43.169.248&ts=1562625615&ed=CjEEfx4wM315RmIGUjN_UUYkCCsBbm4heEF_XlJoeB0PdQF5CnZicn4RawQXMTYNWXYPbFkqPn18EWVXAGlqQAJ4DHkPdWpmLUxvVQdmdQZcY1o-BXN8IyEYZ1cXMjwNWXENbFQ2Z3DkyiYnHhdKng%3D%3D&idi=2006&ut=92341debfd514f78b8af95260264b2f7&pf=4&tu=http%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi.html%3Fsourcezhihu%26id%3DPC-1120&pdi=1523617890848616&au=4280&ar=0.800580626531413"],"creatives":[{"appPromotionUrl":"","brand":{"id":0,"logo":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b02efb32af2aada63f09e5d0a275df75_250x250.jpeg","name":"Togocareer"},"cta":{"value":"查看详情"},"description":"近期热门招聘公司：金融，咨询，五百强外资，互联网等各大行业，从零到卓越，全流程专业化求职，一站式服务留学生。","footer":{"value":""},"landingUrl":"http:\u002F\u002Fwww.togocareer.com\u002Fhaiguiqiuzhi.html?sourcezhihu&id=PC-1120","title":"作为应届留学生，回国后通过哪些途径找工作的？各行业招聘情况如何"}],"debugTracks":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?ut=92341debfd514f78b8af95260264b2f7&tu=http%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi.html%3Fsourcezhihu%26id%3DPC-1120&ed=CjEEcx4wM315RmIGUjN_UUYkCCsBbm4heEF_XlJoeB0PdQF5CnZicn4RawQXMTYNWXYPbFkqPn18EWVXAGlqQAJ4DHkPdWpmLUxvVQdmdQZcY1o-BXN8IyEYZ1cXMjwNWXENbFQ2Z3A9ZHZTs_Clgg%3D%3D&pdi=1523617890848616&ts=1562625615&idi=2006&au=4280&ui=137.43.169.248&nt=0&pf=4&ar=0.800580626531413"],"displayAdvertisingTag":true,"experimentInfo":"{\"ad_card_close_hotspot\":{\"close_area\":\"old\"},\"ad_follow\":{\"is_new_page\":0},\"canvas_form\":{\"new_privacy\":\"1\"},\"fe\":{\"fp_click\":0,\"fp_video_isactive\":0,\"fp_video_isplay\":0,\"fp_video_style\":\"nil\"},\"feed_new_page\":{\"new_page\":\"none\"},\"l_a_p\":{\"p_as_prefetch\":\"0\"},\"l_ad_30\":{\"new_30\":\"0\"},\"l_ad_314_d\":{\"p_ad_314_d\":3},\"l_c_color\":{\"p_c_color\":\"gray\"},\"l_c_st\":{\"p_c_st\":\"0\"},\"l_c_st_31\":{\"p_c_st_31\":\"none\"},\"l_c_st_bi_30\":{\"p_c_st_bi_30\":\"0\"},\"l_c_st_pw_30\":{\"p_c_st_pw_30\":\"0\"},\"l_c_st_pw_8_r\":{\"p_c_st_pw_8_r\":\"0\"},\"l_c_st_v_30\":{\"p_c_st_v_30\":\"0\"},\"l_c_style\":{\"p_c_style\":\"1\"},\"l_creative_st_bi_8\":{\"p_creative_st_bi_8\":\"0\"},\"l_creative_st_v_8\":{\"p_creative_st_v_8\":\"0\"},\"l_f_h\":{\"p_f_h\":\"0\"},\"l_m_bfa\":{\"p_m_bfa\":\"a\"},\"l_m_cf_2\":{\"p_m_cf_2\":\"0\"},\"l_m_cf_3\":{\"p_m_cf_3\":\"0\"},\"l_m_cf_4\":{\"p_m_cf_4\":\"0\"},\"l_m_cf_5\":{\"p_m_cf_5\":\"0\"},\"l_m_cv_3\":{\"p_m_cv_3\":\"0\"},\"l_m_cv_5\":{\"p_m_cv_5\":\"0\"},\"lps_switch\":{\"p_lps_switch\":0,\"p_lps_switch_android\":0},\"lps_switch2\":{\"p_lps_switch2\":\"0\"},\"morph_switch\":{\"p_morph_switch\":\"on\"}}","id":247019,"impressionTracks":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?pdi=1523617890848616&ar=0.800580626531413&tu=http%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi.html%3Fsourcezhihu%26id%3DPC-1120&nt=0&ts=1562625615&ut=92341debfd514f78b8af95260264b2f7&pf=4&ed=CjEEeB4wM315RmIGUjN_UUYkCCsBbm4heEF_XlJoeB0PdQF5CnZicn4RawQXMTYNWXYPbFkqPn18EWVXAGlqQAJ4DHkPdWpmLUxvVQdmdQZcY1o-BXN8IyEYZ1cXMjwNWXENbFQ2Z3AWvVXn5sBXlw%3D%3D&idi=2006&au=4280&ui=137.43.169.248"],"isEvergreen":false,"isNewWebview":true,"isSpeeding":false,"isWebp":false,"landPrefetch":false,"name":"","nativePrefetch":true,"partyId":-2,"revertCloseTrack":"https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?idi=2006&ui=137.43.169.248&pdi=1523617890848616&ed=CjEEch4wM315RmIGUjN_UUYkCCsBbm4heEF_XlJoeB0PdQF5CnZicn4RawQXMTYNWXYPbFkqPn18EWVXAGlqQAJ4DHkPdWpmLUxvVQdmdQZcY1o-BXN8IyEYZ1cXMjwNWXENbFQ2Z3Cav5DqpuR57Q%3D%3D&au=4280&ut=92341debfd514f78b8af95260264b2f7&ar=0.800580626531413&ts=1562625615&pf=4&tu=http%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi.html%3Fsourcezhihu%26id%3DPC-1120&nt=0","template":"web_word","viewTrack":"https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?au=4280&tu=http%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi.html%3Fsourcezhihu%26id%3DPC-1120&pdi=1523617890848616&ed=CjEEeR4wM315RmIGUjN_UUYkCCsBbm4heEF_XlJoeB0PdQF5CnZicn4RawQXMTYNWXYPbFkqPn18EWVXAGlqQAJ4DHkPdWpmLUxvVQdmdQZcY1o-BXN8IyEYZ1cXMjwNWXENbFQ2Z3BFBNBBa0BJEQ%3D%3D&idi=2006&pf=4&ar=0.800580626531413&ts=1562625615&ut=92341debfd514f78b8af95260264b2f7&ui=137.43.169.248&nt=0","viewTracks":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?au=4280&tu=http%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi.html%3Fsourcezhihu%26id%3DPC-1120&pdi=1523617890848616&ed=CjEEeR4wM315RmIGUjN_UUYkCCsBbm4heEF_XlJoeB0PdQF5CnZicn4RawQXMTYNWXYPbFkqPn18EWVXAGlqQAJ4DHkPdWpmLUxvVQdmdQZcY1o-BXN8IyEYZ1cXMjwNWXENbFQ2Z3BFBNBBa0BJEQ%3D%3D&idi=2006&pf=4&ar=0.800580626531413&ts=1562625615&ut=92341debfd514f78b8af95260264b2f7&ui=137.43.169.248&nt=0"],"zaAdInfo":"COuJDxDsASIBMV2NR7pO","zaAdInfoJson":"{\"ad_id\":247019,\"ad_zone_id\":236,\"category\":\"1\",\"timestamp\":1562625700}"},"adjson":"{\"ads\":[{\"id\":247019,\"ad_zone_id\":236,\"template\":\"web_word\",\"impression_tracks\":[\"https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?ut=92341debfd514f78b8af95260264b2f7\\u0026pdi=1523617890848616\\u0026idi=2006\\u0026nt=0\\u0026ui=137.43.169.248\\u0026pf=4\\u0026au=4280\\u0026tu=http%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi.html%3Fsourcezhihu%26id%3DPC-1120\\u0026ar=0.800580626531413\\u0026ed=CjEEeB4wM315RmIGUjN_UUYkCCsBbm4heEF_XlJoeB0PdQF5CnZicn4RawQXMTYNWXYPbFkqPn18EWVXAGlqQAJ4DHkPdWpmLUxvVQdmdQZcY1o-BXN8IyEYZ1cXMjwNWXENbFQ2Z3AWvVXn5sBXlw%3D%3D\\u0026ts=1562625615\"],\"view_tracks\":[\"https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?ed=CjEEeR4wM315RmIGUjN_UUYkCCsBbm4heEF_XlJoeB0PdQF5CnZicn4RawQXMTYNWXYPbFkqPn18EWVXAGlqQAJ4DHkPdWpmLUxvVQdmdQZcY1o-BXN8IyEYZ1cXMjwNWXENbFQ2Z3BFBNBBa0BJEQ%3D%3D\\u0026ts=1562625615\\u0026au=4280\\u0026tu=http%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi.html%3Fsourcezhihu%26id%3DPC-1120\\u0026ui=137.43.169.248\\u0026ut=92341debfd514f78b8af95260264b2f7\\u0026nt=0\\u0026pdi=1523617890848616\\u0026ar=0.800580626531413\\u0026pf=4\\u0026idi=2006\"],\"click_tracks\":[\"https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?ed=CjEEfh4wM315RmIGUjN_UUYkCCsBbm4heEF_XlJoeB0PdQF5CnZicn4RawQXMTYNWXYPbFkqPn18EWVXAGlqQAJ4DHkPdWpmLUxvVQdmdQZcY1o-BXN8IyEYZ1cXMjwNWXENbFQ2Z3C4D1cZGFWAww%3D%3D\\u0026au=4280\\u0026ut=92341debfd514f78b8af95260264b2f7\\u0026pdi=1523617890848616\\u0026ar=0.800580626531413\\u0026pf=4\\u0026tu=http%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi.html%3Fsourcezhihu%26id%3DPC-1120\\u0026ui=137.43.169.248\\u0026ts=1562625615\\u0026idi=2006\\u0026nt=0\"],\"close_tracks\":[\"https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?pf=4\\u0026idi=2006\\u0026tu=http%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi.html%3Fsourcezhihu%26id%3DPC-1120\\u0026nt=0\\u0026ui=137.43.169.248\\u0026ut=92341debfd514f78b8af95260264b2f7\\u0026ts=1562625615\\u0026ed=CjEEfR4wM315RmIGUjN_UUYkCCsBbm4heEF_XlJoeB0PdQF5CnZicn4RawQXMTYNWXYPbFkqPn18EWVXAGlqQAJ4DHkPdWpmLUxvVQdmdQZcY1o-BXN8IyEYZ1cXMjwNWXENbFQ2Z3BWPb_6RfrC-Q%3D%3D\\u0026ar=0.800580626531413\\u0026au=4280\\u0026pdi=1523617890848616\"],\"debug_tracks\":[\"https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?ar=0.800580626531413\\u0026ed=CjEEcx4wM315RmIGUjN_UUYkCCsBbm4heEF_XlJoeB0PdQF5CnZicn4RawQXMTYNWXYPbFkqPn18EWVXAGlqQAJ4DHkPdWpmLUxvVQdmdQZcY1o-BXN8IyEYZ1cXMjwNWXENbFQ2Z3A9ZHZTs_Clgg%3D%3D\\u0026tu=http%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi.html%3Fsourcezhihu%26id%3DPC-1120\\u0026ui=137.43.169.248\\u0026ts=1562625615\\u0026ut=92341debfd514f78b8af95260264b2f7\\u0026au=4280\\u0026pdi=1523617890848616\\u0026idi=2006\\u0026nt=0\\u0026pf=4\"],\"conversion_tracks\":[\"https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper?idi=2006\\u0026nt=0\\u0026ut=92341debfd514f78b8af95260264b2f7\\u0026pdi=1523617890848616\\u0026ar=0.800580626531413\\u0026ed=CjEEfx4wM315RmIGUjN_UUYkCCsBbm4heEF_XlJoeB0PdQF5CnZicn4RawQXMTYNWXYPbFkqPn18EWVXAGlqQAJ4DHkPdWpmLUxvVQdmdQZcY1o-BXN8IyEYZ1cXMjwNWXENbFQ2Z3DkyiYnHhdKng%3D%3D\\u0026tu=http%3A%2F%2Fwww.togocareer.com%2Fhaiguiqiuzhi.html%3Fsourcezhihu%26id%3DPC-1120\\u0026pf=4\\u0026au=4280\\u0026ui=137.43.169.248\\u0026ts=1562625615\"],\"za_ad_info\":\"COuJDxDsASIBMV2NR7pO\",\"za_ad_info_json\":\"{\\\"ad_id\\\":247019,\\\"ad_zone_id\\\":236,\\\"category\\\":\\\"1\\\",\\\"timestamp\\\":1562625700}\",\"creatives\":[{\"id\":266967,\"asset\":{\"brand_name\":\"Togocareer\",\"brand_logo\":\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-b02efb32af2aada63f09e5d0a275df75_250x250.jpeg\",\"title\":\"作为应届留学生，回国后通过哪些途径找工作的？各行业招聘情况如何\",\"desc\":\"近期热门招聘公司：金融，咨询，五百强外资，互联网等各大行业，从零到卓越，全流程专业化求职，一站式服务留学生。\",\"landing_url\":\"http:\u002F\u002Fwww.togocareer.com\u002Fhaiguiqiuzhi.html?sourcezhihu\\u0026id=PC-1120\",\"img_size\":0,\"cta\":\"查看详情\"}}],\"expand\":{\"display_advertising_tag\":true,\"is_new_webview\":true,\"is_cdn_speeding\":false},\"experiment_info\":\"{\\\"ad_card_close_hotspot\\\":{\\\"close_area\\\":\\\"old\\\"},\\\"ad_follow\\\":{\\\"is_new_page\\\":0},\\\"canvas_form\\\":{\\\"new_privacy\\\":\\\"1\\\"},\\\"fe\\\":{\\\"fp_click\\\":0,\\\"fp_video_isactive\\\":0,\\\"fp_video_isplay\\\":0,\\\"fp_video_style\\\":\\\"nil\\\"},\\\"feed_new_page\\\":{\\\"new_page\\\":\\\"none\\\"},\\\"l_a_p\\\":{\\\"p_as_prefetch\\\":\\\"0\\\"},\\\"l_ad_30\\\":{\\\"new_30\\\":\\\"0\\\"},\\\"l_ad_314_d\\\":{\\\"p_ad_314_d\\\":3},\\\"l_c_color\\\":{\\\"p_c_color\\\":\\\"gray\\\"},\\\"l_c_st\\\":{\\\"p_c_st\\\":\\\"0\\\"},\\\"l_c_st_31\\\":{\\\"p_c_st_31\\\":\\\"none\\\"},\\\"l_c_st_bi_30\\\":{\\\"p_c_st_bi_30\\\":\\\"0\\\"},\\\"l_c_st_pw_30\\\":{\\\"p_c_st_pw_30\\\":\\\"0\\\"},\\\"l_c_st_pw_8_r\\\":{\\\"p_c_st_pw_8_r\\\":\\\"0\\\"},\\\"l_c_st_v_30\\\":{\\\"p_c_st_v_30\\\":\\\"0\\\"},\\\"l_c_style\\\":{\\\"p_c_style\\\":\\\"1\\\"},\\\"l_creative_st_bi_8\\\":{\\\"p_creative_st_bi_8\\\":\\\"0\\\"},\\\"l_creative_st_v_8\\\":{\\\"p_creative_st_v_8\\\":\\\"0\\\"},\\\"l_f_h\\\":{\\\"p_f_h\\\":\\\"0\\\"},\\\"l_m_bfa\\\":{\\\"p_m_bfa\\\":\\\"a\\\"},\\\"l_m_cf_2\\\":{\\\"p_m_cf_2\\\":\\\"0\\\"},\\\"l_m_cf_3\\\":{\\\"p_m_cf_3\\\":\\\"0\\\"},\\\"l_m_cf_4\\\":{\\\"p_m_cf_4\\\":\\\"0\\\"},\\\"l_m_cf_5\\\":{\\\"p_m_cf_5\\\":\\\"0\\\"},\\\"l_m_cv_3\\\":{\\\"p_m_cv_3\\\":\\\"0\\\"},\\\"l_m_cv_5\\\":{\\\"p_m_cv_5\\\":\\\"0\\\"},\\\"lps_switch\\\":{\\\"p_lps_switch\\\":0,\\\"p_lps_switch_android\\\":0},\\\"lps_switch2\\\":{\\\"p_lps_switch2\\\":\\\"0\\\"},\\\"morph_switch\\\":{\\\"p_morph_switch\\\":\\\"on\\\"}}\"}]}"}},"advancedStyle":{},"commonAnswerCount":0,"hiddenAnswerCount":0,"meta":{},"autoInvitation":{},"simpleConcernedFollowers":{},"draftStatus":{},"disclaimers":{}},"shareTexts":{},"answers":{"voters":{},"copyrightApplicants":{},"favlists":{},"newAnswer":{},"concernedUpvoters":{},"simpleConcernedUpvoters":{},"paidContent":{},"settings":{}},"banner":{},"topic":{"bios":{},"hot":{},"newest":{},"top":{},"unanswered":{},"questions":{},"followers":{},"contributors":{},"parent":{},"children":{},"bestAnswerers":{},"wikiMeta":{},"index":{},"intro":{},"meta":{},"schema":{},"creatorWall":{},"wikiEditInfo":{},"committedWiki":{}},"explore":{"recommendations":{}},"articles":{"voters":{}},"favlists":{"relations":{}},"pins":{"voters":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotListCategories":[],"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]}},"upload":{},"video":{"data":{},"shareVideoDetail":{},"last":{}},"guide":{"guide":{"isFetching":false,"isShowGuide":false}},"reward":{"answer":{},"article":{},"question":{}},"search":{"recommendSearch":[],"topSearch":{},"searchValue":{},"suggestSearch":{},"attachedInfo":{},"nextOffset":{},"topicReview":{},"generalByQuery":{},"generalByQueryInADay":{},"generalByQueryInAWeek":{},"generalByQueryInThreeMonths":{},"peopleByQuery":{},"topicByQuery":{},"columnByQuery":{},"liveByQuery":{},"albumByQuery":{},"eBookByQuery":{}},"creator":{"currentCreatorUrlToken":null,"homeData":{"recommendQuestions":[]},"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0},"goodatTopics":[]},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{"academy":{"tabs":[],"article":{}}},"rights":[],"rightsStatus":{},"levelUpperLimit":10,"account":{"growthLevel":{}}},"publicEditPermission":{},"readStatus":{},"draftHistory":{"history":{},"drafts":{}},"notifications":{"recent":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"history":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"notificationActors":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"recentNotificationEntry":"all"}},"subAppName":"main"}</script><script src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/vendor.6c16e03dca561b828324.js.download"></script><script src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/main.app.f8010edcb811086e3854.js.download"></script><script src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/main.question-routes.b9110e054ef7cff9ed2e.js.download"></script><script src="./如何用简单例子讲解 Q - learning 的具体过程？ - 知乎_files/zap.js.download"></script><div><div style="display: none;">想来知乎工作？请发送邮件到 jobs@zhihu.com</div></div></body></html>